{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"SealHits This site contains the project documentation for the sealhits project. The project is named after the rather famous compilation music album . Table Of Contents Explanation - The overall explanation of what sealhits does. Tutorials - Some basic tutorials on how to use the various scripts inside sealhits. How-To Guides - Specific how-tos on various things. Reference - An API reference. The Sealhits Database - The description of the Postgresql database behind it all. Requirements and Setup Sealhits is written in Python and requires a number of additional libraries. These can be installed using pip. It is recommended to use a virtual environment as follows: python -m venv venv source ./venv/bin/activate pip install -r requirements.txt A PostgreSQL database setup is required to hold all the data. Most Linux distributions have an install candidate for Postgresql. Once installed, the database can be setup as follows: First, create the user and the database (using psql or similar) create user sealhits with password 'kissfromarose' createdb login; create database sealhits owner sealhits; Then, import the schema: psql -U sealhits sealhits < sealhits.sql The documentation follows the best practice for project documentation as described by Daniele Procida in the Di\u00e1taxis documentation framework .","title":"SealHits"},{"location":"#sealhits","text":"This site contains the project documentation for the sealhits project. The project is named after the rather famous compilation music album .","title":"SealHits"},{"location":"#table-of-contents","text":"Explanation - The overall explanation of what sealhits does. Tutorials - Some basic tutorials on how to use the various scripts inside sealhits. How-To Guides - Specific how-tos on various things. Reference - An API reference. The Sealhits Database - The description of the Postgresql database behind it all.","title":"Table Of Contents"},{"location":"#requirements-and-setup","text":"Sealhits is written in Python and requires a number of additional libraries. These can be installed using pip. It is recommended to use a virtual environment as follows: python -m venv venv source ./venv/bin/activate pip install -r requirements.txt A PostgreSQL database setup is required to hold all the data. Most Linux distributions have an install candidate for Postgresql. Once installed, the database can be setup as follows: First, create the user and the database (using psql or similar) create user sealhits with password 'kissfromarose' createdb login; create database sealhits owner sealhits; Then, import the schema: psql -U sealhits sealhits < sealhits.sql The documentation follows the best practice for project documentation as described by Daniele Procida in the Di\u00e1taxis documentation framework .","title":"Requirements and Setup"},{"location":"database/","text":"The Sealhits Database The major component of the sealhits program is the postgresql database at it's heart. This database is used to house all the required information to produce the tracks and images recorded by SMRU. It derives it's data from the SQLITE files exported by PAMGuard, the PGDF files from PAMGuard and the Tritech Gemini GLF files. Overview There are a total of 5 main tables and 4 support tables in the database. These are: glfs - a list of all the glf files considered. groups - the groups formed of tracks and annotations. The main table. images - a list of all the images extracted from the GLF files. pgdfs - a list of all the pgdfs considered. points - a list of all the points required to recreate the tracks. tracks_groups - a list of all the tracks, their details and which group they belong to. In addition, the following minor tables are used to form relationships groups_glfs - which glfs are needed for which group. groups_images - which images belong to which groups. groups_pgdfs - which pgdfs correspond to which groups. _diesel_schema_migrations - a table for the rust library diesel. An annotated group and its associated points and images are the three main table to be considered. See the How to Guides for examples of how to setup a postgresql query to select points and images for a particular group. Schemas The following is the SQL for each table and what the columns represent. Throughout, the uid field is a unique key. Furthermore, each table has a unique composite key used to prevent accidental duplication. This allows for re-ingesting files without fear of duplicating data. glfs ALTER TABLE public.glfs ADD filename varchar NOT NULL; ALTER TABLE public.glfs ADD startdate timestamptz NOT NULL; ALTER TABLE public.glfs ADD enddate timestamptz NOT NULL; ALTER TABLE public.glfs ADD uid bigserial NOT NULL; filename - the filename of the glf as it appears on disk - no path. startdate - the startdate of this glf read from the glf config file. Date is in UTC. enddate - the enddate of this glf read from the glf config file. Date is in UTC. uid - a unique identifier for this glf - a UUID4 generated randomly on creation. groups ALTER TABLE public.\"groups\" ADD uid uuid NOT NULL DEFAULT gen_random_uuid(); ALTER TABLE public.\"groups\" ADD gid int8 NOT NULL; ALTER TABLE public.\"groups\" ADD timestart timestamptz NOT NULL; ALTER TABLE public.\"groups\" ADD interact bool NOT NULL DEFAULT false; ALTER TABLE public.\"groups\" ADD mammal int4 NOT NULL; ALTER TABLE public.\"groups\" ADD fish int4 NOT NULL; ALTER TABLE public.\"groups\" ADD bird int4 NOT NULL; ALTER TABLE public.\"groups\" ADD sqlite varchar NOT NULL; ALTER TABLE public.\"groups\" ADD code varchar NOT NULL; ALTER TABLE public.\"groups\" ADD \"comment\" text NULL; ALTER TABLE public.\"groups\" ADD timeend timestamptz NOT NULL; ALTER TABLE public.\"groups\" ADD sqliteid int8 NOT NULL; ALTER TABLE public.\"groups\" ADD split int4 NOT NULL DEFAULT 0; ALTER TABLE public.\"groups\" ADD huid varchar DEFAULT gen_random_uuid() NOT NULL; uid - a unique identifier for this glf - a UUID4 generated randomly on creation. gid - the uid taken from the sqlite file. Not guaranteed to be unique. timestart - the starting time in UTC, taken from the SQLITE file, altered by the 'buffer' parameter from ingest.py. interact - was there an interaction? Human labelled annotation. mammal - mammal rating. Human labelled annotation. fish - fish rating. Human labelled annotation. bird - bird rating. Human labelled annotation. sqlite - the filename of the sqlite file this group was taken from. code - Human classification. comment - Human comment. timeend - the ending time in UTC, taken from the SQLITE file, altered by the 'buffer' parameter from ingest.py. sqliteid - the gid value taken from the sqlite file. Not guaranteed to be unique. split - if this group was originally part of another group, what order did it have? For example, if one group was split into 2, the second group would have a split value of one, whilst retaining the same gid, sqliteid and sqlite fields. huid - a human readable uid. This is a string of four words, combined with hyphens. Should be unique but isn't as guaranteed as the uuid. Useful for humans to refer to groups. This table has a primary key of uid. This table has a composite key of (gid, sqliteid, sqlite, split). images ALTER TABLE public.images ADD filename varchar NOT NULL; ALTER TABLE public.images ADD uid uuid NOT NULL; ALTER TABLE public.images ADD hastrack bool NOT NULL DEFAULT false; ALTER TABLE public.images ADD glf varchar NOT NULL; ALTER TABLE public.images ADD \"time\" timestamptz NOT NULL; ALTER TABLE public.images ADD sonarid int4 NOT NULL; ALTER TABLE public.images ADD \"range\" float8 NOT NULL DEFAULT 55; filename - the filename of this exported FITS format image on disk. uid - a unique identifier for this glf - a UUID4 generated randomly on creation. hastrack - does a track appear in this image? glf - the glf file this image was taken from (TODO - replace with a foreign key into the GLF table) time - the time this image was taken in UTC. Extracted from the GLF file. sonarid - the ID of the sonar that took this image. range - the range setting of the sonar when this image was taken, in metres. Images have a primary key of uid. pgdfs ALTER TABLE public.pgdfs ADD filename varchar NOT NULL; ALTER TABLE public.pgdfs ADD startdate timestamptz NOT NULL; ALTER TABLE public.pgdfs ADD enddate timestamptz NOT NULL; ALTER TABLE public.pgdfs ADD uid bigserial NOT NULL; filename - the filename of the pgdf on disk. startdate - the starting date and time of the PGDF in UTC. enddate - the ending date and time of the PGDF in UTC. uid - a unique identifier for this glf - a bigint generated on creation. Should be replaced with a uuid. PGDFs has a primary key of uid. points ALTER TABLE public.points ADD \"time\" timestamptz NOT NULL; ALTER TABLE public.points ADD sonarid int4 NOT NULL; ALTER TABLE public.points ADD minbearing float4 NOT NULL; ALTER TABLE public.points ADD maxbearing float4 NOT NULL; ALTER TABLE public.points ADD minrange float4 NOT NULL; ALTER TABLE public.points ADD maxrange float4 NOT NULL; ALTER TABLE public.points ADD uuid DEFAULT gen_random_uuid() NOT NULL, ALTER TABLE public.points ADD peakbearing float4 NOT NULL; ALTER TABLE public.points ADD peakrange float4 NOT NULL; ALTER TABLE public.points ADD \"maxvalue\" float4 NOT NULL; ALTER TABLE public.points ADD occupancy float4 NOT NULL; ALTER TABLE public.points ADD objsize float4 NOT NULL; ALTER TABLE public.points ADD track_id uuid NOT NULL; ALTER TABLE public.points ADD group_id uuid NOT NULL time - the time this point appears in UTC. sonarid - the id of the sonar that took the image that this point appears in. minbearing - taken from the SQLITE file, in radians (minus is to the RIGHT of the fan image centreline). maxbearing - taken from the SQLITE file, in radians (minus is to the RIGHT of the fan image centreline). minrange - taken from the SQLITE file, in metres. maxrange - taken from the SQLITE file, in metres. uid - a unique identifier for this glf - a UUID4 generated randomly on creation. peakbearing - taken from the SQLITE file, in radians (minus is to the RIGHT of the fan image centreline). peakrange - taken from the SQLITE file, in metres. maxvalue - taken from the SQLITE file. Unknown units. occupancy - taken from the SQLITE file. Unknown units. objsize - taken from the SQLITE file. Unknown units. track_id - FOREIGN KEY into the tracks_group table. Which track does this point belong to? group_id - FOREIGN KEY into the groups table. Which group does this point belong to? Note this may differ from the group that the track that the point belongs to. The former is the original group BEFORE any splits took place. This is a quick solution and may be changed in future versions. There is a primary key on uid. tracks_groups ALTER TABLE public.tracks_groups ADD track_pam_id int8 NOT NULL; ALTER TABLE public.tracks_groups ADD group_id uuid NOT NULL; ALTER TABLE public.tracks_groups ADD binfile varchar NOT NULL; ALTER TABLE public.tracks_groups ADD track_id uuid NOT NULL DEFAULT gen_random_uuid(); track_pam_id - the uid taken from the sqlite file. Not guaranteed to be unique. group_id - FOREIGN KEY into the groups table. Which group does this track belong to? binfile - the PGDF file this track was taken from (TODO - replace with foreign key into PGDF table). uid - a unique identifier for this glf - a UUID4 generated randomly on creation. uid is the primary key. The database consists of a number of indexes on various tables to increase performance.","title":"The Sealhits Database"},{"location":"database/#the-sealhits-database","text":"The major component of the sealhits program is the postgresql database at it's heart. This database is used to house all the required information to produce the tracks and images recorded by SMRU. It derives it's data from the SQLITE files exported by PAMGuard, the PGDF files from PAMGuard and the Tritech Gemini GLF files.","title":"The Sealhits Database"},{"location":"database/#overview","text":"There are a total of 5 main tables and 4 support tables in the database. These are: glfs - a list of all the glf files considered. groups - the groups formed of tracks and annotations. The main table. images - a list of all the images extracted from the GLF files. pgdfs - a list of all the pgdfs considered. points - a list of all the points required to recreate the tracks. tracks_groups - a list of all the tracks, their details and which group they belong to. In addition, the following minor tables are used to form relationships groups_glfs - which glfs are needed for which group. groups_images - which images belong to which groups. groups_pgdfs - which pgdfs correspond to which groups. _diesel_schema_migrations - a table for the rust library diesel. An annotated group and its associated points and images are the three main table to be considered. See the How to Guides for examples of how to setup a postgresql query to select points and images for a particular group.","title":"Overview"},{"location":"database/#schemas","text":"The following is the SQL for each table and what the columns represent. Throughout, the uid field is a unique key. Furthermore, each table has a unique composite key used to prevent accidental duplication. This allows for re-ingesting files without fear of duplicating data.","title":"Schemas"},{"location":"database/#glfs","text":"ALTER TABLE public.glfs ADD filename varchar NOT NULL; ALTER TABLE public.glfs ADD startdate timestamptz NOT NULL; ALTER TABLE public.glfs ADD enddate timestamptz NOT NULL; ALTER TABLE public.glfs ADD uid bigserial NOT NULL; filename - the filename of the glf as it appears on disk - no path. startdate - the startdate of this glf read from the glf config file. Date is in UTC. enddate - the enddate of this glf read from the glf config file. Date is in UTC. uid - a unique identifier for this glf - a UUID4 generated randomly on creation.","title":"glfs"},{"location":"database/#groups","text":"ALTER TABLE public.\"groups\" ADD uid uuid NOT NULL DEFAULT gen_random_uuid(); ALTER TABLE public.\"groups\" ADD gid int8 NOT NULL; ALTER TABLE public.\"groups\" ADD timestart timestamptz NOT NULL; ALTER TABLE public.\"groups\" ADD interact bool NOT NULL DEFAULT false; ALTER TABLE public.\"groups\" ADD mammal int4 NOT NULL; ALTER TABLE public.\"groups\" ADD fish int4 NOT NULL; ALTER TABLE public.\"groups\" ADD bird int4 NOT NULL; ALTER TABLE public.\"groups\" ADD sqlite varchar NOT NULL; ALTER TABLE public.\"groups\" ADD code varchar NOT NULL; ALTER TABLE public.\"groups\" ADD \"comment\" text NULL; ALTER TABLE public.\"groups\" ADD timeend timestamptz NOT NULL; ALTER TABLE public.\"groups\" ADD sqliteid int8 NOT NULL; ALTER TABLE public.\"groups\" ADD split int4 NOT NULL DEFAULT 0; ALTER TABLE public.\"groups\" ADD huid varchar DEFAULT gen_random_uuid() NOT NULL; uid - a unique identifier for this glf - a UUID4 generated randomly on creation. gid - the uid taken from the sqlite file. Not guaranteed to be unique. timestart - the starting time in UTC, taken from the SQLITE file, altered by the 'buffer' parameter from ingest.py. interact - was there an interaction? Human labelled annotation. mammal - mammal rating. Human labelled annotation. fish - fish rating. Human labelled annotation. bird - bird rating. Human labelled annotation. sqlite - the filename of the sqlite file this group was taken from. code - Human classification. comment - Human comment. timeend - the ending time in UTC, taken from the SQLITE file, altered by the 'buffer' parameter from ingest.py. sqliteid - the gid value taken from the sqlite file. Not guaranteed to be unique. split - if this group was originally part of another group, what order did it have? For example, if one group was split into 2, the second group would have a split value of one, whilst retaining the same gid, sqliteid and sqlite fields. huid - a human readable uid. This is a string of four words, combined with hyphens. Should be unique but isn't as guaranteed as the uuid. Useful for humans to refer to groups. This table has a primary key of uid. This table has a composite key of (gid, sqliteid, sqlite, split).","title":"groups"},{"location":"database/#images","text":"ALTER TABLE public.images ADD filename varchar NOT NULL; ALTER TABLE public.images ADD uid uuid NOT NULL; ALTER TABLE public.images ADD hastrack bool NOT NULL DEFAULT false; ALTER TABLE public.images ADD glf varchar NOT NULL; ALTER TABLE public.images ADD \"time\" timestamptz NOT NULL; ALTER TABLE public.images ADD sonarid int4 NOT NULL; ALTER TABLE public.images ADD \"range\" float8 NOT NULL DEFAULT 55; filename - the filename of this exported FITS format image on disk. uid - a unique identifier for this glf - a UUID4 generated randomly on creation. hastrack - does a track appear in this image? glf - the glf file this image was taken from (TODO - replace with a foreign key into the GLF table) time - the time this image was taken in UTC. Extracted from the GLF file. sonarid - the ID of the sonar that took this image. range - the range setting of the sonar when this image was taken, in metres. Images have a primary key of uid.","title":"images"},{"location":"database/#pgdfs","text":"ALTER TABLE public.pgdfs ADD filename varchar NOT NULL; ALTER TABLE public.pgdfs ADD startdate timestamptz NOT NULL; ALTER TABLE public.pgdfs ADD enddate timestamptz NOT NULL; ALTER TABLE public.pgdfs ADD uid bigserial NOT NULL; filename - the filename of the pgdf on disk. startdate - the starting date and time of the PGDF in UTC. enddate - the ending date and time of the PGDF in UTC. uid - a unique identifier for this glf - a bigint generated on creation. Should be replaced with a uuid. PGDFs has a primary key of uid.","title":"pgdfs"},{"location":"database/#points","text":"ALTER TABLE public.points ADD \"time\" timestamptz NOT NULL; ALTER TABLE public.points ADD sonarid int4 NOT NULL; ALTER TABLE public.points ADD minbearing float4 NOT NULL; ALTER TABLE public.points ADD maxbearing float4 NOT NULL; ALTER TABLE public.points ADD minrange float4 NOT NULL; ALTER TABLE public.points ADD maxrange float4 NOT NULL; ALTER TABLE public.points ADD uuid DEFAULT gen_random_uuid() NOT NULL, ALTER TABLE public.points ADD peakbearing float4 NOT NULL; ALTER TABLE public.points ADD peakrange float4 NOT NULL; ALTER TABLE public.points ADD \"maxvalue\" float4 NOT NULL; ALTER TABLE public.points ADD occupancy float4 NOT NULL; ALTER TABLE public.points ADD objsize float4 NOT NULL; ALTER TABLE public.points ADD track_id uuid NOT NULL; ALTER TABLE public.points ADD group_id uuid NOT NULL time - the time this point appears in UTC. sonarid - the id of the sonar that took the image that this point appears in. minbearing - taken from the SQLITE file, in radians (minus is to the RIGHT of the fan image centreline). maxbearing - taken from the SQLITE file, in radians (minus is to the RIGHT of the fan image centreline). minrange - taken from the SQLITE file, in metres. maxrange - taken from the SQLITE file, in metres. uid - a unique identifier for this glf - a UUID4 generated randomly on creation. peakbearing - taken from the SQLITE file, in radians (minus is to the RIGHT of the fan image centreline). peakrange - taken from the SQLITE file, in metres. maxvalue - taken from the SQLITE file. Unknown units. occupancy - taken from the SQLITE file. Unknown units. objsize - taken from the SQLITE file. Unknown units. track_id - FOREIGN KEY into the tracks_group table. Which track does this point belong to? group_id - FOREIGN KEY into the groups table. Which group does this point belong to? Note this may differ from the group that the track that the point belongs to. The former is the original group BEFORE any splits took place. This is a quick solution and may be changed in future versions. There is a primary key on uid.","title":"points"},{"location":"database/#tracks_groups","text":"ALTER TABLE public.tracks_groups ADD track_pam_id int8 NOT NULL; ALTER TABLE public.tracks_groups ADD group_id uuid NOT NULL; ALTER TABLE public.tracks_groups ADD binfile varchar NOT NULL; ALTER TABLE public.tracks_groups ADD track_id uuid NOT NULL DEFAULT gen_random_uuid(); track_pam_id - the uid taken from the sqlite file. Not guaranteed to be unique. group_id - FOREIGN KEY into the groups table. Which group does this track belong to? binfile - the PGDF file this track was taken from (TODO - replace with foreign key into PGDF table). uid - a unique identifier for this glf - a UUID4 generated randomly on creation. uid is the primary key. The database consists of a number of indexes on various tables to increase performance.","title":"tracks_groups"},{"location":"explanation/","text":"Explanation SealHits brings together all the data from the the Tritech Sonar and PAMGuard into a single database and a set of FITS files for ease of use in other applications. Once the data is brought together, it can be processed and converted into datasets for use with deep learning systems. Sealhits brings the data together for analysis, processing and eventual reconfiguring into useful datasets. Ingesting Before any final datasets can be produced, the various sources must be ingested . Once ingestion has finished, you will have a complete postgresql database and a directory of FITS images. The database contains all the information required to reproduced the hand-annotated tracks/groups from PAMGuard. This includeds the classification, the points, any comments etc. In addition, there will be a directory of FITS images (using lz4 compression ). These represent the frames of sonar images required to reproduce the tracks/groups. Some of these will be buffer images - a number of second before and after the group. The ingest program will do the following things in order: Read the groups from the sqlite database, ignoring any groups that already exist in the database (unless the -q switch is passed). Split the groups into subgroups if there are any gaps longer than the buffer amount (set with the -b flag). With the groups obtained, read all the PGDFs, recording their start and end times. Find all the relevant points from the PGDFs, storing them in the database. Read all the GLFs, recording their start and end times. Export all the frames from the GLFs that correspond to the track points time in each group, saving as a FITS image. Creating datasets Once the ingest is complete, we can creat datasets for use with our various deep-learning programs and models. The project CrabSeal can generate datasets for use with our neural network project OceanMotion. Looking at the data With all the data collected in one place, one can look at the data more widely. PostgreSQL queries are one way. The program video.py generates videos of the groups, complete with the tracks as bounding boxes.","title":"Explanation"},{"location":"explanation/#explanation","text":"SealHits brings together all the data from the the Tritech Sonar and PAMGuard into a single database and a set of FITS files for ease of use in other applications. Once the data is brought together, it can be processed and converted into datasets for use with deep learning systems. Sealhits brings the data together for analysis, processing and eventual reconfiguring into useful datasets.","title":"Explanation"},{"location":"explanation/#ingesting","text":"Before any final datasets can be produced, the various sources must be ingested . Once ingestion has finished, you will have a complete postgresql database and a directory of FITS images. The database contains all the information required to reproduced the hand-annotated tracks/groups from PAMGuard. This includeds the classification, the points, any comments etc. In addition, there will be a directory of FITS images (using lz4 compression ). These represent the frames of sonar images required to reproduce the tracks/groups. Some of these will be buffer images - a number of second before and after the group. The ingest program will do the following things in order: Read the groups from the sqlite database, ignoring any groups that already exist in the database (unless the -q switch is passed). Split the groups into subgroups if there are any gaps longer than the buffer amount (set with the -b flag). With the groups obtained, read all the PGDFs, recording their start and end times. Find all the relevant points from the PGDFs, storing them in the database. Read all the GLFs, recording their start and end times. Export all the frames from the GLFs that correspond to the track points time in each group, saving as a FITS image.","title":"Ingesting"},{"location":"explanation/#creating-datasets","text":"Once the ingest is complete, we can creat datasets for use with our various deep-learning programs and models. The project CrabSeal can generate datasets for use with our neural network project OceanMotion.","title":"Creating datasets"},{"location":"explanation/#looking-at-the-data","text":"With all the data collected in one place, one can look at the data more widely. PostgreSQL queries are one way. The program video.py generates videos of the groups, complete with the tracks as bounding boxes.","title":"Looking at the data"},{"location":"how-to-guides/","text":"How-to guides The following are a number of short guides on how perform some of the key processes in sealhits. Bring data from PAMGuard and Tritech into the database The ingest program requires a database be available (in this case, a postgresql database called sealhits, with the sealhits user). The following parameters are required: -s - The path to the sqlite3 file we are importing from. -g - The path to where the GLFs live. -p - The path to the PGDFs. -b - The buffer size in seconds. -o - The output directory for the FITS image files. -d - The database name. Ingest will look at the groups, tracks and pgdfs in the sqlite file. It will then find out which are the new groups (that don't already exist in the database) and will add the new ones. The PGDFs and GLFS do not need to be organised or grouped to match - ingest will search all GLFs and PGDFs available and find the ones that match the groups from the sqlite. python ingest.py -s /mnt/spinning0/sealhits/sqlites/MeygenTritechDetectHDD_12Binary_GH_221220.sqlite3 -g /mnt/smru-sonar -p /mnt/spinning0/sealhits/pgdfs -b 4 -o /mnt/spinning0/sealhits/fits -d sealhits The resulting output will be a populated database with the associated FITS files, compressed with LZ4 compression. From here, other programs such as video.py. Process GLFs on their own If you passed the '-v' option to ingest.py, you will have skipped processing the GLF files. This can be useful if you just need the data from the PGDFs and the sqlite databases. Sometimes, GLF ingests will fail for various reasons and may need running again. If you have read in the databases correctly and just need to re-ingest the GLF files, you can do so with the following command: python ingest_glfs_hdd.py -i /path/to/glfs -d sealhits -a oursqlitefile.sqlite3 -o /output/folder/for/images This command will look in the GLFs folder and extract the matching images against the datbase, spitting out fits files into the output directory. This program works on a per sqlite3/hard-disk capacity. Any groups with a matching 'sqlite' column will be checked. Ingesting GLFs is the most time consuming operation of the entire ingest process by quite some way. Verify the data is correct to-do Generate Videos from the data It's useful to see what the data look like in the form of a video. python video.py -o ~/tmp -d testseals -u testseals -w testseals -y 1200 -i ../sealhits_testdata/fits -c ~/tmp/cache -b b51d4e92-7259-4fc2-b13e-83878763fccf This command will generate a video of the group 'b51d4e92-7259-4fc2-b13e-83878763fccf', from the database 'testseals'. The following parameters are: -o - The output directory for the video. -d - The database name. -u - The username for the database. -w - The password for the database. -y - The height of the final video. -i - The path to the fits files. -c - The path to the cache of fan images (if it exists). -b - Draw the bounding boxes for this track. SQL queries Some SQL queries that can be performed on the database. Get all the images for a group select * from images ig inner join groups_images gi on gi.image_id = ig.uid inner join groups gg on gg.uid = gi.group_id where gg.uid = '16010b86-8c0e-4ec0-93b3-00414842286d'; Get all the points in a group These points are associated with a group post splitting. select * from points ps inner join groups gg on gg.uid = ps.group_id where gg.uid = '16010b86-8c0e-4ec0-93b3-00414842286d';","title":"How-to guides"},{"location":"how-to-guides/#how-to-guides","text":"The following are a number of short guides on how perform some of the key processes in sealhits.","title":"How-to guides"},{"location":"how-to-guides/#bring-data-from-pamguard-and-tritech-into-the-database","text":"The ingest program requires a database be available (in this case, a postgresql database called sealhits, with the sealhits user). The following parameters are required: -s - The path to the sqlite3 file we are importing from. -g - The path to where the GLFs live. -p - The path to the PGDFs. -b - The buffer size in seconds. -o - The output directory for the FITS image files. -d - The database name. Ingest will look at the groups, tracks and pgdfs in the sqlite file. It will then find out which are the new groups (that don't already exist in the database) and will add the new ones. The PGDFs and GLFS do not need to be organised or grouped to match - ingest will search all GLFs and PGDFs available and find the ones that match the groups from the sqlite. python ingest.py -s /mnt/spinning0/sealhits/sqlites/MeygenTritechDetectHDD_12Binary_GH_221220.sqlite3 -g /mnt/smru-sonar -p /mnt/spinning0/sealhits/pgdfs -b 4 -o /mnt/spinning0/sealhits/fits -d sealhits The resulting output will be a populated database with the associated FITS files, compressed with LZ4 compression. From here, other programs such as video.py.","title":"Bring data from PAMGuard and Tritech into the database"},{"location":"how-to-guides/#process-glfs-on-their-own","text":"If you passed the '-v' option to ingest.py, you will have skipped processing the GLF files. This can be useful if you just need the data from the PGDFs and the sqlite databases. Sometimes, GLF ingests will fail for various reasons and may need running again. If you have read in the databases correctly and just need to re-ingest the GLF files, you can do so with the following command: python ingest_glfs_hdd.py -i /path/to/glfs -d sealhits -a oursqlitefile.sqlite3 -o /output/folder/for/images This command will look in the GLFs folder and extract the matching images against the datbase, spitting out fits files into the output directory. This program works on a per sqlite3/hard-disk capacity. Any groups with a matching 'sqlite' column will be checked. Ingesting GLFs is the most time consuming operation of the entire ingest process by quite some way.","title":"Process GLFs on their own"},{"location":"how-to-guides/#verify-the-data-is-correct","text":"to-do","title":"Verify the data is correct"},{"location":"how-to-guides/#generate-videos-from-the-data","text":"It's useful to see what the data look like in the form of a video. python video.py -o ~/tmp -d testseals -u testseals -w testseals -y 1200 -i ../sealhits_testdata/fits -c ~/tmp/cache -b b51d4e92-7259-4fc2-b13e-83878763fccf This command will generate a video of the group 'b51d4e92-7259-4fc2-b13e-83878763fccf', from the database 'testseals'. The following parameters are: -o - The output directory for the video. -d - The database name. -u - The username for the database. -w - The password for the database. -y - The height of the final video. -i - The path to the fits files. -c - The path to the cache of fan images (if it exists). -b - Draw the bounding boxes for this track.","title":"Generate Videos from the data"},{"location":"how-to-guides/#sql-queries","text":"Some SQL queries that can be performed on the database.","title":"SQL queries"},{"location":"how-to-guides/#get-all-the-images-for-a-group","text":"select * from images ig inner join groups_images gi on gi.image_id = ig.uid inner join groups gg on gg.uid = gi.group_id where gg.uid = '16010b86-8c0e-4ec0-93b3-00414842286d';","title":"Get all the images for a group"},{"location":"how-to-guides/#get-all-the-points-in-a-group","text":"These points are associated with a group post splitting. select * from points ps inner join groups gg on gg.uid = ps.group_id where gg.uid = '16010b86-8c0e-4ec0-93b3-00414842286d';","title":"Get all the points in a group"},{"location":"reference/","text":"Reference and API This page covers the various functions inside the sealhits project and is intended for Python programmers. There are two modules - sealhits and sealsources. Sealhits covers the database and pulling the data out for processing. Sealsources covers pulling the data in from the various databases, pgdfs and glf files. Bounding Boxes bbox.py - Classes and functions related to bounding boxes. BearBox Source code in sealhits/bbox.py 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 class BearBox : def __init__ ( self , bearmin : float , bearmax : float , distmin : float , distmax : float , sonar_range : float , ): \"\"\"Bearings are in radians and are left-/right+ of the vertical / Y axis, but from the top. Sonar range is the range the sonar was set to in metres. Args: bearmin (float): The minimum bearing. bearmax (float): The maximum bearing. distmin (float): The minimum distance. distmax (float): The maximum distance. sonar_range (float): The range of the sonar at this point (in metres). \"\"\" self . bearing_min = bearmin self . bearing_max = bearmax self . dist_min = distmin self . dist_max = distmax self . sonar_range = sonar_range def to_xy_raw ( self , image_size : Tuple [ int , int ]) -> XYBox : \"\"\"Return a bearing box that is x,y but for the RAW, non-fan image. This image may be resized from the original but is still a rectangle. with no spatial distortion. Args: image_size (Tuple[int, int]): the size of the image this raw box belongs to (in pixels, width then height). Returns: XYBox: A new XYBox but within the *raw* image space (i.e original, not fan/polar transformed) \"\"\" from sealhits.btable import bearing_table def _find_idx ( c ): for i in range ( len ( bearing_table ) - 1 ): a = bearing_table [ i ] b = bearing_table [ i + 1 ] if a >= c and b < c : return i return 0 # xmin = int(((-self.bearing_max - math.radians(MIN_ANGLE)) / (math.radians(MAX_ANGLE) - math.radians(MIN_ANGLE))) * image_size[0]) # xmax = int(((-self.bearing_min - math.radians(MIN_ANGLE)) / (math.radians(MAX_ANGLE) - math.radians(MIN_ANGLE))) * image_size[0]) r = float ( image_size [ 0 ]) / float ( len ( bearing_table )) xmin = int ( _find_idx ( self . bearing_max ) * r ) # Swap due to the bearings being postive to negative xmax = int ( _find_idx ( self . bearing_min ) * r ) ymin = int ( self . dist_min / self . sonar_range * image_size [ 1 ]) ymax = int ( self . dist_max / self . sonar_range * image_size [ 1 ]) return XYBox ( xmin , ymin , xmax , ymax ) def to_xy ( self , image_size : Tuple [ int , int ]) -> XYBox : \"\"\"Given a fan/polar image size return the minx/y maxx/y in pixel coordinates. Image size is width/height. Resulting xy assumes origin at top left. Args: image_size (Tuple[int, int]): the size of the image this raw box belongs to (in pixels, width then height). Returns: XYBox: A new XYBox but within the *polar* image space (i.e fan/polar transformed, not raw rectangle) \"\"\" txy = [] txy . append ( dist_bearing_to_xy ( self . bearing_min , self . dist_min , self . sonar_range , image_size ) ) txy . append ( dist_bearing_to_xy ( self . bearing_max , self . dist_max , self . sonar_range , image_size ) ) txy . append ( dist_bearing_to_xy ( self . bearing_min , self . dist_max , self . sonar_range , image_size ) ) txy . append ( dist_bearing_to_xy ( self . bearing_max , self . dist_min , self . sonar_range , image_size ) ) min_x = txy [ 0 ][ 0 ] min_y = txy [ 0 ][ 1 ] max_x = txy [ 0 ][ 0 ] max_y = txy [ 0 ][ 1 ] for tt in txy [ 1 :]: if tt [ 0 ] < min_x : min_x = tt [ 0 ] if tt [ 0 ] > max_x : max_x = tt [ 0 ] if tt [ 1 ] < min_y : min_y = tt [ 1 ] if tt [ 1 ] > max_y : max_y = tt [ 1 ] return XYBox ( min_x , min_y , max_x , max_y ) def __str__ ( self ): return ( str ( self . bearing_min ) + \",\" + str ( self . bearing_max ) + \",\" + str ( self . dist_min ) + \",\" + str ( self . dist_max ) ) __init__ ( bearmin , bearmax , distmin , distmax , sonar_range ) Bearings are in radians and are left-/right+ of the vertical / Y axis, but from the top. Sonar range is the range the sonar was set to in metres. Parameters: bearmin ( float ) \u2013 The minimum bearing. bearmax ( float ) \u2013 The maximum bearing. distmin ( float ) \u2013 The minimum distance. distmax ( float ) \u2013 The maximum distance. sonar_range ( float ) \u2013 The range of the sonar at this point (in metres). Source code in sealhits/bbox.py 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 def __init__ ( self , bearmin : float , bearmax : float , distmin : float , distmax : float , sonar_range : float , ): \"\"\"Bearings are in radians and are left-/right+ of the vertical / Y axis, but from the top. Sonar range is the range the sonar was set to in metres. Args: bearmin (float): The minimum bearing. bearmax (float): The maximum bearing. distmin (float): The minimum distance. distmax (float): The maximum distance. sonar_range (float): The range of the sonar at this point (in metres). \"\"\" self . bearing_min = bearmin self . bearing_max = bearmax self . dist_min = distmin self . dist_max = distmax self . sonar_range = sonar_range to_xy ( image_size ) Given a fan/polar image size return the minx/y maxx/y in pixel coordinates. Image size is width/height. Resulting xy assumes origin at top left. Parameters: image_size ( Tuple [ int , int ] ) \u2013 the size of the image this raw box belongs to (in pixels, width then height). Returns: XYBox ( XYBox ) \u2013 A new XYBox but within the polar image space (i.e fan/polar transformed, not raw rectangle) Source code in sealhits/bbox.py 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 def to_xy ( self , image_size : Tuple [ int , int ]) -> XYBox : \"\"\"Given a fan/polar image size return the minx/y maxx/y in pixel coordinates. Image size is width/height. Resulting xy assumes origin at top left. Args: image_size (Tuple[int, int]): the size of the image this raw box belongs to (in pixels, width then height). Returns: XYBox: A new XYBox but within the *polar* image space (i.e fan/polar transformed, not raw rectangle) \"\"\" txy = [] txy . append ( dist_bearing_to_xy ( self . bearing_min , self . dist_min , self . sonar_range , image_size ) ) txy . append ( dist_bearing_to_xy ( self . bearing_max , self . dist_max , self . sonar_range , image_size ) ) txy . append ( dist_bearing_to_xy ( self . bearing_min , self . dist_max , self . sonar_range , image_size ) ) txy . append ( dist_bearing_to_xy ( self . bearing_max , self . dist_min , self . sonar_range , image_size ) ) min_x = txy [ 0 ][ 0 ] min_y = txy [ 0 ][ 1 ] max_x = txy [ 0 ][ 0 ] max_y = txy [ 0 ][ 1 ] for tt in txy [ 1 :]: if tt [ 0 ] < min_x : min_x = tt [ 0 ] if tt [ 0 ] > max_x : max_x = tt [ 0 ] if tt [ 1 ] < min_y : min_y = tt [ 1 ] if tt [ 1 ] > max_y : max_y = tt [ 1 ] return XYBox ( min_x , min_y , max_x , max_y ) to_xy_raw ( image_size ) Return a bearing box that is x,y but for the RAW, non-fan image. This image may be resized from the original but is still a rectangle. with no spatial distortion. Parameters: image_size ( Tuple [ int , int ] ) \u2013 the size of the image this raw box belongs to (in pixels, width then height). Returns: XYBox ( XYBox ) \u2013 A new XYBox but within the raw image space (i.e original, not fan/polar transformed) Source code in sealhits/bbox.py 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 def to_xy_raw ( self , image_size : Tuple [ int , int ]) -> XYBox : \"\"\"Return a bearing box that is x,y but for the RAW, non-fan image. This image may be resized from the original but is still a rectangle. with no spatial distortion. Args: image_size (Tuple[int, int]): the size of the image this raw box belongs to (in pixels, width then height). Returns: XYBox: A new XYBox but within the *raw* image space (i.e original, not fan/polar transformed) \"\"\" from sealhits.btable import bearing_table def _find_idx ( c ): for i in range ( len ( bearing_table ) - 1 ): a = bearing_table [ i ] b = bearing_table [ i + 1 ] if a >= c and b < c : return i return 0 # xmin = int(((-self.bearing_max - math.radians(MIN_ANGLE)) / (math.radians(MAX_ANGLE) - math.radians(MIN_ANGLE))) * image_size[0]) # xmax = int(((-self.bearing_min - math.radians(MIN_ANGLE)) / (math.radians(MAX_ANGLE) - math.radians(MIN_ANGLE))) * image_size[0]) r = float ( image_size [ 0 ]) / float ( len ( bearing_table )) xmin = int ( _find_idx ( self . bearing_max ) * r ) # Swap due to the bearings being postive to negative xmax = int ( _find_idx ( self . bearing_min ) * r ) ymin = int ( self . dist_min / self . sonar_range * image_size [ 1 ]) ymax = int ( self . dist_max / self . sonar_range * image_size [ 1 ]) return XYBox ( xmin , ymin , xmax , ymax ) XYBox A basic XY bounding box using minimums and maximums. Source code in sealhits/bbox.py 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 class XYBox : \"\"\"A basic XY bounding box using minimums and maximums.\"\"\" def __init__ ( self , minx : int , miny : int , maxx : int , maxy : int ): \"\"\"Create a 2D Bounding Box. Args: minx(int): minimum x value in pixels. miny(int): minimum y value in pixels. maxx(int): maximum x value in pixels. maxy(int): maximum y value in pixels. \"\"\" self . x_min = minx self . y_min = miny self . x_max = maxx self . y_max = maxy def __str__ ( self ): return ( str ( self . x_min ) + \",\" + str ( self . y_min ) + \",\" + str ( self . x_max ) + \",\" + str ( self . y_max ) ) def area ( self ) -> int : \"\"\"Return the area of this bbox. Args: None Returns: int: the area \"\"\" return ( self . x_max - self . x_min ) * ( self . y_max - self . y_min ) def tuple ( self ) -> Tuple [ int , int , int , int ]: \"\"\"Return the box as a Tuple Args: None Returns: Tuple[int,int,int,int] \"\"\" return ( self . x_min , self . y_min , self . x_max , self . y_max ) def pair ( self ) -> Tuple [ Tuple [ int , int ], Tuple [ int , int ]]: \"\"\"Return the box as two Tuples Returns: Tuple[Tuple[int,int],Tuple[int,int]]: the box as [xmin,ymin],[xmax,ymax] \"\"\" return (( self . x_min , self . y_min ), ( self . x_max , self . y_max )) def com ( self ) -> Tuple [ int , int ]: \"\"\"Return the centre of this bbox. Args: None Returns: Tuple[int,int]: the centre of this bounding box, rounded and as ints. \"\"\" return ( int (( self . x_max + self . x_min ) / 2 ), int (( self . y_max + self . y_min ) / 2 )) def flipv ( self , img_height ): \"\"\"An in-place flip vertically. Args: img_height (int): the height of the image to which this box belongs. \"\"\" tt = self . y_max self . y_max = img_height - self . y_min self . y_min = img_height - tt return self def equals ( self , b ) -> bool : \"\"\"Does this box equal another? Args: b (XYBox): the other box to compare against Returns: bool \"\"\" return ( self . x_min == b . x_min and self . x_max == b . x_max and self . y_min == b . y_min and self . y_max == b . y_max ) __init__ ( minx , miny , maxx , maxy ) Create a 2D Bounding Box. Args: minx(int): minimum x value in pixels. miny(int): minimum y value in pixels. maxx(int): maximum x value in pixels. maxy(int): maximum y value in pixels. Source code in sealhits/bbox.py 134 135 136 137 138 139 140 141 142 143 144 145 def __init__ ( self , minx : int , miny : int , maxx : int , maxy : int ): \"\"\"Create a 2D Bounding Box. Args: minx(int): minimum x value in pixels. miny(int): minimum y value in pixels. maxx(int): maximum x value in pixels. maxy(int): maximum y value in pixels. \"\"\" self . x_min = minx self . y_min = miny self . x_max = maxx self . y_max = maxy area () Return the area of this bbox. Args: None Returns: int ( int ) \u2013 the area Source code in sealhits/bbox.py 158 159 160 161 162 163 164 165 166 def area ( self ) -> int : \"\"\"Return the area of this bbox. Args: None Returns: int: the area \"\"\" return ( self . x_max - self . x_min ) * ( self . y_max - self . y_min ) com () Return the centre of this bbox. Args: None Returns: Tuple [ int , int ] \u2013 Tuple[int,int]: the centre of this bounding box, rounded and as ints. Source code in sealhits/bbox.py 186 187 188 189 190 191 192 193 194 def com ( self ) -> Tuple [ int , int ]: \"\"\"Return the centre of this bbox. Args: None Returns: Tuple[int,int]: the centre of this bounding box, rounded and as ints. \"\"\" return ( int (( self . x_max + self . x_min ) / 2 ), int (( self . y_max + self . y_min ) / 2 )) equals ( b ) Does this box equal another? Args: b (XYBox): the other box to compare against Returns: bool \u2013 bool Source code in sealhits/bbox.py 207 208 209 210 211 212 213 214 215 216 217 218 219 220 def equals ( self , b ) -> bool : \"\"\"Does this box equal another? Args: b (XYBox): the other box to compare against Returns: bool \"\"\" return ( self . x_min == b . x_min and self . x_max == b . x_max and self . y_min == b . y_min and self . y_max == b . y_max ) flipv ( img_height ) An in-place flip vertically. Parameters: img_height ( int ) \u2013 the height of the image to which this box belongs. Source code in sealhits/bbox.py 196 197 198 199 200 201 202 203 204 205 def flipv ( self , img_height ): \"\"\"An in-place flip vertically. Args: img_height (int): the height of the image to which this box belongs. \"\"\" tt = self . y_max self . y_max = img_height - self . y_min self . y_min = img_height - tt return self pair () Return the box as two Tuples Returns: Tuple [ Tuple [ int , int ], Tuple [ int , int ]] \u2013 Tuple[Tuple[int,int],Tuple[int,int]]: the box as [xmin,ymin],[xmax,ymax] Source code in sealhits/bbox.py 178 179 180 181 182 183 184 def pair ( self ) -> Tuple [ Tuple [ int , int ], Tuple [ int , int ]]: \"\"\"Return the box as two Tuples Returns: Tuple[Tuple[int,int],Tuple[int,int]]: the box as [xmin,ymin],[xmax,ymax] \"\"\" return (( self . x_min , self . y_min ), ( self . x_max , self . y_max )) tuple () Return the box as a Tuple Args: None Returns: Tuple [ int , int , int , int ] \u2013 Tuple[int,int,int,int] Source code in sealhits/bbox.py 168 169 170 171 172 173 174 175 176 def tuple ( self ) -> Tuple [ int , int , int , int ]: \"\"\"Return the box as a Tuple Args: None Returns: Tuple[int,int,int,int] \"\"\" return ( self . x_min , self . y_min , self . x_max , self . y_max ) XYZBox Source code in sealhits/bbox.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 class XYZBox : def __init__ ( self , minx : int , miny : int , minz : int , maxx : int , maxy : int , maxz : int ): \"\"\"Initialise a 3D bounding box. Args: minx (int): minimum x value in pixels. miny (int): minimum y value in pixels. minz (int): minimum z value in pixels. maxx (int): maximum x value in pixels. maxy (int): maximum y value in pixels. maxz (int): maximum z value in pixels. \"\"\" self . x_min = minx self . y_min = miny self . z_min = minz self . x_max = maxx self . y_max = maxy self . z_max = maxz def __str__ ( self ): return ( str ( self . x_min ) + \",\" + str ( self . y_min ) + \",\" + str ( self . z_min ) + \",\" + str ( self . x_max ) + \",\" + str ( self . y_max ) + \",\" + str ( self . z_max ) ) def volume ( self ) -> int : \"\"\"Return the volume of this bbox. Args: None Returns: int: the volume \"\"\" # Z is always a minimum of 1, as zmax can equal zmin - this XYZBox is ultimately an XYBox return ( ( self . x_max - self . x_min ) * ( self . y_max - self . y_min ) * max ( self . z_max - self . z_min , 1 ) ) def tuple ( self ) -> Tuple [ int , int , int , int , int , int ]: \"\"\"Return the box as a Tuple Args: None Returns: Tuple[int,int,int,int,int,int]: the volume as xmin,ymin,zmin,xmax,ymax,zmax \"\"\" return ( self . x_min , self . y_min , self . z_min , self . x_max , self . y_max , self . y_max ) def pair ( self ) -> Tuple [ Tuple [ int , int , int ], Tuple [ int , int , int ]]: \"\"\"Return the box as two Tuples Args: None Returns: Tuple[Tuple[int,int,int],Tuple[int,int,int]]: the volume as [xmin,ymin,zmin],[xmax,ymax,zmax] \"\"\" return ( ( self . x_min , self . y_min , self . z_min ), ( self . x_max , self . y_max , self . z_max ), ) def com ( self ) -> Tuple [ int , int , int ]: \"\"\"Return the centre of this bbox. Args: None Returns: Tuple[int,int,int]: the centre of this bounding box, rounded and as ints. \"\"\" return ( int (( self . x_max + self . x_min ) / 2 ), int (( self . y_max + self . y_min ) / 2 ), int (( self . z_max + self . z_min ) / 2 ), ) def equals ( self , b : XYZBox ) -> bool : \"\"\"Does this box equal another? Args: b (XYZBox): the other box to compare against Returns: bool \"\"\" return ( self . x_min == b . x_min and self . x_max == b . x_max and self . y_min == b . y_min and self . y_max == b . y_max and self . z_min == b . z_min and self . z_max == b . z_max ) __init__ ( minx , miny , minz , maxx , maxy , maxz ) Initialise a 3D bounding box. Parameters: minx ( int ) \u2013 minimum x value in pixels. miny ( int ) \u2013 minimum y value in pixels. minz ( int ) \u2013 minimum z value in pixels. maxx ( int ) \u2013 maximum x value in pixels. maxy ( int ) \u2013 maximum y value in pixels. maxz ( int ) \u2013 maximum z value in pixels. Source code in sealhits/bbox.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 def __init__ ( self , minx : int , miny : int , minz : int , maxx : int , maxy : int , maxz : int ): \"\"\"Initialise a 3D bounding box. Args: minx (int): minimum x value in pixels. miny (int): minimum y value in pixels. minz (int): minimum z value in pixels. maxx (int): maximum x value in pixels. maxy (int): maximum y value in pixels. maxz (int): maximum z value in pixels. \"\"\" self . x_min = minx self . y_min = miny self . z_min = minz self . x_max = maxx self . y_max = maxy self . z_max = maxz com () Return the centre of this bbox. Args: None Returns: Tuple [ int , int , int ] \u2013 Tuple[int,int,int]: the centre of this bounding box, rounded and as ints. Source code in sealhits/bbox.py 99 100 101 102 103 104 105 106 107 108 109 110 111 def com ( self ) -> Tuple [ int , int , int ]: \"\"\"Return the centre of this bbox. Args: None Returns: Tuple[int,int,int]: the centre of this bounding box, rounded and as ints. \"\"\" return ( int (( self . x_max + self . x_min ) / 2 ), int (( self . y_max + self . y_min ) / 2 ), int (( self . z_max + self . z_min ) / 2 ), ) equals ( b ) Does this box equal another? Args: b (XYZBox): the other box to compare against Returns: bool \u2013 bool Source code in sealhits/bbox.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 def equals ( self , b : XYZBox ) -> bool : \"\"\"Does this box equal another? Args: b (XYZBox): the other box to compare against Returns: bool \"\"\" return ( self . x_min == b . x_min and self . x_max == b . x_max and self . y_min == b . y_min and self . y_max == b . y_max and self . z_min == b . z_min and self . z_max == b . z_max ) pair () Return the box as two Tuples Args: None Returns: Tuple [ Tuple [ int , int , int ], Tuple [ int , int , int ]] \u2013 Tuple[Tuple[int,int,int],Tuple[int,int,int]]: the volume as [xmin,ymin,zmin],[xmax,ymax,zmax] Source code in sealhits/bbox.py 86 87 88 89 90 91 92 93 94 95 96 97 def pair ( self ) -> Tuple [ Tuple [ int , int , int ], Tuple [ int , int , int ]]: \"\"\"Return the box as two Tuples Args: None Returns: Tuple[Tuple[int,int,int],Tuple[int,int,int]]: the volume as [xmin,ymin,zmin],[xmax,ymax,zmax] \"\"\" return ( ( self . x_min , self . y_min , self . z_min ), ( self . x_max , self . y_max , self . z_max ), ) tuple () Return the box as a Tuple Args: None Returns: Tuple [ int , int , int , int , int , int ] \u2013 Tuple[int,int,int,int,int,int]: the volume as xmin,ymin,zmin,xmax,ymax,zmax Source code in sealhits/bbox.py 76 77 78 79 80 81 82 83 84 def tuple ( self ) -> Tuple [ int , int , int , int , int , int ]: \"\"\"Return the box as a Tuple Args: None Returns: Tuple[int,int,int,int,int,int]: the volume as xmin,ymin,zmin,xmax,ymax,zmax \"\"\" return ( self . x_min , self . y_min , self . z_min , self . x_max , self . y_max , self . y_max ) volume () Return the volume of this bbox. Args: None Returns: int ( int ) \u2013 the volume Source code in sealhits/bbox.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def volume ( self ) -> int : \"\"\"Return the volume of this bbox. Args: None Returns: int: the volume \"\"\" # Z is always a minimum of 1, as zmax can equal zmin - this XYZBox is ultimately an XYBox return ( ( self . x_max - self . x_min ) * ( self . y_max - self . y_min ) * max ( self . z_max - self . z_min , 1 ) ) bb_combine ( a , b ) Combine two bounding boxes into one. Either XYBox or XYZBox. Parameters: a ( Union [ XYBox , XYZBox ] ) \u2013 the XYBox or XYZBox. b ( Union [ XYBox , XYZBox ] ) \u2013 the XYBox or XYZBox. Returns: Union [ XYBox , XYZBox ] \u2013 Union[XYBox, XYZBox]: the combined box. Source code in sealhits/bbox.py 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 def bb_combine ( a : Union [ XYBox , XYZBox ], b : Union [ XYBox , XYZBox ] ) -> Union [ XYBox , XYZBox ]: \"\"\"Combine two bounding boxes into one. Either XYBox or XYZBox. Args: a (Union[XYBox, XYZBox]): the XYBox or XYZBox. b (Union[XYBox, XYZBox]): the XYBox or XYZBox. Returns: Union[XYBox, XYZBox]: the combined box. \"\"\" assert type ( a ) == type ( b ) x_min = min ( a . x_min , b . x_min ) y_min = min ( a . y_min , b . y_min ) x_max = max ( a . x_max , b . x_max ) y_max = max ( a . y_max , b . y_max ) if hasattr ( a , \"z_min\" ): z_min = min ( a . z_min , b . z_min ) z_max = max ( a . z_max , b . z_max ) return XYZBox ( x_min , y_min , z_min , x_max , y_max , z_max ) return XYBox ( x_min , y_min , x_max , y_max ) bb_expand ( bb , img_size , b_size ) Expand the bounding box. bb is either an XYBox or XYZ Box. img_size is a tuple of either (w,h) or (w,h,d). b_size is either a single int, or a tuple of different sizes. Parameters: bb ( Union [ XYBox , XYZBox ] ) \u2013 the XYBox or XYZBox to expand. img_size ( Union [ Tuple [ int , int ], Tuple [ int , int , int ]] ) \u2013 The size of the image to which this box belongs, in pixels. b_size ( Union [ int , Tuple ] ) \u2013 the amount to expand either uniformly (a single int), or expand by different amounts in each dimension. Returns: Union [ XYBox , XYZBox ] \u2013 Union[XYBox, XYZBox]: A new XYBox or XYZBox but within the raw image space (i.e original, not fan/polar transformed) Source code in sealhits/bbox.py 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 def bb_expand ( bb : Union [ XYBox , XYZBox ], img_size : Union [ Tuple [ int , int ], Tuple [ int , int , int ]], b_size : Union [ int , Tuple ], ) -> Union [ XYBox , XYZBox ]: \"\"\" Expand the bounding box. bb is either an XYBox or XYZ Box. img_size is a tuple of either (w,h) or (w,h,d). b_size is either a single int, or a tuple of different sizes. Args: bb (Union[XYBox, XYZBox]): the XYBox or XYZBox to expand. img_size (Union[Tuple[int, int], Tuple[int, int, int]]): The size of the image to which this box belongs, in pixels. b_size (Union[int, Tuple]): the amount to expand either uniformly (a single int), or expand by different amounts in each dimension. Returns: Union[XYBox, XYZBox]: A new XYBox or XYZBox but within the *raw* image space (i.e original, not fan/polar transformed) \"\"\" be = [] if hasattr ( b_size , \"__iter__\" ): if len ( b_size ) < 2 : # TODO - maybe raise an error/exception here? return bb be = b_size elif isinstance ( b_size , int ): if b_size <= 0 : return bb be = [ b_size , b_size , b_size ] x = bb . x_min y = bb . y_min a = bb . x_max b = bb . y_max c = 0 z = 0 w = a - x h = b - y d = 0 if hasattr ( bb , \"z_min\" ): z = bb . z_min c = bb . z_max d = c - z d = d + be [ 2 ] z = z - be [ 2 ] x = x - be [ 0 ] if x <= 0 : x = 0 if x >= img_size [ 0 ]: x = img_size [ 0 ] - 1 y = y - be [ 1 ] if y <= 0 : y = 0 if y >= img_size [ 1 ]: y = img_size [ 1 ] - 1 w += be [ 0 ] if w >= img_size [ 0 ]: w = img_size [ 0 ] - 1 h += be [ 1 ] if h >= img_size [ 1 ]: h = img_size [ 1 ] - 1 if hasattr ( bb , \"z_min\" ): if z <= 0 : z = 0 if z >= img_size [ 2 ]: z = img_size [ 2 ] - 1 if d <= 0 : d = 0 if d >= img_size [ 2 ]: d = img_size [ 2 ] - 1 return XYZBox ( x , y , z , x + w , y + h , z + d ) return XYBox ( x , y , x + w , y + h ) bb_inside ( a , b ) Return true if a is completely enclosed by b. False otherwise. Parameters: a ( Union [ XYBox , XYZBox ] ) \u2013 the XYBox or XYZBox. b ( Union [ XYBox , XYZBox ] ) \u2013 the XYBox or XYZBox. Returns: bool ( bool ) \u2013 is a completely enclosed by b? Source code in sealhits/bbox.py 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 def bb_inside ( a : Union [ XYBox , XYZBox ], b : Union [ XYBox , XYZBox ]) -> bool : \"\"\"Return true if a is completely enclosed by b. False otherwise. Args: a (Union[XYBox, XYZBox]): the XYBox or XYZBox. b (Union[XYBox, XYZBox]): the XYBox or XYZBox. Returns: bool: is a *completely* enclosed by b? \"\"\" if type ( a ) != type ( b ): return False if a . x_min >= b . x_min and a . x_max <= b . x_max : if a . y_min >= b . y_min and a . y_max <= b . y_max : if hasattr ( a , \"z_min\" ) and hasattr ( b , \"z_min\" ): if a . z_min >= b . z_min and a . z_max <= b . z_max : return True else : return True return False bb_overlap ( a , b ) Return True if two boxes overlap, False if not. Takes either an XYBox or XYZ box. For now, both a and b must be the same type, else False is returned. Parameters: a ( Union [ XYBox , XYZBox ] ) \u2013 the XYBox or XYZBox. b ( Union [ XYBox , XYZBox ] ) \u2013 the XYBox or XYZBox. Returns: bool ( bool ) \u2013 do a and b overlap? Source code in sealhits/bbox.py 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 def bb_overlap ( a : Union [ XYBox , XYZBox ], b : Union [ XYBox , XYZBox ]) -> bool : \"\"\"Return True if two boxes overlap, False if not. Takes either an XYBox or XYZ box. For now, both a and b must be the same type, else False is returned. Args: a (Union[XYBox, XYZBox]): the XYBox or XYZBox. b (Union[XYBox, XYZBox]): the XYBox or XYZBox. Returns: bool: do a and b overlap? \"\"\" if type ( a ) != type ( b ): return False if a . x_min <= b . x_max and a . x_max >= b . x_min : if a . y_min <= b . y_max and a . y_max >= b . y_min : if hasattr ( a , \"z_min\" ) and hasattr ( b , \"z_min\" ): if a . z_min <= b . z_max and a . z_max >= b . z_min : return True else : return True return False bb_raw_to_fan ( bbox , raw_size , fan_size , sonar_range ) Given an XY or XYZ bounding box in the raw image space, convert to fan space. Parameters: bbox ( Union [ XYBox , XYZBox ] ) \u2013 the XYBox or XYZBox to convert. raw_size ( Tuple [ int , int ] ) \u2013 the size of the raw rectangle. fan_size ( Tuple [ int , int ] ) \u2013 the size of the fan image. sonar_range ( float ) \u2013 the range of the sonar in this image Returns: Union [ XYBox , XYZBox ] \u2013 Union[XYBox, XYZBox]: the new XYBox or XYZBox. Source code in sealhits/bbox.py 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 def bb_raw_to_fan ( bbox : Union [ XYBox , XYZBox ], raw_size : Tuple ( int , int ), fan_size : Tuple ( int , int ), sonar_range : float , ) -> Union [ XYBox , XYZBox ]: \"\"\"Given an XY or XYZ bounding box in the raw image space, convert to fan space. Args: bbox (Union[XYBox, XYZBox]): the XYBox or XYZBox to convert. raw_size (Tuple[int, int]): the size of the raw rectangle. fan_size (Tuple[int, int]): the size of the fan image. sonar_range (float): the range of the sonar in this image Returns: Union[XYBox, XYZBox]: the new XYBox or XYZBox. \"\"\" from sealhits.btable import bearing_table x_min = bearing_table [ max ( bbox . x_min , 0 )] x_max = bearing_table [ min ( bbox . x_max , len ( bearing_table ) - 1 )] y_min = bbox . y_min / raw_size [ 1 ] * sonar_range y_max = bbox . y_max / raw_size [ 1 ] * sonar_range txy = [] txy . append ( dist_bearing_to_xy ( x_min , y_min , sonar_range , fan_size )) txy . append ( dist_bearing_to_xy ( x_max , y_max , sonar_range , fan_size )) txy . append ( dist_bearing_to_xy ( x_min , y_max , sonar_range , fan_size )) txy . append ( dist_bearing_to_xy ( x_max , y_min , sonar_range , fan_size )) min_x = txy [ 0 ][ 0 ] min_y = txy [ 0 ][ 1 ] max_x = txy [ 0 ][ 0 ] max_y = txy [ 0 ][ 1 ] for tt in txy [ 1 :]: if tt [ 0 ] < min_x : min_x = tt [ 0 ] if tt [ 0 ] > max_x : max_x = tt [ 0 ] if tt [ 1 ] < min_y : min_y = tt [ 1 ] if tt [ 1 ] > max_y : max_y = tt [ 1 ] return XYBox ( min_x , min_y , max_x , max_y ) bb_to_fix ( bbox , fix_size , image_size ) Given an XYBox, a fix_size and image_size, return an xy crop size that has a fixed size. Image size is width/height. Parameters: bbox ( XYBox ) \u2013 the XYBox we are changing. fix_size ( Tuple [ int , int ] ) \u2013 the new fixed size. image_size ( Tuple [ int , int ] ) \u2013 the size of the image to which this bbox belongs. Returns: XYBox ( XYBox ) \u2013 the new XYBox with the new size. Source code in sealhits/bbox.py 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 def bb_to_fix ( bbox : XYBox , fix_size : Tuple [ int , int ], image_size : Tuple [ int , int ] ) -> XYBox : \"\"\" Given an XYBox, a fix_size and image_size, return an xy crop size that has a fixed size. Image size is width/height. Args: bbox (XYBox): the XYBox we are changing. fix_size (Tuple[int, int]): the new fixed size. image_size (Tuple[int, int]): the size of the image to which this bbox belongs. Returns: XYBox: the new XYBox with the new size. \"\"\" (( min_x , min_y ), ( max_x , max_y )) = bbox . pair () cx = int (( max_x + min_x ) / 2 ) cy = int (( max_y + min_y ) / 2 ) s_x = cx - int ( fix_size [ 0 ] / 2 ) s_y = cy - int ( fix_size [ 1 ] / 2 ) e_x = cx + int ( fix_size [ 0 ] / 2 ) e_y = cy + int ( fix_size [ 1 ] / 2 ) if s_x < 0 : dd = 0 - s_x s_x = 0 e_x += dd if e_x >= image_size [ 0 ]: dd = e_x - image_size [ 0 ] s_x -= dd e_x -= dd if s_y < 0 : dd = 0 - s_y s_y = 0 e_y += dd if e_y >= image_size [ 1 ]: dd = e_y - image_size [ 1 ] s_y -= dd e_y -= dd return XYBox ( int ( s_x ), int ( s_y ), int ( e_x ), int ( e_y )) bb_to_min ( bbox , min_size , image_size ) Given an XYBox, return an xy crop size that has a minimum size and is within the image size. Image size is height/width. Parameters: bbox ( XYBox ) \u2013 the XYBox to crop. min_size ( Tuple [ int , int ] ) \u2013 the new minimum size. image_size ( Tuple [ int , int ] ) \u2013 the size of the image to which this bbox belongs. Returns: XYBox ( XYBox ) \u2013 the new XYBox. Source code in sealhits/bbox.py 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 def bb_to_min ( bbox : XYBox , min_size : Tuple [ int , int ], image_size : Tuple [ int , int ] ) -> XYBox : \"\"\"Given an XYBox, return an xy crop size that has a minimum size and is within the image size. Image size is height/width. Args: bbox (XYBox): the XYBox to crop. min_size (Tuple[int, int]): the new minimum size. image_size (Tuple[int, int]): the size of the image to which this bbox belongs. Returns: XYBox: the new XYBox. \"\"\" (( min_x , min_y ), ( max_x , max_y )) = bbox . pair () x = min_x y = min_y w = max_x - x h = max_y - y if h < min_size [ 1 ]: d = ( min_size [ 1 ] - h ) / 2 max_y += d min_y -= d # Checks to see if we are exceeding the size of the image if min_y < 0 : dd = 0 - min_y min_y = 0 max_y += dd if max_y > image_size [ 1 ]: dd = max_y - image_size [ 1 ] min_y -= dd max_y -= dd # Now do the width if w < min_size [ 0 ]: d = ( min_size [ 0 ] - w ) / 2 max_x += d min_x -= d if min_x < 0 : dd = 0 - min_x min_x = 0 max_x += dd if max_x > image_size [ 0 ]: dd = max_x - image_size [ 0 ] min_x -= dd max_x -= dd return XYBox ( int ( min_x ), int ( min_y ), int ( max_x ), int ( max_y )) combine_boxes ( bbs ) Given a list of 2D or 3D bbs, combine if they overlap. Overlapping groups are joined to create a bigger group, which makes further overlapping easier. 2D boxes are assumed to all be on the same Z step. This algorithm has bad complexity though and needs improvement. Parameters: bbs ( List [ Union [ XYBox , XYZBox ]] ) \u2013 a list of either XYBox or XYZBox. Returns: Tuple [ List [ Union [ XYBox , XYZBox ]], List [ Union [ XYBox , XYZBox ]]] \u2013 Tuple[List[Union[XYBox, XYZBox]], List[Union[XYBox, XYZBox]]]: Two lists - new boxes combined and a list of many lists of the grouped boxes. Source code in sealhits/bbox.py 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 def combine_boxes ( bbs : List [ Union [ XYBox , XYZBox ]]) -> Tuple [ List [ Union [ XYBox , XYZBox ]], List [ Union [ XYBox , XYZBox ]]]: \"\"\"Given a list of 2D or 3D bbs, combine if they overlap. Overlapping groups are joined to create a bigger group, which makes further overlapping easier. 2D boxes are assumed to all be on the same Z step. This algorithm has bad complexity though and needs improvement. Args: bbs (List[Union[XYBox, XYZBox]]): a list of either XYBox or XYZBox. Returns: Tuple[List[Union[XYBox, XYZBox]], List[Union[XYBox, XYZBox]]]: Two lists - new boxes combined and a list of many lists of the grouped boxes. \"\"\" merged = True new_bbs = bbs . copy () bbs_grouped = [[ a ] for a in new_bbs ] while merged : # Create a pairwise matrix of all combinations pairwise = [] # Divide list into pairs for i in range ( 0 , len ( new_bbs ) - 1 ): for j in range ( i + 1 , len ( new_bbs )): pairwise . append (( i , j )) merged = False # Now combine if we can for i , j in pairwise : a = new_bbs [ i ] b = new_bbs [ j ] if bb_overlap ( a , b ): new_bbs . pop ( j ) # Pop j first as it's always larger new_bbs . pop ( i ) c = bb_combine ( a , b ) d = bbs_grouped . pop ( j ) e = bbs_grouped . pop ( i ) f = d + e bbs_grouped . append ( f ) new_bbs . append ( c ) merged = True break del pairwise [:] return new_bbs , bbs_grouped points_to_bb ( points , sonar_range ) Given a list of points, generate the bounding box for them. The list of points is from the db and is (time, sonarid, minbearing, maxbearing, minrange, maxrange, track, uid). All points should be from the same sonar. The max sonar range can change and therefore needs to be passed in. Parameters: points ( List [ Points ] ) \u2013 a list of Points. sonar_range ( float ) \u2013 the range of the sonar at this time. Returns: BearBox \u2013 BearBox Source code in sealhits/bbox.py 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 def points_to_bb ( points : List [ Points ], sonar_range : float ) -> BearBox : \"\"\"Given a list of points, generate the bounding box for them. The list of points is from the db and is (time, sonarid, minbearing, maxbearing, minrange, maxrange, track, uid). All points should be from the same sonar. The max sonar range can change and therefore needs to be passed in. Args: points (List[Points]): a list of Points. sonar_range (float): the range of the sonar at this time. Returns: BearBox \"\"\" # Assume the min/maxes based on the sonar bearing_limits = ( math . radians ( MIN_ANGLE ), math . radians ( MAX_ANGLE )) distance_limits = ( 0.0 , sonar_range ) min_b = bearing_limits [ 1 ] max_b = bearing_limits [ 0 ] min_d = distance_limits [ 1 ] max_d = distance_limits [ 0 ] # TODO - Minbearing and maxbearing have been swapped here! It works but clearly # there is an issue somehere in what min and max actually mean in the PAMGuard # database for point in points : if point . maxbearing < min_b and point . maxbearing >= bearing_limits [ 0 ]: min_b = point . maxbearing if point . minbearing > max_b and point . minbearing < bearing_limits [ 1 ]: max_b = point . minbearing if point . minrange < min_d and point . minrange >= distance_limits [ 0 ]: min_d = point . minrange if point . maxrange > max_d and point . maxrange < distance_limits [ 1 ]: max_d = point . maxrange bb = BearBox ( min_b , max_b , min_d , max_d , sonar_range ) return bb Compression compress.py - image compression and decompression. Functions related to compressing and decompressing images. We use LZ4 as the various python zip routines are quite slow. compress ( data , header , image_path ) Given a FITS hdul, write this out to an lz4 compressed file. We unfortunately need to take the data out of the hdul and put it back in, so we just take the np.array and header as is. Parameters: data ( array ) \u2013 the image to save. header ( PrimaryHDU ) \u2013 the header to add to the FITS file. image_path ( str ) \u2013 the image to save. Source code in sealhits/compress.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 def compress ( data : np . array , header : fits . hdu . image . PrimaryHDU , image_path : str ): ''' Given a FITS hdul, write this out to an lz4 compressed file. We unfortunately need to take the data out of the hdul and put it back in, so we just take the np.array and header as is. Args: data (np.array): the image to save. header (PrimaryHDU): the header to add to the FITS file. image_path (str): the image to save. ''' assert ( os . path . splitext ( image_path )[ 1 ] == \".lz4\" ) with lz4 . frame . open ( image_path , mode = 'wb' ) as fp : fits . writeto ( fp , data , header = header ) decompress ( image_path ) Decompress with LZ4. We have to return the data from within the lz4 context so we assume only one HDU which is true for our data at present. Parameters: image_path ( str ) \u2013 the path to the LZ4 image we want to decompress. Returns: Tuple [ array , PrimaryHDU ] \u2013 Tuple[np.array, fits.hdu.image.PrimaryHDU]: the image as a numpy array, and the FITS Primary HDU. Source code in sealhits/compress.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 def decompress ( image_path : str ) -> Tuple [ np . array , fits . hdu . image . PrimaryHDU ]: ''' Decompress with LZ4. We have to return the data from within the lz4 context so we assume only one HDU which is true for our data at present. Args: image_path (str): the path to the LZ4 image we want to decompress. Returns: Tuple[np.array, fits.hdu.image.PrimaryHDU]: the image as a numpy array, and the FITS Primary HDU. ''' assert ( os . path . exists ( image_path )) assert ( os . path . splitext ( image_path )[ 1 ] == \".lz4\" ) with lz4 . frame . open ( image_path , mode = 'rb' ) as fp : img = fits . open ( fp , memmap = False , lazy_load_hdus = False ) return ( img [ 0 ] . data , img [ 0 ] . header ) Database Object db.py - The Postgresql Database Object. This file holds the class that represents everything about our data. We use SQLAlchemy as the ORM to wrap the various postgresql commands and schemas. DB Our database class that holds the engine and ORM and what not. Source code in sealhits/db/db.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 class DB : \"\"\"Our database class that holds the engine and ORM and what not.\"\"\" # TODO - Import the many functions onto this class (perhaps there is a better way?) from sealhits.db.dbget import ( get_groups , get_num_groups , get_groups_filters , get_pgdf_filename , get_glf_filename , get_group_id , get_group_uid , get_group_huid , get_sqlites , get_points_group , get_pgdfs , get_pgdfs_distinct , get_group_gid_sqlite_id_split , get_group_gid_sqlite_id , get_group_track , get_images_groups , get_tracks_groups , get_tracks_groups_groups_binfile , get_image_points_by_filename , get_image_points_by_filename_group , get_image_groups_by_filename , get_tracks_group_uid , get_points_from_track , get_images_group , get_images_group_sonarid , get_image_uid , get_img_fname ) from sealhits.db.dbset import ( set_group_timestart , set_group_timeend , set_point_track , set_point_group , set_group_split , set_group_huid ) from sealhits.db.dbdel import ( del_by_sqlite ) def __init__ ( self , db_name , username , password , host = \"localhost\" , echo = False ): con_str = ( \"postgresql+psycopg2://\" + username + \":\" + password + \"@\" + host + \"/\" + db_name ) self . engine = create_engine ( con_str , echo = echo ) Database Getters dbget.py - The Postgresql getter functions. These functions retrieve records from the db, performing certain queries to return the data required. get_glf_filename ( self , fname ) Return a single GLF matching the filename. Parameters: fname ( str ) \u2013 the filename to match against. Returns: Union [ GLFS | None] \u2013 Union[GLFS | None]: either the matching GLFS or None. Source code in sealhits/db/dbget.py 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 def get_glf_filename ( self , fname : str ) -> Union [ GLFS | None ]: \"\"\"Return a single GLF matching the filename. Args: fname (str): the filename to match against. Returns: Union[GLFS | None]: either the matching GLFS or None. \"\"\" result = None with Session ( self . engine ) as session : stmt = select ( GLFS ) . where ( GLFS . filename == fname ) result = session . scalars ( stmt ) . one_or_none () return result get_group_gid_sqlite_id ( self , gid , sqlite_name , sqliteid ) Return a Groups object matching the gid, sqliteid and sqlite name. These three fields combine for a unique key - a bit too complicated sadly. We want all the splits. Parameters: gid ( int ) \u2013 the uid to match against. sqlite_name ( str ) \u2013 value matched against the \"sqlite\" field in the Groups Schema. sqliteid ( int ) \u2013 value matched against the \"sqliteid\" field in the Groups Schema. Returns: Union [ List [ Groups ] | None] \u2013 Union[List[Groups] | None] : Either a list of matching Groups or None Source code in sealhits/db/dbget.py 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 def get_group_gid_sqlite_id ( self , gid : int , sqlite_name : str , sqliteid : int ) -> Union [ List [ Groups ] | None ]: \"\"\"Return a Groups object matching the gid, sqliteid and sqlite name. These three fields combine for a unique key - a bit too complicated sadly. We want all the splits. Args: gid (int): the uid to match against. sqlite_name (str): value matched against the \"sqlite\" field in the Groups Schema. sqliteid (int): value matched against the \"sqliteid\" field in the Groups Schema. Returns: Union[List[Groups] | None] : Either a list of matching Groups or None \"\"\" results = None with Session ( self . engine ) as session : results = session . execute ( select ( Groups ) . where ( Groups . gid == gid ) . where ( Groups . sqlite == sqlite_name ) . where ( Groups . sqliteid == sqliteid ) ) . all () results = [ r [ 0 ] for r in results ] return results get_group_gid_sqlite_id_split ( self , gid , sqlite_name , sqliteid , split ) Return a Groups object matching the gid, sqliteid, split and sqlite name. These four fields combine for a unique key. Parameters: gid ( int ) \u2013 the uid to match against. sqlite_name ( str ) \u2013 value matched against the \"sqlite\" field in the Groups Schema. sqliteid ( int ) \u2013 value matched against the \"sqliteid\" field in the Groups Schema. split ( int ) \u2013 value matched against the \"split\" field in the Groups Schema. Returns: Union [ Groups | None] \u2013 Union[Groups | None]: either the matching 'Groups' or None. Source code in sealhits/db/dbget.py 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 def get_group_gid_sqlite_id_split ( self , gid : int , sqlite_name : str , sqliteid : int , split : int ) -> Union [ Groups | None ]: \"\"\"Return a Groups object matching the gid, sqliteid, split and sqlite name. These four fields combine for a unique key. Args: gid (int): the uid to match against. sqlite_name (str): value matched against the \"sqlite\" field in the Groups Schema. sqliteid (int): value matched against the \"sqliteid\" field in the Groups Schema. split (int): value matched against the \"split\" field in the Groups Schema. Returns: Union[Groups | None]: either the matching 'Groups' or None. \"\"\" result = None with Session ( self . engine ) as session : result = session . execute ( select ( Groups ) . where ( Groups . gid == gid ) . where ( Groups . sqlite == sqlite_name ) . where ( Groups . sqliteid == sqliteid ) . where ( Groups . split == split ) ) . scalar_one_or_none () return result get_group_huid ( self , huid ) Return a Groups object matching the human uid. Should be only one but we can't be sure. Parameters: huid ( str ) \u2013 the huid to match against. Returns: Union [ Groups | None] \u2013 Union[Groups | None]: either the matching 'Groups' or None. Source code in sealhits/db/dbget.py 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def get_group_huid ( self , huid : str ) -> Union [ Groups | None ]: \"\"\"Return a Groups object matching the human uid. Should be only one but we can't be sure. Args: huid (str): the huid to match against. Returns: Union[Groups | None]: either the matching 'Groups' or None. \"\"\" result = None with Session ( self . engine ) as session : result = session . execute ( select ( Groups ) . where ( Groups . huid == huid ) ) . scalar_one_or_none () return result get_group_id ( self , id ) Get a group with either a string UID, or a HUID. Parameters: id ( str ) \u2013 the id either as a UUID or a HUID. Returns: Union [ Groups | None] \u2013 Union[Groups | None]: either the matching 'Groups' or None. Source code in sealhits/db/dbget.py 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 def get_group_id ( self , id : str ) -> Union [ Groups | None ]: \"\"\"Get a group with either a string UID, or a HUID. Args: id (str): the id either as a UUID or a HUID. Returns: Union[Groups | None]: either the matching 'Groups' or None. \"\"\" try : uid = uuid . UUID ( id ) return self . get_group_uid ( uid ) except ValueError : return self . get_group_huid ( id ) get_group_track ( self , track_id ) Return a group, given a track uid. Parameters: track_id ( uuid4 ) \u2013 the uid of the track to match against. Returns: Union [ Groups | None] \u2013 Union[Groups | None]: either the matching 'Groups' or None. Source code in sealhits/db/dbget.py 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 def get_group_track ( self , track_id : uuid . uuid4 ) -> Union [ Groups | None ]: \"\"\"Return a group, given a track uid. Args: track_id (uuid.uuid4): the uid of the track to match against. Returns: Union[Groups | None]: either the matching 'Groups' or None. \"\"\" result = None with Session ( self . engine ) as session : result = session . execute ( select ( Groups ) . join ( TrackGroup ) . where ( Groups . uid == TrackGroup . group_id ) . filter ( TrackGroup . track_id == track_id ) ) . one_or_none () if result is not None : return result [ 0 ] return result get_group_uid ( self , uid ) Return a Groups object matching the uid. Parameters: uid ( uuid4 ) \u2013 the uid to match against. Returns: Union [ Groups | None] \u2013 Union[Groups | None]: either the matching 'Groups' or None. Source code in sealhits/db/dbget.py 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 def get_group_uid ( self , uid : uuid . uuid4 ) -> Union [ Groups | None ]: \"\"\"Return a Groups object matching the uid. Args: uid (uuid.uuid4): the uid to match against. Returns: Union[Groups | None]: either the matching 'Groups' or None. \"\"\" result = None with Session ( self . engine ) as session : result = session . execute ( select ( Groups ) . where ( Groups . uid == uid ) ) . scalar_one_or_none () return result get_groups ( self , code = None ) return all the groups, ordered by the pamguard sqlite uid ascending. Optionally a search can be made using a code. Parameters: code ( str , default: None ) \u2013 a string to match against the 'code' field on groups. Returns: List [ Groups ] \u2013 List[Groups]: the resulting list of 'Groups' Source code in sealhits/db/dbget.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 def get_groups ( self , code = None ) -> List [ Groups ]: \"\"\"return all the groups, ordered by the pamguard sqlite uid ascending. Optionally a search can be made using a code. Args: code (str): a string to match against the 'code' field on groups. Returns: List[Groups]: the resulting list of 'Groups' \"\"\" results = [] with Session ( self . engine ) as session : if code is not None : results = ( session . query ( Groups ) . where ( Groups . code == code ) . order_by ( Groups . gid . asc ()) . all () ) else : results = session . query ( Groups ) . order_by ( Groups . gid . asc ()) . all () return results get_groups_filters ( self , filters ) Return groups using a number of filters that the user can pass in. Filters is a list or tuple containing an appropriate filters for groups such as Groups.mammal > 1. Filters must be a list or tuple, not a single filter on it's own (though a single filter can be used so long as it's inside a list.) Parameters: filters ( List[] ) \u2013 as list of 'filters' such as 'Groups.mammal > 1' Returns: List [ Groups ] \u2013 List[Groups]: the resulting list of 'Groups' Source code in sealhits/db/dbget.py 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 def get_groups_filters ( self , filters ) -> List [ Groups ]: \"\"\"Return groups using a number of filters that the user can pass in. Filters is a list or tuple containing an appropriate filters for groups such as Groups.mammal > 1. Filters must be a list or tuple, not a single filter on it's own (though a single filter can be used so long as it's inside a list.) Args: filters (List[]): as list of 'filters' such as 'Groups.mammal > 1' Returns: List[Groups]: the resulting list of 'Groups' \"\"\" results = [] with Session ( self . engine ) as session : q = session . query ( Groups ) . filter ( * filters ) . all () results = q return results get_image_groups_by_filename ( self , filename ) Return a list of all the groups in the image with this filename. Parameters: filename ( str ) \u2013 the image filename to check against. Returns: List [ Groups ] \u2013 List[Groups]: List of matching Groups. Source code in sealhits/db/dbget.py 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 def get_image_groups_by_filename ( self , filename : str ) -> List [ Groups ]: \"\"\"Return a list of all the groups in the image with this filename. Args: filename (str): the image filename to check against. Returns: List[Groups]: List of matching Groups. \"\"\" results = [] with Session ( self . engine ) as session : try : results = ( session . execute ( select ( Groups ) . join_from ( Groups , groups_images ) . join_from ( groups_images , Images ) . filter ( Images . filename == filename ) ) . scalars () . all () ) except IntegrityError as e : print ( \"get_images_groups failed\" , e ) return results get_image_points_by_filename ( self , imagefile ) Get all the points for a particular image regardless of group. Parameters: imagefile ( str ) \u2013 the name of the imagefile to match against. Returns: List [ Points ] \u2013 List[Points]: List of matching Points. Source code in sealhits/db/dbget.py 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 def get_image_points_by_filename ( self , imagefile : str ) -> List [ Points ]: \"\"\"Get all the points for a particular image regardless of group. Args: imagefile (str): the name of the imagefile to match against. Returns: List[Points]: List of matching Points. \"\"\" results = [] with Session ( self . engine ) as session : results = ( session . execute ( select ( Points ) . join_from ( TrackGroup , Points ) . join_from ( TrackGroup , Groups ) . join_from ( Groups , groups_images ) . join_from ( groups_images , Images ) . filter ( Images . hastrack is True ) . filter ( Points . sonarid == Images . sonarid ) . filter ( Images . filename == imagefile ) ) . scalars () . all () ) return results get_image_points_by_filename_group ( self , imagefile , group_id ) Get all the tracks in an annotated group for this single image. We use the groups_key on points to do the matching. Parameters: imagefile ( str ) \u2013 the image filename to check against. group_id ( Union [ UUID , str ] ) \u2013 group to match against, either with a uuid or huid string. Returns: List [ Points ] \u2013 List[Points]: List of matching Points. Source code in sealhits/db/dbget.py 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 def get_image_points_by_filename_group ( self , imagefile : str , group_id : Union [ uuid . UUID , str ] ) -> List [ Points ]: \"\"\"Get all the tracks in an annotated group for this single image. We use the groups_key on points to do the matching. Args: imagefile (str): the image filename to check against. group_id (Union[uuid.UUID, str]): group to match against, either with a uuid or huid string. Returns: List[Points]: List of matching Points. \"\"\" results = [] filters = [] if type ( group_id ) is uuid . UUID : filters . append ( Groups . uid == group_id ) else : filters . append ( Groups . huid == group_id ) with Session ( self . engine ) as session : results = ( session . execute ( select ( Points ) . join_from ( Groups , Points ) . join_from ( Groups , groups_images ) . join_from ( groups_images , Images ) . filter ( * filters ) . filter ( Points . group_id == Groups . uid ) . filter ( Images . filename == imagefile ) . filter ( Points . time == Images . time ) . filter ( Points . sonarid == Images . sonarid ) ) . scalars () . all () ) return results get_image_uid ( self , image_uid ) Does this image exist already? Return it if it does or None if not. Parameters: image_uid ( uuid4 ) \u2013 the uid of the image Returns: Union [ Images | None] \u2013 Union[Images | None]: List of matching Images or None. Source code in sealhits/db/dbget.py 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 def get_image_uid ( self , image_uid : uuid . uuid4 ) -> Union [ Images | None ]: \"\"\"Does this image exist already? Return it if it does or None if not. Args: image_uid (uuid.uuid4): the uid of the image Returns: Union[Images | None]: List of matching Images or None. \"\"\" result = None try : with Session ( self . engine ) as session : result = session . query ( Images ) . where ( Images . uid == image_uid ) . first () except IntegrityError as e : print ( \"get_image_uid failed\" , e ) return result get_images_group ( self , group_id ) Given a group uid return all the images for this group in order. Group_uid can be either uuid.UUID4 or a string representing the huid. Parameters: group_id ( Union [ str , UUID ] ) \u2013 either the huid (str) or uid (uuid) of the group. Returns: Union [ List [ Images ], None] \u2013 Union[List[Images], None]: List of matching Images or None. Source code in sealhits/db/dbget.py 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 def get_images_group ( self , group_id : Union [ str , uuid . UUID ]) -> Union [ List [ Images ], None ]: \"\"\"Given a group uid return all the images for this group in order. Group_uid can be either uuid.UUID4 or a string representing the huid. Args: group_id (Union[str, uuid.UUID]): either the huid (str) or uid (uuid) of the group. Returns: Union[List[Images], None]: List of matching Images or None. \"\"\" results = None filters = [] if type ( group_id ) is uuid . UUID : filters . append ( Groups . uid == group_id ) else : filters . append ( Groups . huid == group_id ) with Session ( self . engine ) as session : try : results = ( session . execute ( select ( Images ) . join_from ( Groups , groups_images ) . join_from ( groups_images , Images ) . filter ( * filters ) . order_by ( Images . time . asc ()) ) . scalars () . all () ) except IntegrityError as e : print ( \"get_images_group failed\" , e ) return results get_images_group_sonarid ( self , group_id , sonar_id ) Given a group uid and a sonarid, return all the images for this group in order. Group_id is either a uuid.uuid4 or a huid string. Parameters: group_id ( Union [ str , UUID ] ) \u2013 either the huid (str) or uid (uuid) of the group. sonar_id ( int ) \u2013 the sonar id to check against. Returns: Union [ List [ Images ], None] \u2013 Union[List[Images], None]: List of matching Images or None. Source code in sealhits/db/dbget.py 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 def get_images_group_sonarid ( self , group_id , sonar_id : int ) -> Union [ List [ Images ], None ]: \"\"\"Given a group uid and a sonarid, return all the images for this group in order. Group_id is either a uuid.uuid4 or a huid string. Args: group_id (Union[str, uuid.UUID]): either the huid (str) or uid (uuid) of the group. sonar_id (int): the sonar id to check against. Returns: Union[List[Images], None]: List of matching Images or None. \"\"\" results = None filters = [] if type ( group_id ) is uuid . UUID : filters . append ( Groups . uid == group_id ) else : filters . append ( Groups . huid == group_id ) filters . append ( Images . sonarid == sonar_id ) with Session ( self . engine ) as session : try : results = ( session . execute ( select ( Images ) . join_from ( Groups , groups_images ) . join_from ( groups_images , Images ) . filter ( * filters ) . order_by ( Images . time . asc ()) ) . scalars () . all () ) except IntegrityError as e : print ( \"get_images_group failed\" , e ) return results get_images_groups ( self , sonar_id ) Return a list of all the images that have a group within them that is annotated for a particular sonar. It need not have an actual track within the image, therefore buffer images and images appearing between tracks appear as well. Parameters: sonar_id ( int ) \u2013 the sonar id to match against. Returns: List [ Groups ] \u2013 List[Groups]: List of matching Groups Source code in sealhits/db/dbget.py 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 def get_images_groups ( self , sonar_id : int ) -> List [ Groups ]: \"\"\"Return a list of all the images that have a group within them that is annotated for a particular sonar. It need not have an actual track within the image, therefore buffer images and images appearing between tracks appear as well. Args: sonar_id (int): the sonar id to match against. Returns: List[Groups]: List of matching Groups \"\"\" results = [] with Session ( self . engine ) as session : results = session . execute ( select ( Images , Groups . uid ) . join ( Images . groups ) . where ( Images . sonarid == sonar_id ) ) . all () return results get_img_fname ( self , image_fname ) Does this image exist already? Return it if it does or None if not. Parameters: image_fname ( str ) \u2013 the filename of the image (minus the .lz4) Returns: Union [ Images , None] \u2013 Union[Images | None]: List of matching Images or None. Source code in sealhits/db/dbget.py 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 def get_img_fname ( self , image_fname : str ) -> Union [ Images , None ]: \"\"\"Does this image exist already? Return it if it does or None if not. Args: image_fname (str): the filename of the image (minus the .lz4) Returns: Union[Images | None]: List of matching Images or None. \"\"\" result = None try : with Session ( self . engine ) as session : result = session . query ( Images ) . where ( Images . filename == image_fname ) . first () except MultipleResultsFound as e : print ( \"get_img_fname failed - result is not unique\" , e ) except NoResultFound : pass # Okay to pass here as this result is valid except IntegrityError as e : print ( \"get_img_fname failed\" , e ) return result get_num_groups ( self , code = None , originals = False ) Return the count of the groups. Optionally a search can be made using a code. Optionally, Originals means these with a split number of 0, i.e the group from PAMGuard before it was split by ingest.py. Parameters: code ( str , default: None ) \u2013 a string that matches against the 'code' field on Groups. originals ( bool , default: False ) \u2013 return only these groups that were not generated with a split. Returns: int ( int ) \u2013 the number of groups Source code in sealhits/db/dbget.py 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 def get_num_groups ( self , code = None , originals = False ) -> int : \"\"\"Return the count of the groups. Optionally a search can be made using a code. Optionally, Originals means these with a split number of 0, i.e the group from PAMGuard before it was split by ingest.py. Args: code (str): a string that matches against the 'code' field on Groups. originals (bool): return only these groups that were not generated with a split. Returns: int: the number of groups \"\"\" res = 0 # TODO - this is a bit verbose. Maybe compose where clauses? with Session ( self . engine ) as session : if code is not None : if originals : res = int ( session . query ( Groups ) . where ( Groups . code == code ) . where ( Groups . split == 0 ) . count () ) else : res = int ( session . query ( Groups ) . where ( Groups . code == code ) . count ()) else : if originals : res = int ( session . query ( Groups ) . where ( Groups . split == 0 ) . count ()) else : res = int ( session . query ( Groups ) . count ()) return res get_pgdf_filename ( self , fname ) Return a single PGDF matching the filename. Parameters: fname ( str ) \u2013 the filename to match against. Returns: Union [ PGDFS | None] \u2013 Union[PGDFS | None]: either the matching PGDFS or None. Source code in sealhits/db/dbget.py 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 def get_pgdf_filename ( self , fname : str ) -> Union [ PGDFS | None ]: \"\"\"Return a single PGDF matching the filename. Args: fname (str): the filename to match against. Returns: Union[PGDFS | None]: either the matching PGDFS or None. \"\"\" result = None with Session ( self . engine ) as session : stmt = select ( PGDFS ) . where ( PGDFS . filename == fname ) result = session . scalars ( stmt ) . one_or_none () return result get_pgdfs ( self , limit =- 1 ) Return all the PGDFs from the DB, with optional limit. Parameters: limit ( int , default: -1 ) \u2013 an optional limit on how many to return. -1 means no limit. Returns: List [ PGDFS ] \u2013 List[PGDFS]: the resulting list of 'PGDFS' Source code in sealhits/db/dbget.py 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 def get_pgdfs ( self , limit =- 1 ) -> List [ PGDFS ]: \"\"\"Return all the PGDFs from the DB, with optional limit. Args: limit (int): an optional limit on how many to return. -1 means no limit. Returns: List[PGDFS]: the resulting list of 'PGDFS' \"\"\" results = [] with Session ( self . engine ) as session : if limit > 0 : results = ( session . query ( PGDFS ) . order_by ( PGDFS . startdate . asc ()) . limit ( limit ) . all () ) else : results = session . query ( PGDFS ) . order_by ( PGDFS . startdate . asc ()) . all () return results get_pgdfs_distinct ( self ) Return all the PGDFs with no duplicates. Why there should be duplicates I am not sure. Returns: List [ PGDFS ] \u2013 List[PGDFS]: the resulting list of 'PGDFS' Source code in sealhits/db/dbget.py 163 164 165 166 167 168 169 170 171 172 173 174 175 def get_pgdfs_distinct ( self ) -> List [ PGDFS ]: \"\"\"Return all the PGDFs with no duplicates. Why there should be duplicates I am not sure. Returns: List[PGDFS]: the resulting list of 'PGDFS' \"\"\" results = [] with Session ( self . engine ) as session : results = session . query ( PGDFS ) . order_by ( PGDFS . startdate . asc ()) . distinct () . all () return results get_points_from_track ( self , track_uid ) Get all the points for a particular track, given the track_uid. Returns a list of tuple - (time, sonarid, minbearing, maxbearing, minrange, maxrange, track, uid). Parameters: track_uid ( UUID ) \u2013 the uid of the track Returns: List [ Points ] \u2013 List[Points]: List of matching Points. Source code in sealhits/db/dbget.py 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 def get_points_from_track ( self , track_uid : uuid . UUID ) -> List [ Points ]: \"\"\" Get all the points for a particular track, given the track_uid. Returns a list of tuple - (time, sonarid, minbearing, maxbearing, minrange, maxrange, track, uid). Args: track_uid (uuid.UUID): the uid of the track Returns: List[Points]: List of matching Points. \"\"\" results = [] with Session ( self . engine ) as session : try : q = session . query ( Points ) . filter ( Points . track_id == track_uid ) . all () results = q except IntegrityError as e : print ( \"get_group_tracks_uid failed\" , e ) return results get_points_group ( self , group_id ) Get all the points for this group. group_id can either be a uuid.uuid4 or it can be huid string. Parameters: group_id ( Union [ UUID , str ] ) \u2013 Either a uuid for uid, or a string for huid matching. Returns: List [ Points ] \u2013 List[Points]: List of matching Points. Source code in sealhits/db/dbget.py 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 def get_points_group ( self , group_id : Union [ uuid . UUID , str ]) -> List [ Points ]: \"\"\"Get all the points for this group. group_id can either be a uuid.uuid4 or it can be huid string. Args: group_id (Union[uuid.UUID, str]): Either a uuid for uid, or a string for huid matching. Returns: List[Points]: List of matching Points. \"\"\" results = [] group_uid = None if type ( group_id ) is uuid . UUID : group_uid = group_id else : g = self . get_group_huid ( group_id ) group_uid = g . uid with Session ( self . engine ) as session : q = session . query ( Points ) . filter ( Points . group_id == group_uid ) . all () results = q return results get_sqlites ( self ) Return the SQLites we have considered. Returns: List [ str ] \u2013 List[str]: List of sqlites in the database as strings. Source code in sealhits/db/dbget.py 456 457 458 459 460 461 462 463 464 465 466 def get_sqlites ( self ) -> List [ str ]: \"\"\"Return the SQLites we have considered. Returns: List[str]: List of sqlites in the database as strings. \"\"\" results = [] with Session ( self . engine ) as session : results = session . execute ( select ( Groups . sqlite ) . distinct ()) . scalars () . all () return results get_tracks_group_uid ( self , group_id ) Get all the tracks in a particular group. Parameters: group_id ( Union [ UUID , str ] ) \u2013 group to match against, either with a uuid or huid string. Returns: List [ TrackGroup ] \u2013 List[TrackGroup]: List of matching TrackGroup. Source code in sealhits/db/dbget.py 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 def get_tracks_group_uid ( self , group_id : Union [ uuid . UUID , str ]) -> List [ TrackGroup ]: \"\"\"Get all the tracks in a particular group. Args: group_id (Union[uuid.UUID, str]): group to match against, either with a uuid or huid string. Returns: List[TrackGroup]: List of matching TrackGroup. \"\"\" results = [] group_uid = None if type ( group_id ) is uuid . UUID : group_uid = group_id else : g = self . get_group_huid ( group_id ) group_uid = g . uid with Session ( self . engine ) as session : try : q = session . query ( TrackGroup ) . filter ( TrackGroup . group_id == group_uid ) . all () results = q except IntegrityError as e : print ( \"get_group_tracks_uid failed\" , e ) return results get_tracks_groups ( self ) Return all the track groups table. Args: None Returns: List [ TrackGroup ] \u2013 List[TrackGroup]: List of matching TrackGroup. Source code in sealhits/db/dbget.py 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 def get_tracks_groups ( self ) -> List [ TrackGroup ]: \"\"\"Return all the track groups table. Args: None Returns: List[TrackGroup]: List of matching TrackGroup. \"\"\" results = [] with Session ( self . engine ) as session : stmt = select ( TrackGroup ) results = session . scalars ( stmt ) . all () return results get_tracks_groups_groups_binfile ( self , group_uids , binfile ) Return all the track groups table that match the group uids and pgdf binary file. Parameters: group_uids ( List [ UUID ] ) \u2013 a list of uids to match against. binfile ( str ) \u2013 a string that represents the binary file. Matched against binfile. Returns: List [ TrackGroup ] \u2013 List[TrackGroup]: List of matching TrackGroup. Source code in sealhits/db/dbget.py 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 def get_tracks_groups_groups_binfile ( self , group_uids : List [ uuid . UUID ], binfile : str ) -> List [ TrackGroup ]: \"\"\"Return all the track groups table that match the group uids and pgdf binary file. Args: group_uids (List[uuid.UUID]): a list of uids to match against. binfile (str): a string that represents the binary file. Matched against binfile. Returns: List[TrackGroup]: List of matching TrackGroup. \"\"\" results = [] with Session ( self . engine ) as session : q = ( session . query ( TrackGroup ) . filter ( TrackGroup . group_id . in_ ( group_uids )) . filter ( TrackGroup . binfile == binfile ) . all () ) results = q return results Database model dbmodel.py - The Postgresql database model. The model of the database, with all the objects loaded into a session for comparison. DBModel holds a representation of the database in order to compare against the real database. This comparison is used when updating the real database with new information. DBModel Simmple struct like class to hold the model. Source code in sealhits/db/dbmodel.py 21 22 23 24 25 26 27 28 29 class DBModel : \"\"\" Simmple struct like class to hold the model.\"\"\" def __init__ ( self ): self . groups = [] self . points = [] self . images = [] self . pgdfs = [] self . glfs = [] self . track_groups = [] diff_models ( session , model_a , model_b ) Compare two models - find the objects that do not exist in model_b but do exist in model_a and therefore should be removed from model_a. Parameters: session ( Session ) \u2013 the current sqlalchemy session. model_a ( DBModel ) \u2013 the first model of the database. model_b ( DBModel ) \u2013 the second model to compare against the first. Returns: DBModel ( DBModel ) \u2013 a model of the differences. Source code in sealhits/db/dbmodel.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 def diff_models ( session : Session , model_a : DBModel , model_b : DBModel ) -> DBModel : \"\"\" Compare two models - find the objects that do not exist in model_b but do exist in model_a and therefore should be removed from model_a. Args: session (Session): the current sqlalchemy session. model_a (DBModel): the first model of the database. model_b (DBModel): the second model to compare against the first. Returns: DBModel: a model of the differences. \"\"\" model_diff = DBModel () # Start with groups groups_a = { a . uid for a in model_a . groups } groups_b = { b . uid for b in model_b . groups } groups_diff = groups_a - groups_b for uid in groups_diff : tt = ( session . query ( Groups ) . filter ( Groups . uid == uid ) . one () ) model_diff . groups . append ( tt ) # Now Points points_a = { a . uid for a in model_a . points } points_b = { b . uid for b in model_b . points } points_diff = points_a - points_b for uid in points_diff : tt = ( session . query ( Points ) . filter ( Points . uid == uid ) . one () ) model_diff . points . append ( tt ) # Images images_a = { a . uid for a in model_a . images } images_b = { b . uid for b in model_b . images } images_diff = images_a - images_b for uid in images_diff : tt = ( session . query ( Images ) . filter ( Images . uid == uid ) . one () ) model_diff . images . append ( tt ) # PGDFS pgdfs_a = { a . uid for a in model_a . pgdfs } pgdfs_b = { b . uid for b in model_b . pgdfs } pgdfs_diff = pgdfs_a - pgdfs_b for uid in pgdfs_diff : tt = ( session . query ( PGDFS ) . filter ( PGDFS . uid == uid ) . one () ) model_diff . pgdfs . append ( tt ) # GLFS glfs_a = { a . uid for a in model_a . glfs } glfs_b = { b . uid for b in model_b . glfs } glfs_diff = glfs_a - glfs_b for uid in glfs_diff : tt = ( session . query ( GLFS ) . filter ( GLFS . uid == uid ) . one () ) model_diff . glfs . append ( tt ) # TrackGroups tg_a = { a . track_id for a in model_a . track_groups } tg_b = { b . track_id for b in model_b . track_groups } tg_diff = tg_a - tg_b for uid in tg_diff : tt = ( session . query ( TrackGroup ) . filter ( TrackGroup . track_id == uid ) . one () ) model_diff . track_groups . append ( tt ) return model_diff gen_model ( session , sqlname ) Return the objects in our ORM model for a particular sqlname, within a session. Mostly this is used to compare against a recent ingest to see what needs to be deleted. Parameters: session ( Session ) \u2013 the current sqlalchemy session. sqlname ( str ) \u2013 The name of the sqlite file of the recent ingest. Returns: DBModel ( DBModel ) \u2013 the generated model reflecting just the single sqlite ingest. Source code in sealhits/db/dbmodel.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 def gen_model ( session : Session , sqlname : str ) -> DBModel : \"\"\" Return the objects in our ORM model for a particular sqlname, within a session. Mostly this is used to compare against a recent ingest to see what needs to be deleted. Args: session (Session): the current sqlalchemy session. sqlname (str): The name of the sqlite file of the recent ingest. Returns: DBModel: the generated model reflecting just the single sqlite ingest. \"\"\" model = DBModel () with session . no_autoflush : model . groups = session . query ( Groups ) . filter ( Groups . sqlite == sqlname ) . all () for group in model . groups : model . images += group . images model . pgdfs += group . pgdfs model . glfs += group . glfs tg = ( session . query ( TrackGroup ) . filter ( TrackGroup . group_id == group . uid ) . all () ) model . track_groups += tg tp = ( session . query ( Points ) . filter ( Points . group_id == group . uid ) . all () ) model . points += tp return model Database schema dbschema.py - The Postgresql schema. author: Benjamin Blundell (bjb8@st-andrews.ac.uk) The Postgresql Database Schema. We use SQLAlchemy as the ORM to wrap the various postgresql. TrackGroup Bases: Base Represents the tracks that are part of this group. We use our own UUID but the constraint is that track_id and binfile together must be unique. Source code in sealhits/db/dbschema.py 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 class TrackGroup ( Base ): \"\"\"Represents the tracks that are part of this group. We use our own UUID but the constraint is that track_id and binfile together must be unique.\"\"\" __tablename__ = \"tracks_groups\" track_pam_id : Mapped [ int ] track_id = Column ( UUID ( as_uuid = True ), primary_key = True ) group_id = Column ( UUID ( as_uuid = True ), ForeignKey ( Groups . uid ) ) # TODO - foreign relation here? binfile : Mapped [ str ] def __repr__ ( self ) -> str : return f \"TrackGroup(track_id= { self . track_id !r} , group_id= { self . group_id !r} )\" Database searches dbsearch.py - The Postgresql search functions. Various functions for finding and generating data from our images and database. gid_to_fans_imgs ( uid , db , fits_path , sonar_id , scale_factor = 1.0 , limit =- 1 ) Given a GID, DB, path to fits files and sonarid, return the fan images for a particular group along with the filename path. The fan image sizes are created using the heighest resolution. Parameters: uid ( str ) \u2013 uid or huid of the group as a string. db ( DB ) \u2013 The active database object. fits_path ( str ) \u2013 The path the fits files. sonar_id ( int ) \u2013 The sonar_id. scale_factor ( float , default: 1.0 ) \u2013 optional scaling factor for the fan images. limit ( int , default: -1 ) \u2013 optional number of results to process (-1 is no limit) Returns: List [ Tuple [ array , Images ]] \u2013 List[Tuple[np.array, Images]]: a list of both the fans and their associate Images records. Source code in sealhits/db/dbsearch.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def gid_to_fans_imgs ( uid : str , db : DB , fits_path : str , sonar_id : int , scale_factor = 1.0 , limit =- 1 ) -> List [ Tuple [ np . array , Images ]]: \"\"\" Given a GID, DB, path to fits files and sonarid, return the fan images for a particular group along with the filename path. The fan image sizes are created using the heighest resolution. Args: uid (str): uid or huid of the group as a string. db (DB): The active database object. fits_path (str): The path the fits files. sonar_id (int): The sonar_id. scale_factor (float): optional scaling factor for the fan images. limit (int): optional number of results to process (-1 is no limit) Returns: List[Tuple[np.array, Images]]: a list of both the fans and their associate Images records. \"\"\" current_frames = [] results = db . get_images_group ( uid , sonar_id ) if results is not None : if limit > 0 : results = results [: limit ] # Do the resize only once - All images in the group # should be the same. It'd be bad otherwise fname = results [ 0 ] . filename fresult = utils . fast_find ( fname , fits_path ) hdul = fits . open ( fresult ) fits_height = int ( hdul [ 0 ] . data . shape [ 0 ]) if scale_factor != 1.0 : fits_height = int ( math . floor ( fits_height * scale_factor )) for idx , img in enumerate ( tqdm ( results )): fname = img . filename fresult = utils . fast_find ( fname , fits_path ) if fresult is not None : # Start with the sonar image # We need flip up down and left right! hdul = fits . open ( fresult ) fan_image = image . fan_distort ( hdul [ 0 ] . data , fits_height , btable . bearing_table ) current_frames . append ( np . fliplr ( np . flipud ( fan_image ))) return zip ( current_frames , results ) Database setters dbset.py - The Postgresql set functions. The Postgresql Database SET functions. These functions set records in the db. set_group_huid ( self , group_uid , new_huid ) Set a human readable uid. Args: group_uid (uuid.uuid4): uid of the group new_huid(str): the new human readable ID (huid) Source code in sealhits/db/dbset.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 def set_group_huid ( self , group_uid : uuid . uuid4 , new_huid : str ): \"\"\"Set a human readable uid. Args: group_uid (uuid.uuid4): uid of the group new_huid(str): the new human readable ID (huid) \"\"\" with Session ( self . engine ) as session : try : session . query ( Groups ) . filter ( Groups . uid == group_uid ) . update ( { \"huid\" : new_huid } ) session . commit () except IntegrityError as e : print ( \"set_group_huid failed\" , e ) set_group_split ( self , group_uid , new_split ) Change the group split. Useful when splitting the group. Parameters: group_uid ( uuid4 ) \u2013 uid of the group new_split ( int ) \u2013 the new datetime for the group start. Source code in sealhits/db/dbset.py 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 def set_group_split ( self , group_uid : uuid . uuid4 , new_split : int ): \"\"\"Change the group split. Useful when splitting the group. Args: group_uid (uuid.uuid4): uid of the group new_split (int): the new datetime for the group start. \"\"\" with Session ( self . engine ) as session : try : session . query ( Groups ) . filter ( Groups . uid == group_uid ) . update ( { \"split\" : new_split } ) session . commit () except IntegrityError as e : print ( \"set_group_split failed\" , e ) set_group_timeend ( self , group_uid , new_timeend ) Change the group timeend value. Useful when splitting the group. Parameters: group_uid ( uuid4 ) \u2013 uid of the group new_timeend ( datetime ) \u2013 the new datetime for the group end. Source code in sealhits/db/dbset.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 def set_group_timeend ( self , group_uid : uuid . uuid4 , new_timeend : datetime . datetime ): \"\"\"Change the group timeend value. Useful when splitting the group. Args: group_uid (uuid.uuid4): uid of the group new_timeend (datetime.datetime): the new datetime for the group end. \"\"\" with Session ( self . engine ) as session : try : session . query ( Groups ) . filter ( Groups . uid == group_uid ) . update ( { \"timeend\" : new_timeend } ) session . commit () except IntegrityError as e : print ( \"set_group_timeend failed\" , e ) set_group_timestart ( self , group_uid , new_timestart ) Change the group timestart value. Useful when splitting the group. Parameters: group_uid ( uuid4 ) \u2013 uid of the group new_timestart ( datetime ) \u2013 the new datetime for the group start. Source code in sealhits/db/dbset.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def set_group_timestart ( self , group_uid : uuid . uuid4 , new_timestart : datetime . datetime ): \"\"\"Change the group timestart value. Useful when splitting the group. Args: group_uid (uuid.uuid4): uid of the group new_timestart (datetime.datetime): the new datetime for the group start. \"\"\" with Session ( self . engine ) as session : try : session . query ( Groups ) . filter ( Groups . uid == group_uid ) . update ( { \"timestart\" : new_timestart } ) session . commit () except IntegrityError as e : print ( \"set_group_timeend failed\" , e ) set_point_group ( self , point_id , new_group ) Change the group that this point belongs to. Parameters: point_id ( uuid4 ) \u2013 uid of the point new_group ( uuid4 ) \u2013 the new group this point belongs to. Source code in sealhits/db/dbset.py 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 def set_point_group ( self , point_id : uuid . uuid4 , new_group : uuid . uuid4 ): \"\"\"Change the group that this point belongs to. Args: point_id (uuid.uuid4): uid of the point new_group (uuid.uuid4): the new group this point belongs to. \"\"\" with Session ( self . engine ) as session : try : session . query ( Points ) . filter ( Points . uid == point_id ) . update ( { \"group_id\" : new_group } ) session . commit () except IntegrityError as e : print ( \"set_point_group failed\" , e ) set_point_track ( self , point_uid , new_track ) Change the track that this point belongs to. Parameters: point_uid ( uuid4 ) \u2013 uid of the point new_track ( uuid4 ) \u2013 the new track this point belongs to. Source code in sealhits/db/dbset.py 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 def set_point_track ( self , point_uid : uuid . uuid4 , new_track : uuid . uuid4 ): \"\"\"Change the track that this point belongs to. Args: point_uid (uuid.uuid4): uid of the point new_track (uuid.uuid4): the new track this point belongs to. \"\"\" with Session ( self . engine ) as session : try : session . query ( Points ) . filter ( Points . uid == point_uid ) . update ( { \"track_id\" : new_track } ) session . commit () except IntegrityError as e : print ( \"set_point_track failed\" , e ) Database updates dbupdate.py - The Postgresql update functions. The Postgresql Database update functions. Updating the objects that may already exist. update_group ( self , new_group ) Update a single group. UID is the main key ingredient. We ignore foreign keys for now however. This function only updates the local group. Parameters: new_group ( Groups ) \u2013 the new Groups data. We update based on the uid. Source code in sealhits/db/dbup.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def update_group ( self , new_group : Groups ): \"\"\" Update a single group. UID is the main key ingredient. We ignore foreign keys for now however. This function only updates the local group. Args: new_group (Groups): the new Groups data. We update based on the uid. \"\"\" with Session ( self . engine ) as session : exist_group = session . execute ( select ( Groups ) . where ( Groups . uid == new_group . uid ) ) . scalar_one_or_none () if exist_group is not None : exist_group . bird = new_group . bird exist_group . code = new_group . code exist_group . comment = new_group . comment exist_group . fish = new_group . fish exist_group . gid = new_group . gid exist_group . glfs = new_group . glfs exist_group . huid = new_group . huid exist_group . interact = new_group . interact exist_group . mammal = new_group . mammal exist_group . sqlite = new_group . sqlite exist_group . sqliteid = new_group . sqliteid exist_group . split = new_group . split exist_group . timestart = new_group . timestart exist_group . timeend = new_group . timeend session . commit () Image functions image.py - functions for reading and drawing images. Functions related to drawing out images. draw_bb ( image , bb , colour = '#1111ff' ) Given an RGB Fan distorted image and a set of bb coords, draw a bounding box. Parameters: image ( Image ) \u2013 the PIL Image to draw on. bb ( XYBox ) \u2013 the box to draw. colour ( str , default: '#1111ff' ) \u2013 the colour of the box as an '#rrggbb' string. Source code in sealhits/image.py 34 35 36 37 38 39 40 41 42 43 44 45 46 def draw_bb ( image : Image , bb : XYBox , colour = \"#1111ff\" ): \"\"\"Given an RGB Fan distorted image and a set of bb coords, draw a bounding box. Args: image (PIL.Image): the PIL Image to draw on. bb (XYBox): the box to draw. colour (str): the colour of the box as an '#rrggbb' string. \"\"\" assert image . mode == \"RGB\" draw = ImageDraw . Draw ( image ) (( x0 , y0 ), ( x1 , y1 )) = bb . pair () xy = (( x0 , y0 ), ( x0 , y1 ), ( x1 , y1 ), ( x1 , y0 )) draw . polygon ( xy , fill = None , outline = colour ) draw_text ( image , pos , colour = '#1111ff' , text = 'none' ) Draw some text on our image. Parameters: image ( Image ) \u2013 the PIL Image to draw on. pos ( Tuple [ int , int ] ) \u2013 the coordinates to draw at. colour ( str , default: '#1111ff' ) \u2013 the colour of the box as an '#rrggbb' string. text ( str , default: 'none' ) \u2013 the text to draw. Source code in sealhits/image.py 49 50 51 52 53 54 55 56 57 58 59 def draw_text ( image : Image , pos : Tuple [ int , int ], colour = \"#1111ff\" , text = \"none\" ): \"\"\"Draw some text on our image. Args: image (PIL.Image): the PIL Image to draw on. pos (Tuple[int, int]): the coordinates to draw at. colour (str): the colour of the box as an '#rrggbb' string. text (str): the text to draw. \"\"\" draw = ImageDraw . Draw ( image ) draw . text ( pos , text , align = \"left\" , fill = colour ) fan_distort ( input_array , fan_height , bearing_table ) The fan distortion function. We choose a height that works as our scaling ratio (1.732). Parameters: input_array ( ndarray ) \u2013 the image to distort. fan_height ( int ) \u2013 the height of the resulting fan image. bearing_table ( List [ float ] ) \u2013 The bearing table. Returns: ndarray \u2013 np.ndarray: the new fan image. Source code in sealhits/image.py 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 @numba . njit def fan_distort ( input_array : np . ndarray , fan_height : int , bearing_table : NumbaList [ float ] ) -> np . ndarray : \"\"\"The fan distortion function. We choose a height that works as our scaling ratio (1.732). Args: input_array (np.ndarray): the image to distort. fan_height (int): the height of the resulting fan image. bearing_table (NumbaList[float]): The bearing table. Returns: np.ndarray: the new fan image. \"\"\" fan_size = ( int ( math . floor ( 1.732 * float ( fan_height ))), fan_height , ) # cant use get_fan_size(fan_height) function with numba sadly fan_image = np . zeros (( fan_height , fan_size [ 0 ])) hx = int ( fan_size [ 0 ] / 2 ) # Allow any width, not just 512. wratio = float ( input_array . shape [ 1 ]) / 512.0 # TODO - Definitely room for speedup here - can use tables and lookups. # TODO - bilinear filtering - but as an option as masks shouldn't be filtered. for y in range ( 1 , fan_height ): # Limit x range as we progress up the fan tt = int ( y * math . tan ( math . radians ( 60 ))) sx = max ( 0 , hx - tt - 10 ) sy = min ( fan_size [ 0 ], hx + tt + 10 ) for x in range ( sx , sy ): dx = int ( x - hx ) bearing = 0 if dx != 0 : bearing = math . atan2 ( dx , y ) bearing_deg = math . degrees ( bearing ) if bearing_deg >= MIN_ANGLE and bearing_deg <= MAX_ANGLE : # We are inside the sweep so we need to now find the lookup # in the original image. However, we have a non-linear relationship # between the sonar x position and the true bearing, so we need # to find the closest sample point to our bearing via the lookup # table bearing_sidx = 0 distance = int ( y / math . cos ( math . fabs ( bearing )) / fan_height * input_array . shape [ 0 ] ) if distance < 0 or distance >= input_array . shape [ 0 ]: continue for idx in range ( 511 ): # Size of the bearing table bt = bearing_table . getitem_unchecked ( idx ) # This seems to speed up the numbalist stuff a lot! bn = bearing_table . getitem_unchecked ( idx + 1 ) if bt >= bearing and bn < bearing : bearing_sidx = int ( float ( idx ) * wratio ) break sample = input_array [ distance ][ bearing_sidx ] fan_image [ y ][ x ] = sample return fan_image fits_to_np ( fits_path ) Given a path to a FITS file, attempt to return the numpy array and the fits header, or None if fails. Parameters: fits_path ( str ) \u2013 the path to the fits to load. Returns: Tuple [ array , PrimaryHDU ] \u2013 Tuple[np.array, fits.hdu.image.PrimaryHDU]: the loaded image as np.array and the FITS PrimaryHDU. Source code in sealhits/image.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 def fits_to_np ( fits_path : str ) -> Tuple [ np . array , fits . hdu . image . PrimaryHDU ]: \"\"\"Given a path to a FITS file, attempt to return the numpy array and the fits header, or None if fails. Args: fits_path (str): the path to the fits to load. Returns: Tuple[np.array, fits.hdu.image.PrimaryHDU]: the loaded image as np.array and the FITS PrimaryHDU. \"\"\" if \".lz4\" in fits_path : return decompress ( fits_path ) img = fits . open ( fits_path , memmap = False , lazy_load_hdus = False ) return ( img [ 0 ] . data , img [ 0 ] . header ) get_group_images ( db , fits_path , group_id , sonar_id = 854 , height = 400 , cache_path = '.' , fan_transform = True ) Return the image data and images for the group with this HUID. We check the cache first. All images must be preset and the correct size, otherwise we generate from new. Parameters: db ( DB ) \u2013 the database object. fits_path ( str ) \u2013 the path to the fits files. group_id ( group_id ) \u2013 Union[uuid.UUID, str]): either the uid or the huid for the group. sonar_id ( int , default: 854 ) \u2013 the id of the sonar to export. height ( int , default: 400 ) \u2013 the height of the resulting images. cache_path ( str , default: '.' ) \u2013 the path to the image cache. fan_transform ( bool , default: True ) \u2013 return fans instead of rectangles. Returns: Tuple [ array , List [ Images ]] \u2013 Tuple[np.array, List[Images]]: the images as a 3D np.array and a list of Images objects for each frame/image. Source code in sealhits/image.py 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 def get_group_images ( db : DB , fits_path : str , group_id : Union [ uuid . UUID , str ], sonar_id = 854 , height = 400 , cache_path = \".\" , fan_transform = True , ) -> Tuple [ np . array , List [ Images ]]: \"\"\"Return the image data and images for the group with this HUID. We check the cache first. All images must be preset and the correct size, otherwise we generate from new. Args: db (DB): the database object. fits_path (str): the path to the fits files. group_id (group_id: Union[uuid.UUID, str]): either the uid or the huid for the group. sonar_id (int): the id of the sonar to export. height (int): the height of the resulting images. cache_path (str): the path to the image cache. fan_transform (bool): return fans instead of rectangles. Returns: Tuple[np.array, List[Images]]: the images as a 3D np.array and a list of Images objects for each frame/image. \"\"\" if type ( group_id ) is uuid . UUID : group_uid = group_id else : g = db . get_group_huid ( group_id ) group_uid = g . uid group = db . get_group_uid ( group_uid ) uid = group . uid group_images = db . get_images_group_sonarid ( uid , sonar_id ) imgs = [] num_images = len ( group_images ) if num_images <= 0 : print ( \"No images found for group\" , uid ) return None np_frames = [] # Find all the images and create the base frames. Must be in the cache! Is probably # compressed as well. for img in group_images : dpath = is_cached_fan ( cache_path , img . filename ) cached = False if dpath is not None : data , header = decompress ( dpath ) np_frames . append ( data ) cached = True if not cached : fresult = fast_find ( img . filename , fits_path ) if fresult is not None : data , _ = fits_to_np ( fresult ) if fan_transform : fan_image = np . fliplr ( np . flipud ( fan_distort ( data , height , bearing_table )) ) np_frames . append ( fan_image ) else : np_frames . append ( data ) imgs . append ( img ) # It is possible, for some reason, that frames may not be the same size when in the RAW form, so # we run a check, making all images equal to the first. If the difference is too big we throw an error if not fan_transform : tframes = np_frames . copy () np_frames = [] first_shape = tframes [ 0 ] . shape np_frames . append ( tframes [ 0 ]) for frame in tframes [ 1 :]: # Row differences diff = first_shape [ 0 ] - frame . shape [ 0 ] if abs ( diff ) > 1 : raise AssertionError ( \"get_groups_images: RAW images in group differ too much\" ) if diff == - 1 : np . delete ( frame , - 1 , 0 ) if diff == 1 : frame = np . append ( frame , np . zeros (( 1 , first_shape [ 1 ])), 0 ) # Column differences diff = first_shape [ 1 ] - frame . shape [ 1 ] if abs ( diff ) > 1 : raise AssertionError ( \"get_groups_images: RAW images in group differ too much\" ) if diff == - 1 : np . delete ( frame , - 1 , 1 ) if diff == 1 : frame = np . append ( frame , np . zeros (( first_shape [ 0 ], 1 )), 1 ) np_frames . append ( frame ) try : np_frames = np . stack ( np_frames ) except Exception as e : print ( e ) return None return np_frames , imgs normalise_image ( img ) Normalise the image to range 0 to 1. Parameters: img ( array ) \u2013 the image to normalise. Returns: array \u2013 np.array: the normalised image (with a float32 type) Source code in sealhits/image.py 182 183 184 185 186 187 188 189 190 191 192 193 194 195 def normalise_image ( img : np . array ) -> np . array : \"\"\"Normalise the image to range 0 to 1. Args: img (np.array): the image to normalise. Returns: np.array: the normalised image (with a float32 type) \"\"\" fimg = img . astype ( np . float32 ) min_val = np . min ( img ) max_val = np . max ( img ) rval = ( fimg - min_val ) / ( max_val - min_val ) return rval np_to_fits ( save_path , img , image_time ) Given an np array, write out our fits image to a cache. Parameters: save_path ( str ) \u2013 the path and filename to save the fits to. img ( array ) \u2013 the image to save. image_time ( datetime ) \u2013 the time the image was taken. Source code in sealhits/image.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 def np_to_fits ( save_path : str , img : np . array , image_time : datetime . datetime ): \"\"\"Given an np array, write out our fits image to a cache. Args: save_path (str): the path and filename to save the fits to. img (np.array): the image to save. image_time: the time the image was taken. \"\"\" if not os . path . exists ( save_path ): try : hdr = fits . Header () hdr [ \"WIDTH\" ] = img . shape [ 1 ] hdr [ \"HEIGHT\" ] = img . shape [ 0 ] hdr [ \"YEAR\" ] = image_time . year hdr [ \"MONTH\" ] = image_time . month hdr [ \"DAY\" ] = image_time . day hdr [ \"HOUR\" ] = image_time . hour hdr [ \"MINUTE\" ] = image_time . minute hdr [ \"SECOND\" ] = image_time . second hdr [ \"MILLI\" ] = int ( image_time . microsecond / 1000 ) hdr = fits . PrimaryHDU ( img , header = hdr ) hdul = fits . HDUList ([ hdr ]) hdul . writeto ( save_path ) except Exception as e : print ( \"Could not save fits image:\" , save_path , e ) Track functions track.py - functions relating to tracks, such as overlaps. A number of functions used to detect tracks in the fan sonar images. get_bounding_boxes ( db , huid , imgs , img_size , fan_distort = True ) Get the track details for this group. Parameters: db ( DB ) \u2013 the database object. huid ( str ) \u2013 the huid for hte group we are looking for. imgs ( List [ Images ] ) \u2013 the list of Images records for this group. img_size ( Tuple [ int , int ] ) \u2013 The size of the images in pixels. Width then height. fan_distort ( bool , default: True ) \u2013 Are these images fans? Returns: List [ Tuple [ int , XYBox , str ]] \u2013 List[Tuple[int, XYBox, str]]: a list of frame numbers, bounding boxes and corresponding colours. Source code in sealhits/track.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 def get_bounding_boxes ( db : DB , huid : str , imgs : List [ Images ], img_size : Tuple [ int , int ], fan_distort = True ) -> List [ Tuple [ int , XYBox , str ]]: \"\"\"Get the track details for this group. Args: db (DB): the database object. huid (str): the huid for hte group we are looking for. imgs (List[Images]): the list of Images records for this group. img_size (Tuple[int,int]): The size of the images in pixels. Width then height. fan_distort (bool): Are these images fans? Returns: List[Tuple[int, XYBox, str]]: a list of frame numbers, bounding boxes and corresponding colours. \"\"\" bbs = [] group = db . get_group_huid ( huid ) uid = group . uid for idx , img in enumerate ( imgs ): points = db . get_image_points_by_filename_group ( img . filename , uid ) if ( len ( points ) > 0 ): # Since we have buffer start / end images and intermediates, 0 tracks are possible bb = points_to_bb ( points , img . range ) if fan_distort : bbox = bb . to_xy ( img_size ) (( xmin , ymin ), ( xmax , ymax )) = bbox . pair () # Flip BBS vertically to match flipped fan (normally flipped. #TODO - We should make this consistent) # This is a bit silly it seems bbs . append ( ( idx , XYBox ( xmin , img_size [ 1 ] - ymax , xmax , img_size [ 1 ] - ymin )) ) else : bbox = bb . to_xy_raw ( img_size ) # Flip BBS vertically to match flipped fan (normally flipped. #TODO - We should make this consistent) # This is a bit silly it seems bbs . append (( idx , bbox )) # Add a colour to the bounding box for ease of video and drawing. bbs = [( f , b , \"#ff0000\" ) for ( f , b ) in bbs ] return bbs Utility functions utils.py - misc utility functions. Useful utilities for various Sealhits related tasks. create_dir ( path ) Create a directory if it doesn't already exist. Parameters: path ( str ) \u2013 the path to the directory we want to make. Returns: bool ( bool ) \u2013 success? Source code in sealhits/utils.py 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 def create_dir ( path : str ) -> bool : \"\"\"Create a directory if it doesn't already exist. Args: path (str): the path to the directory we want to make. Returns: bool: success? \"\"\" if not os . path . exists ( path ): try : os . makedirs ( path ) except Exception : return False return True dist_bearing_to_xy ( bearing , distance , max_range , image_size ) Convert bearing (radians) and distance from a track to fan/polar image x, y coords. Image size is width/height. X and Y are integers within image_size. Parameters: bearing ( float ) \u2013 the bearing in radians. distance ( float ) \u2013 the distance in metres. max_range ( float ) \u2013 the maximum possible range (the sonar range) in metres. image_size ( Tuple [ int , int ] ) \u2013 the size of the image in pixels - width then height. Returns: Tuple [ int , int ] \u2013 Tuple[int, int]: the x and y coordinates in pixels. Source code in sealhits/utils.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 def dist_bearing_to_xy ( bearing : float , distance : float , max_range : float , image_size : Tuple [ int , int ] ) -> Tuple [ int , int ]: \"\"\"Convert bearing (radians) and distance from a track to fan/polar image x, y coords. Image size is width/height. X and Y are integers within image_size. Args: bearing (float): the bearing in radians. distance (float): the distance in metres. max_range (float): the maximum possible range (the sonar range) in metres. image_size: the size of the image in pixels - width then height. Returns: Tuple[int, int]: the x and y coordinates in pixels. \"\"\" # Assume bearing is either side of the vertical line? # Assume distance is in metres with the max being passed in as max_range # assert math.degrees(bearing) >= MIN_ANGLE and math.degrees(bearing) <= MAX_ANGLE # We are effectively rotating this 90 degrees anti clockwise and a POSITIVE bearing # should be to the LEFT of the centre line post rotation (this seems silly but is # the way according to PAMGuard) # TODO - we are returning integers here. We might wish to do floats at some point # so we can do interpolation. d0 = float ( distance / max_range * image_size [ 1 ]) x0 = math . sin ( math . fabs ( bearing )) * d0 y0 = math . cos ( math . fabs ( bearing )) * d0 hl = image_size [ 0 ] / 2 if bearing > 0 : x0 = hl - x0 else : x0 = hl + x0 return ( int ( x0 ), int ( y0 )) fast_find ( name , base_path ) Find the full FITS bath based on how we store the images. This is much faster than a full path walk. We also look for images without the .lz4 extension. We prioritise the non compressed file. Parameters: name ( str ) \u2013 the file we are looking for. base_path ( str ) \u2013 the directory we are searching. Returns: Union [ str , None] \u2013 Union[str, None]: the path to the file if found, or None. Source code in sealhits/utils.py 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 def fast_find ( name : str , base_path : str ) -> Union [ str , None ]: \"\"\"Find the full FITS bath based on how we store the images. This is much faster than a full path walk. We also look for images without the .lz4 extension. We prioritise the non compressed file. Args: name (str): the file we are looking for. base_path (str): the directory we are searching. Returns: Union[str, None]: the path to the file if found, or None. \"\"\" joined_unzipped = os . path . join ( base_path , name ) if os . path . exists ( joined_unzipped ): return joined_unzipped joined = os . path . join ( base_path , name + \".lz4\" ) if os . path . exists ( joined ): return joined for tt in os . listdir ( base_path ): tt = os . path . join ( base_path , tt ) if os . path . isdir ( tt ): full_path_unzipped = os . path . join ( tt , name ) if os . path . isfile ( full_path_unzipped ): return full_path_unzipped full_path = os . path . join ( tt , name + \".lz4\" ) if os . path . isfile ( full_path ): return full_path print ( \"Could not find:\" , name ) return None find ( pattern , path ) Find a file somewhere underneath path, matching the pattern. Args: pattern (str): a pattern string for fnmatch. distance (float): a path to a directory to search. Returns: List [ str ] \u2013 List[str]: the resulting files as full paths. Source code in sealhits/utils.py 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 def find ( pattern : str , path : str ) -> List [ str ]: \"\"\"Find a file somewhere underneath path, matching the pattern. Args: pattern (str): a pattern string for fnmatch. distance (float): a path to a directory to search. Returns: List[str]: the resulting files as full paths. \"\"\" results = [] for root , dirs , files in os . walk ( path ): for name in files : if fnmatch . fnmatch ( name , pattern ): results . append ( os . path . join ( root , name )) return results get_fan_size ( height ) Return a fan size, given the height. Uses a ratio of 1.732 given the 120 degree spread. Parameters: height ( int ) \u2013 the height we want in pixels. Returns: Tuple [ int , int ] \u2013 Tuple[int, int]: the width and height of the fan image in pixels. Source code in sealhits/utils.py 124 125 126 127 128 129 130 131 132 133 134 def get_fan_size ( height : int ) -> Tuple [ int , int ]: \"\"\" Return a fan size, given the height. Uses a ratio of 1.732 given the 120 degree spread. Args: height (int): the height we want in pixels. Returns: Tuple[int, int]: the width and height of the fan image in pixels. \"\"\" return ( int ( math . floor ( 1.732 * float ( height ))), height ) Video functions video.py - create videos from a numpy array. Generate video from a list of frames, bounding boxes and group information. gen_video ( frames , bbs , text , out_path = '.' , colour_map = Batlow_20 , rate = 4 ) Given the original frames and a Bounding Box list, create our final video. The BBS list is in the format (frame number, xmin, ymin, xmax, ymax, colour). The path is the full path to the video file, plus the extension (which determines the type). Parameters: frames ( array ) \u2013 the frames that makeup the video. bbs ( List [ Tuple [ int , XYBox , str ]] ) \u2013 the list of bounding boxes, frame numbers and the colour. text ( str ) \u2013 text to draw. out_path ( str , default: '.' ) \u2013 the full path to the video. Must end in a video type like '.mp4' or '.webm'. colour_map ( Palette , default: Batlow_20 ) \u2013 the colour palette to use. rate ( int , default: 4 ) \u2013 frames per second. Returns: str ( str ) \u2013 path to the saved video. Source code in sealhits/video.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def gen_video ( frames : np . array , bbs : List [ Tuple [ int , XYBox , str ]], text : str , out_path = \".\" , colour_map = Batlow_20 , rate = 4 ) -> str : \"\"\"Given the original frames and a Bounding Box list, create our final video. The BBS list is in the format (frame number, xmin, ymin, xmax, ymax, colour). The path is the full path to the video file, plus the extension (which determines the type). Args: frames (np.array): the frames that makeup the video. bbs (List[Tuple[int, XYBox, str]]): the list of bounding boxes, frame numbers and the colour. text (str): text to draw. out_path (str): the full path to the video. Must end in a video type like '.mp4' or '.webm'. colour_map (palettable.Palette): the colour palette to use. rate (int): frames per second. Returns: str: path to the saved video. \"\"\" over_frames = [] font = ImageFont . truetype ( \"./Hack-Regular.ttf\" , 16 ) for fidx in range ( frames . shape [ 0 ]): frame = frames [ fidx ] blank_image = Image . fromarray ( np . full (( frame . shape [ 0 ], frame . shape [ 1 ], 3 ), ( 0 , 0 , 0 ), np . uint8 ) ) draw = ImageDraw . Draw ( blank_image ) lines = text . split ( \" \\n \" ) for i , line in enumerate ( lines ): draw . text (( 10 , i * 20 ), str ( line ), font = font ) if bbs is not None : for frame_id , bbox , colour in bbs : if frame_id == fidx : draw . rectangle ( bbox . tuple (), outline = colour ) over_frames . append ( np . array ( blank_image )) # Now create a quick video coloured = frames # If this is a luminance image, do the nice colour mapping if len ( frames . shape ) == 3 or ( frames . shape [ - 1 ] == 1 and len ( frames . shape ) == 4 ): np_fan = np . array ( normalise_image ( frames ) * 255 , dtype = np . uint8 ) coloured = np . zeros (( * np_fan . shape , 3 ), dtype = np . uint8 ) # Take entries from RGB LUT according to greyscale values in image lut = [ colour_map . mpl_colormap ( x / 255.0 ) for x in range ( 256 )] lut = [[ int ( x [ 0 ] * 255 ), int ( x [ 1 ] * 255 ), int ( x [ 2 ] * 255 )] for x in lut ] np . take ( lut , np_fan , axis = 0 , out = coloured ) np_track = np . array ( over_frames , dtype = np . uint8 ) coloured = np . maximum ( coloured , np_track ) ffmpegio . video . write ( out_path , rate , coloured , overwrite = True , show_log = False ) return out_path File functions files.py - find the files on disk that we need. The various functions that search out the files we want and the tracks we are looking for. bin_files_avail ( pdir ) Return a list of binary PGDF files with the corresponding date range. Parameters: pdir ( str ) \u2013 the path to the gemini binary files. Returns: List [ Tuple [ str , datetime , datetime ]] \u2013 List[Tuple[str, datetime.datetime, datetime.datetime]]: A list of paths to the pgdf including the start and end datetimes Source code in sealhits/sources/files.py 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 def bin_files_avail ( pdir : str , ) -> List [ Tuple [ str , datetime . datetime , datetime . datetime ]]: \"\"\"Return a list of binary PGDF files with the corresponding date range. Args: pdir (str): the path to the gemini binary files. Returns: List[Tuple[str, datetime.datetime, datetime.datetime]]: A list of paths to the pgdf including the start and end datetimes \"\"\" efiles = [] for root , _ , files in os . walk ( pdir , topdown = False ): for name in files : fpath = os . path . join ( root , name ) _ , file_extension = os . path . splitext ( name ) try : if \"pgdf\" in file_extension . lower (): print ( \"Opening pgdf:\" , fpath ) tp = pgdf . PGDF ( fpath ) if len ( tp . module . objects ) > 0 : date_min = None date_max = None for pamobj in tp . module . objects : tdate = pamobj . pam . date assert tdate is not None if date_min is None : date_min = tdate elif date_min > tdate : date_min = tdate if date_max is None : date_max = tdate elif date_max < tdate : date_max = tdate print ( \"Adding pgdf:\" , fpath ) efiles . append (( fpath , date_min , date_max )) else : print ( \"PGDF has no module objects.\" ) except Exception as e : print ( \"Could not read\" , fpath , e ) # Sort the efile bases on the dates efiles . sort ( key = lambda ef : ef [ 1 ]) return efiles get_tracks ( bin_files ) Read the found PGDF files and generate the tracks for our eventual dataset. Parameters: bin_files ( str ) \u2013 the path to the PGDF files. Returns: List [ PGObject ] \u2013 List[PGObject]: A list of PGObject representing tracks from PAMGuard. Source code in sealhits/sources/files.py 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 def get_tracks ( bin_files : List [ str ]) -> List [ PGObject ]: \"\"\"Read the found PGDF files and generate the tracks for our eventual dataset. Args: bin_files (str): the path to the PGDF files. Returns: List[PGObject]: A list of PGObject representing tracks from PAMGuard. \"\"\" tracks = [] # TODO - it is possible that one track can cross multiple files. # We've ignored this for now # https://github.com/PAMGuard/PAMGuardMatlab/blob/main/pgmatlab/loadPamguardMultiFile.m # UID in the track group is different to the UID in the Track group children. # Trackgroup children UID *should* match the UID in the pgdf for fb in bin_files : print ( \"Reading bin file:\" , fb ) pbin = pgdf . PGDF ( fb ) assert pbin . header . module_type == \"Gemini Threshold Detector\" for obj in pbin . module . objects : tracks . append ( obj ) # Sort tracks in order of time if not already tracks = sorted ( tracks , key = lambda track : track . data . track . time_start ) return tracks glf_files_avail ( glf_path ) Given a starting path, find all the possible glf files. Parameters: glf_path ( str ) \u2013 the path to the GLF files. Returns: List [ str ] \u2013 List[str]: A list of full paths to all found GLF files. Source code in sealhits/sources/files.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 def glf_files_avail ( glf_path : str ) -> List [ str ]: \"\"\"Given a starting path, find all the possible glf files. Args: glf_path (str): the path to the GLF files. Returns: List[str]: A list of full paths to all found GLF files. \"\"\" full_paths = [] for root , _ , files in os . walk ( glf_path , topdown = False ): for name in files : _ , file_extension = os . path . splitext ( name ) if file_extension . lower () == \".glf\" : full_paths . append ( os . path . join ( root , name )) return full_paths pgdfs_paths ( pgdf_path ) Find all PGDFs under a given path. Parameters: pgdf_path ( str ) \u2013 the path to the PGDF files. Returns: List [ str ] \u2013 List[str]: A list of full paths to all found PGDF files. Source code in sealhits/sources/files.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 def pgdfs_paths ( pgdf_path : str ) -> List [ str ]: \"\"\" Find all PGDFs under a given path. Args: pgdf_path (str): the path to the PGDF files. Returns: List[str]: A list of full paths to all found PGDF files. \"\"\" full_paths = [] for root , _ , files in os . walk ( pgdf_path , topdown = False ): for name in files : _ , file_extension = os . path . splitext ( name ) if file_extension . lower () == \".pgdf\" : fpath = os . path . join ( root , name ) full_paths . append ( fpath ) return full_paths pgdfs_to_full_paths ( pgdf_path , pgdfs ) Fill out the pgdf filenames with their full paths. Parameters: pgdf_path ( str ) \u2013 the path to the PGDFS files. pgdfs ( List [ str ] ) \u2013 a list of PGDF names to look for Returns: List [ str ] \u2013 List[str]: A list of full paths to the PGDFS in the pgdfs list. Source code in sealhits/sources/files.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 def pgdfs_to_full_paths ( pgdf_path : str , pgdfs : List [ str ]) -> List [ str ]: \"\"\"Fill out the pgdf filenames with their full paths. Args: pgdf_path (str): the path to the PGDFS files. pgdfs (List[str]): a list of PGDF names to look for Returns: List[str]: A list of full paths to the PGDFS in the pgdfs list. \"\"\" full_paths = [] for root , _ , files in os . walk ( pgdf_path , topdown = False ): for name in files : if name in pgdfs : fpath = os . path . join ( root , name ) full_paths . append ( fpath ) return full_paths Ingesting GLFs glf.py - all the functions related to GLF files Functions for ingesting the GLF files. This involves finding the files on disk, picking the ones we need, reading the required image records, then saving these images to disk as LZ4 compressed files. GDat Bases: object An internal object that holds all the details we want on our GLFs as we process them. Source code in sealhits/sources/glf.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 class GDat ( object ): \"\"\"An internal object that holds all the details we want on our GLFs as we process them.\"\"\" def __init__ ( self , startdate : datetime . datetime , enddate : datetime . datetime , gobj : GLFS , full_path : str , ): self . start_date = startdate self . end_date = enddate self . gobj = gobj self . full_path = full_path get_times ( gpath ) Get the start and end times from this GLF file. Parameters: gpath ( str ) \u2013 path to a single GLF file. Returns: Union [ Tuple [ datetime , datetime ], None] \u2013 Union[Tuple[datetime.datetime, datetime.datetime], None]: either the start and end times, or None. Source code in sealhits/sources/glf.py 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 def get_times ( gpath : str ) -> Union [ Tuple [ datetime . datetime , datetime . datetime ], None ]: \"\"\"Get the start and end times from this GLF file. Args: gpath (str): path to a single GLF file. Returns: Union[Tuple[datetime.datetime, datetime.datetime], None]: either the start and end times, or None. \"\"\" try : with GLF ( gpath ) as gf : time_start = None time_end = None for image_rec in gf . images : image_time = image_rec . db_tx_time if time_start is None : time_start = image_time elif image_time < time_start : time_start = image_time if time_end is None : time_end = image_time elif image_time > time_end : time_end = image_time assert time_start is not None assert time_end is not None return ( time_start , time_end ) except Exception as e : logging . error ( \"Failed to read glf: %s , %s \" , gpath , e ) return None glf_get_image ( gpath , image_rec ) Get an image from a GLF file using the given record Parameters: gpath ( str ) \u2013 path to a single GLF file. image_rec ( ImageRecord ) \u2013 the GLF File image record Returns: Tuple [ bytes , Tuple [ int , int ]] \u2013 Tuple[bytes, Tuple[int, int]]: raw data as bytes and the width and height in pixels as ints. Source code in sealhits/sources/glf.py 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 def glf_get_image ( gpath : str , image_rec ) -> Tuple [ bytes , Tuple [ int , int ]]: \"\"\" Get an image from a GLF file using the given record Args: gpath (str): path to a single GLF file. image_rec (ImageRecord): the GLF File image record Returns: Tuple[bytes, Tuple[int, int]]: raw data as bytes and the width and height in pixels as ints. \"\"\" # TODO - as nice as it is to wrap the GLF path here, this means a double # open often, as glf_get_image can be inside a loop with glf_times_range above # Might need to think about how these two functions are used together. # It's also a small function so maybe there's a better way? try : with GLF ( gpath ) as gf : image_data , image_size = gf . extract_image ( image_rec ) return image_data , image_size except Exception as e : logging . error ( \"_glf_times failed to read glf: %s \" , gpath ) logging . error ( \"Exception %s \" , e ) return None glf_times_range ( gpath , start_t , end_t ) Given a path to a GLF file, and a start and end time, return an img record and the sonar range. This is an iterator function. Parameters: gpath ( str ) \u2013 path to a single GLF file. start_t ( datetime ) \u2013 the start datetime. end_t ( datetime ) \u2013 the end datetime. Returns: Tuple [ ImageRecord , float ] \u2013 Union[Tuple[datetime.datetime, datetime.datetime], None]: either the start and end times, or None. Source code in sealhits/sources/glf.py 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 def glf_times_range ( gpath : str , start_t : datetime . datetime , end_t : datetime . datetime ) -> Tuple [ ImageRecord , float ]: \"\"\" Given a path to a GLF file, and a start and end time, return an img record and the sonar range. This is an iterator function. Args: gpath (str): path to a single GLF file. start_t (datetime.datetime): the start datetime. end_t (datetime.datetime): the end datetime. Returns: Union[Tuple[datetime.datetime, datetime.datetime], None]: either the start and end times, or None. \"\"\" # TODO - I wonder if this could be made faster with a time index? # Also, yielding from within a with statement might mean a lot of opening and closing? Or maybe not? try : with GLF ( gpath ) as gf : for image_rec in gf . images : image_time = image_rec . db_tx_time if image_time >= start_t and image_time <= end_t : sonar_range = int ( round ( calculate_range ( image_rec ))) yield ( image_rec , sonar_range ) except Exception as e : logging . error ( \"_glf_times failed to read glf: %s \" , gpath ) logging . error ( \"Exception %s \" , e ) raise IOError has_track ( session , image_time , sonar_id ) Look in the database to see if this image has a track. Parameters: session ( Session ) \u2013 current SQLAlchemy session. image_time ( datetime ) \u2013 the datetime of the current image. sonar_id ( int ) \u2013 the sonar id. Returns: bool ( bool ) \u2013 does this image have a track Source code in sealhits/sources/glf.py 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 def has_track ( session : Session , image_time : datetime . datetime , sonar_id : int ) -> bool : \"\"\"Look in the database to see if this image has a track. Args: session (Session): current SQLAlchemy session. image_time (datetime.datetime): the datetime of the current image. sonar_id (int): the sonar id. Returns: bool: does this image have a track \"\"\" delta = datetime . timedelta ( milliseconds = 10 ) points = [] with session . no_autoflush : points = ( session . query ( Points ) . filter ( Points . time >= image_time - delta , Points . time <= image_time + delta ) . all () ) for point in points : if point . sonarid == sonar_id : return True return False process_glf_by_group ( session , group , fname_lookup , gdats , max_glf , outpath ) Process a single group, outputting all of the images from both sonars for the time period of this group. fname_lookup is altered by the function, storing the new image objects by the filename. Parameters: session ( Session ) \u2013 the current SQLAlchemy session. group ( Groups ) \u2013 the Group we are currently looking at. fname_lookup ( dict ) \u2013 a lookup of images by filename. Can be an empty dict. gdats ( List [ GDat ] ) \u2013 a list of GDat objects. max_glf ( int ) \u2013 the maximum number of images to consider. outpath ( str ) \u2013 where to save the output images. Returns: Tuple [ List [ Images ], dict ] \u2013 Tuple[List[Images], dict]: A list of the new Images objects created and the dict mapping filenames onto these images objects. Source code in sealhits/sources/glf.py 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 def process_glf_by_group ( session : Session , group : Groups , fname_lookup , gdats : List [ GDat ], max_glf : int , outpath : str ) -> Tuple [ List [ Images ], dict ]: \"\"\"Process a single group, outputting all of the images from both sonars for the time period of this group. fname_lookup is altered by the function, storing the new image objects by the filename. Args: session (Session): the current SQLAlchemy session. group (Groups): the Group we are currently looking at. fname_lookup (dict): a lookup of images by filename. Can be an empty dict. gdats (List[GDat]): a list of GDat objects. max_glf (int): the maximum number of images to consider. outpath (str): where to save the output images. Returns: Tuple[List[Images], dict]: A list of the new Images objects created and the dict mapping filenames onto these images objects. \"\"\" group_start = group . timestart group_end = group . timeend group_image_count = 0 new_images = [] # We need a check here on the length of the group as there are some erroneous # SUPER long groups we can't ingest really. Anything longer than 2 minutes ignore # TODO - could potentialy ingest *up-to* this seconds amount? gdd = group . timeend - group . timestart if gdd . total_seconds () > max_glf : logging . warn ( \"Group %s is much too long with a time of %s seconds.\" , group . huid , str ( gdd . total_seconds ()), ) return new_images , fname_lookup # Empty at this point found_gdat = False for gdat in gdats : glfname = gdat . gobj . filename # If our start is greater, we've already passed it as gdats is ordered. # probably doesn't save us too much time. if gdat . start_date > group_end : break if group_start <= gdat . end_date and group_end >= gdat . start_date : # Link GLF to group. with session . no_autoflush : # Check it doesn't exist already as sometimes we get duplicates if group not in gdat . gobj . groups : gdat . gobj . groups . append ( group ) # Now fully read in the GLF # We read one frame at a time because it will use a lot of memory # TODO - could we check to see if the FITS already exists BEFORE the call to _glf_times? try : for image_rec , srange in glf_times_range ( gdat . full_path , group_start , group_end ): found_gdat = True image_time = image_rec . db_tx_time sonar_id = image_rec . header . device_id milli = int ( image_time . microsecond / 1000 ) ext = \".fits\" # Setup the FITS filename; a combination of time and sonar id. fname = ( image_time . strftime ( \"%Y_%m_ %d _%H_%M_%S_\" ) + f \" { milli : 03d } \" + \"_\" + str ( sonar_id ) + ext ) fname_compressed = fname + \".lz4\" subdir = os . path . join ( outpath , image_time . strftime ( \"%Y_%m_ %d \" )) if not os . path . exists ( subdir ): os . mkdir ( subdir ) # Check to see if this image already exists. It might if groups overlap full_fits_path = os . path . join ( subdir , fname_compressed ) if not os . path . exists ( full_fits_path ): # TODO - remove this I think or fire up an error to ignore this GLF # as looping around is not ideal! # For some reason, this can fail too but the file seems fine. # Therefore, we should loop around again till it reads correctly res = glf_get_image ( gdat . full_path , image_rec ) image_data , image_size = res try : image_np = np . frombuffer ( image_data , dtype = np . uint8 ) . reshape ( ( image_size [ 1 ], image_size [ 0 ]) ) hdr = fits . Header () hdr [ \"SONARID\" ] = sonar_id hdr [ \"WIDTH\" ] = image_size [ 0 ] hdr [ \"HEIGHT\" ] = image_size [ 1 ] hdr [ \"YEAR\" ] = image_time . year hdr [ \"MONTH\" ] = image_time . month hdr [ \"DAY\" ] = image_time . day hdr [ \"HOUR\" ] = image_time . hour hdr [ \"MINUTE\" ] = image_time . minute hdr [ \"SECOND\" ] = image_time . second hdr [ \"MILLI\" ] = int ( image_time . microsecond / 1000 ) # hdr = fits.PrimaryHDU(image_np, header=hdr) # hdul = fits.HDUList([hdr]) # hdul.writeto(full_fits_path) compress ( image_np , hdr , full_fits_path ) del image_data del image_np del hdr logging . info ( \"Generated %s from %s \" , full_fits_path , gdat . full_path ) except Exception as e : logging . error ( \"Could not generate FITS: %s , %s \" , fname , e , ) logging . error ( \"Traceback %s \" , traceback . format_exc ()) # else: # logging.info(\"File %s already exists for group %s\", full_fits_path, group.huid) # Now create the DB objects. We create new image objects, or # we find the existing one and modify it. We return all images # and hope our transaction does the right thing in adding or # updating. group_image_count += 1 new_image = None with session . no_autoflush : q = session . query ( Images ) . filter ( Images . filename == fname ) new_image = q . one_or_none () # It's also possible that we already have this image ready to be committed to the # DB but it gets pulled in again for a different group so we must check if fname in fname_lookup . keys (): new_image = fname_lookup [ fname ] if new_image is None : ht = has_track ( session , image_time , sonar_id ) new_image = Images ( uid = uuid . uuid4 (), filename = fname , hastrack = ht , glf = glfname , time = image_time , sonarid = sonar_id , range = srange , ) fname_lookup [ fname ] = new_image if group not in new_image . groups : new_image . groups . append ( group ) new_images . append ( new_image ) except IOError : logging . Error ( \"Failed to read GLF %s . Skipping...\" , glfname ) if not found_gdat : logging . error ( \"Found no GDATS for group %s \" , group . huid ) if group_image_count == 0 : logging . error ( \"*** Group %s has no images! ***\" , group . huid ) return new_images , fname_lookup process_glfs ( session , groups , glfpath , outpath , max_glf ) Once the PGDFs and SQLITE are processed, we can begin to look for the GLF files we need. We want each new group to have a number of FITS images fitting the range timestart - buffer to time-end + buffer. The function split_groups will have split groups and adjusted start and end times already so we judt need to look at the group times. Parameters: session ( Session ) \u2013 the current SQLAlchemy session. groups ( List [ Groups ] ) \u2013 the Groups we are currently looking at. glfpath ( str ) \u2013 the path to the GLF fules max_glf ( int ) \u2013 the maximum number of images to consider. outpath ( str ) \u2013 where to save the output images. Returns: Tuple [ List [ GLFS ], List [ Images ]] \u2013 Tuple[List[GLFS], List[Images]]: Two lists - the new GLFS objects to save to the DB and the new Images objects to save to the DB. Source code in sealhits/sources/glf.py 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 def process_glfs ( session : Session , groups : List [ Groups ], glfpath : str , outpath : str , max_glf : int ) -> Tuple [ List [ GLFS ], List [ Images ]]: \"\"\"Once the PGDFs and SQLITE are processed, we can begin to look for the GLF files we need. We want each new group to have a number of FITS images fitting the range timestart - buffer to time-end + buffer. The function split_groups will have split groups and adjusted start and end times already so we judt need to look at the group times. Args: session (Session): the current SQLAlchemy session. groups (List[Groups]): the Groups we are currently looking at. glfpath (str): the path to the GLF fules max_glf (int): the maximum number of images to consider. outpath (str): where to save the output images. Returns: Tuple[List[GLFS], List[Images]]: Two lists - the new GLFS objects to save to the DB and the new Images objects to save to the DB. \"\"\" new_glfs = [] logging . info ( \"Finding available GLF files...\" ) glf_files = glf_files_avail ( glfpath ) # Go through all GLFs and get the times of each. # Check the database first for the times we already have and # only grab the times for GLFs we don't have # There might be duplicates due to directory renaming or similar and rsync. times , glf_files_missing = _find_glf_times_db ( session , glf_files ) logging . info ( \"Retrieving GLF Times (this may take a while)...\" ) if len ( glf_files_missing ) > 0 : logging . info ( \"GLFs missing from DB: %s \" , len ( glf_files_missing )) # commented out due to errors on hdd5 for some reason? # with ThreadPool(NUM_THREADS) as pool: # times += pool.map(_get_glf_times, glf_files_missing, chunksize=NUM_THREADS) for glf_file in glf_files_missing : new_times = _get_glf_times ( glf_file ) if new_times [ 1 ] is not None : times . append ( new_times ) # Add the GLFS regardless of whether or not they are used. # times_glfs is the list of all the info we need. # gdats are just a struct that holds the times, database # object and path together. gdats = [] for ( glf_start , glf_end ), glf_path in times : glfname = os . path . basename ( glf_path ) new_glf = None with session . no_autoflush : q = session . query ( GLFS ) . filter ( GLFS . filename == glfname ) new_glf = q . one_or_none () # It's possible (for some reason) that GLFS might get double # counted in the addition phase, so make sure it doesn't exist # in the new_glfs already for ng in new_glfs : if ng . filename == glfname : new_glf = ng if new_glf is None : new_glf = GLFS ( uid = uuid . uuid4 (), filename = glfname , startdate = glf_start , enddate = glf_end , groups = [], ) new_glfs . append ( new_glf ) gdats . append ( GDat ( glf_start , glf_end , new_glf , glf_path )) # Make sure there are no errored entries and organise by time # Sorting by time makes the FITS export a little quicker. gdats = sorted ( gdats , key = functools . cmp_to_key ( sort_times )) logging . info ( \"Number of initial time ranges: %s \" , len ( gdats )) gdat_latest = gdats [ - 1 ] . end_date for gd in gdats : if gd . end_date > gdat_latest : gdat_latest = gd . end_date logging . info ( \"GDats earliest %s and latest %s .\" , str ( gdats [ 0 ] . start_date ), str ( gdat_latest ) ) sofar = 0 tlist = [] logging . info ( \"Number of groups: %s \" , len ( groups )) # The goal at this stage is to match up new group times with GLF times # and only process the files we need to, even if we've recorded a whole # batch of GLFs for a time period. groups = sorted ( groups , key = functools . cmp_to_key ( sort_times_group )) group_earliest = groups [ 0 ] . timestart group_latest = groups [ 0 ] . timeend for g in groups : if g . timeend > group_latest : group_latest = g . timeend assert group_earliest < group_latest logging . info ( \"Groups earliest %s and latest %s .\" , str ( group_earliest ), str ( group_latest ) ) new_images = [] new_images_by_fname = {} # Fast lookup # Start off the threads, chunking up the IDs then submitting to the threaded function. while len ( groups ) > 0 : while len ( groups ) > 0 and len ( tlist ) < NUM_THREADS : tlist . append ( groups . pop ()) for group in tlist : ni , nf = process_glf_by_group ( session , group , new_images_by_fname , gdats , max_glf , outpath ) new_images += ni new_images_by_fname = nf sofar += len ( tlist ) tlist = [] logging . info ( \"Processed %s . %s remaining.\" , sofar , len ( groups )) return ( new_glfs , new_images ) sort_times ( a , b ) Sort GDat by start date Source code in sealhits/sources/glf.py 101 102 103 104 105 106 107 108 109 110 111 112 def sort_times ( a , b ): \"\"\"Sort GDat by start date\"\"\" s0 = a . start_date s1 = b . start_date if s0 < s1 : return - 1 if s0 == s1 : return 1 return 0 sort_times_group ( a , b ) Sort Groups by time start Source code in sealhits/sources/glf.py 115 116 117 118 119 120 121 122 123 124 125 126 def sort_times_group ( a , b ): \"Sort Groups by time start\" s0 = a . timestart s1 = b . timestart if s0 < s1 : return - 1 if s0 == s1 : return 1 return 0 Extraction from GLFs glfextract.py - extract numpy arrays from GLF files. A short utility function to extract numpy arrays from a GLF file within a particular time range. extract ( gpath , start_t , end_t ) Given a path to a GLF file, a start and end time, return a 3D numpy array of images within that time frame. Parameters: gpath ( str ) \u2013 The path to a GLF file. start_t ( datetime ) \u2013 The starting date time. end_t ( datetime ) \u2013 The ending date time. Returns: Union [None, array ] \u2013 Union[None, np.array]: either None, or an np.array of frames Source code in sealhits/sources/glfextract.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 def extract ( gpath : str , start_t : datetime . datetime , end_t : datetime . datetime ) -> Union [ None , np . array ]: \"\"\" Given a path to a GLF file, a start and end time, return a 3D numpy array of images within that time frame. Args: gpath (str): The path to a GLF file. start_t (datetime.datetime): The starting date time. end_t (datetime.datetime): The ending date time. Returns: Union[None, np.array]: either None, or an np.array of frames \"\"\" try : with GLF ( gpath ) as gf : time_start = None time_end = None frames = [] for image_rec in tqdm ( gf . images , desc = \"Ingesting Images\" ): image_time = image_rec . db_tx_time if time_start is None : time_start = image_time elif image_time < time_start : time_start = image_time if time_end is None : time_end = image_time elif image_time > time_end : time_end = image_time if image_time >= start_t and image_time <= end_t : image_data , image_size = gf . extract_image ( image_rec ) image_np = np . frombuffer ( image_data , dtype = np . uint8 ) . reshape (( image_size [ 1 ], image_size [ 0 ])) frames . append ( image_np ) frames = np . array ( frames ) return frames except Exception as e : print ( e ) return None Ingesting Groups group.py - group object functions Functions for sorting out group objects, such as fixing the times and splitting based on a buffer time. find_group_objects ( session , sqldb , sqlname , sqlalias , max_secs = 800 ) Start by finding the Groups and TrackGroups from the SQLITE file and creating these in memory. sqlalias is required in case one is reading in an updated or otherwise changed sqlitefile that has a different name. Groups will exist from this previous version of the file and should be included. sqlalias should be set to the name of that older file. Otherwise, alias should match sqlname. Parameters: session ( Session ) \u2013 The current SQLAlchemy session. sqldb ( SQLPAM ) \u2013 The SQLPAM database object we are reading. sqlname ( str ) \u2013 The filename of the sqlite file. sqlalias ( str ) \u2013 If we are reading from a different sqlite file but want to lookup with an alias, add a different name here. max_secs ( int , default: 800 ) \u2013 The maximum length of group to consider in seconds. Returns: Tuple [ List [ Groups ], List [ TrackGroup ]] \u2013 Tuple[List[Groups], List[TrackGroup]]: Two lists, the new Groups objects and the TrackGroup objects. Source code in sealhits/sources/group.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 def find_group_objects ( session : Session , sqldb : sqlpam . SQLPAM , sqlname : str , sqlalias : str , max_secs = 800 ) -> Tuple [ List [ Groups ], List [ TrackGroup ]]: \"\"\"Start by finding the Groups and TrackGroups from the SQLITE file and creating these in memory. sqlalias is required in case one is reading in an updated or otherwise changed sqlitefile that has a different name. Groups will exist from this previous version of the file and should be included. sqlalias should be set to the name of that older file. Otherwise, alias should match sqlname. Args: session (Session): The current SQLAlchemy session. sqldb (SQLPAM): The SQLPAM database object we are reading. sqlname (str): The filename of the sqlite file. sqlalias (str): If we are reading from a different sqlite file but want to lookup with an alias, add a different name here. max_secs (int): The maximum length of group to consider in seconds. Returns: Tuple[List[Groups], List[TrackGroup]]: Two lists, the new Groups objects and the TrackGroup objects. \"\"\" group_list = sqldb . track_groups found_groups = [] found_tracks = [] logging . info ( \"Reading Groups...\" ) for g in tqdm ( group_list , desc = \"Reading Groups\" ): interaction = g . interaction code = \"none\" if g . track_type is not None : code = g . track_type . lower () # Start with the master group object # Check the *real* composite primary key. If it exists, # return that instead of a new thing. Means we can use # merge later on to do easy updates. # split can be -1 or 0. If it's not been split or it's been split and it's the first one. new_group = None with session . no_autoflush : q = session . query ( Groups ) . filter ( Groups . gid == g . uid , Groups . sqliteid == g . id , or_ ( Groups . split == - 1 , Groups . split == 0 ), Groups . sqlite == sqlalias , ) new_group = q . one_or_none () if new_group is None : huid = generate_id () new_group = Groups ( uid = uuid . uuid4 (), gid = g . uid , sqliteid = g . id , sqlite = sqlname , huid = huid , timestart = g . utc , timeend = g . end_time , code = code , comment = g . comment , mammal = g . mammal , fish = g . fish , bird = g . bird , interact = interaction , split =- 1 , pgdfs = [], images = [], glfs = [], ) logging . info ( \"New Group: %d , %d , %s , %s \" , g . uid , g . id , sqlname , huid ) else : # Check to see if this group needs updating new_group . timestart = g . utc new_group . timeend = g . end_time new_group . code = code new_group . comment = g . comment new_group . mammal = g . mammal new_group . fish = g . fish new_group . bird = g . bird new_group . interact = interaction # We may need to change the sqlname which is part of the composite key # We set it to the new SQLITEDB we are importing. new_group . sqlite = sqlname # Early rejection of groups that are too long. Note that due to bugs in PAMGuard # the group times may not be accurate. However, we reject here so that the tracks # are also not included. TODO - this could be improved. td = datetime . timedelta ( seconds = max_secs ) if new_group . timeend - new_group . timestart > td : logging . warn ( \"Found %s Group that exceed %d seconds! Not including!\" , new_group . huid , max_secs ) continue # Now Look at the track children - this links to the # PGDFs we are interested in. # We also add the track_id to the tracks_groups table # using our own UUID instead of the gids for c in g . children : binary_file = c . binary_file # Make sure the binary file is a single filename # with the pgdf extension binary_file = os . path . basename ( binary_file ) if binary_file [ - 5 :] != \".pgdf\" : binary_file += \".pgdf\" new_track = None with session . no_autoflush : q = session . query ( TrackGroup ) . filter ( TrackGroup . track_pam_id == c . uid , #TrackGroup.group_id == new_group.uid, # Commented as it's pam and binary that are unique identifiers before we add uids TrackGroup . binfile == binary_file , ) new_track = q . one_or_none () if new_track is None : new_track = TrackGroup ( track_id = uuid . uuid4 (), track_pam_id = c . uid , group_id = new_group . uid , binfile = binary_file , ) else : # update the new trackgroup with latest new_track . track_pam_id = c . uid new_track . binfile = binary_file found_tracks . append ( new_track ) if len ( found_tracks ) > 0 : found_groups . append ( new_group ) else : logging . warn ( \"Found %d Group with 0 tracks! Not including!\" , new_group . huid ) logging . info ( \"Found %d Groups and %d Tracks.\" , len ( found_groups ), len ( found_tracks )) earliest = found_groups [ 0 ] . timestart latest = found_groups [ 0 ] . timeend for group in found_groups : if group . timestart < earliest : earliest = group . timestart if group . timeend < earliest : latest = group . timeend logging . info ( \"Found %s Groups and %s Tracks.\" , str ( earliest ), str ( latest )) return ( found_groups , found_tracks ) fix_group_times ( new_groups , new_points ) PAMGuard SQLITE file reports incorrect group times. We therefore look at all the tracks and find the earliest and latest times and set the group times to match these. Parameters: new_groups ( List [ Groups ] ) \u2013 The latest groups we want to split. new_points ( List [ Points ] ) \u2013 The latest points we want to re-assign. Returns: List [ Groups ] \u2013 List[Groups]: The corrected groups. Source code in sealhits/sources/group.py 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 def fix_group_times ( new_groups : List [ Groups ], new_points : List [ Points ]) -> List [ Groups ]: \"\"\"PAMGuard SQLITE file reports incorrect group times. We therefore look at all the tracks and find the earliest and latest times and set the group times to match these. Args: new_groups (List[Groups]): The latest groups we want to split. new_points (List[Points]): The latest points we want to re-assign. Returns: List[Groups]: The corrected groups. \"\"\" logging . info ( \"Fixing group times...\" ) fixed_groups = [] for group in tqdm ( new_groups , desc = \"Groups fixed\" ): min_time = datetime . datetime . now () . astimezone ( tz = pytz . UTC ) max_time = datetime . datetime ( 2000 , 1 , 1 ) . astimezone ( tz = pytz . UTC ) # Check to see if we have points directly on this group # if we do then we can use these as the groups is points = [] for point in new_points : if point . group_id == group . uid : points . append ( point ) if len ( points ) > 0 : for point in points : if point . time < min_time : min_time = point . time if point . time > max_time : max_time = point . time group . timestart = min_time group . timeend = max_time fixed_groups . append ( group ) return fixed_groups split_groups ( session , new_groups , new_points , buffer_gap = 4 ) It is possible that some groups may have large gaps with no tracks. Such groups need to be split, creating new groups . This function returns a new list of all the groups including the original groups and the new splits, and the changed points. Split groups also buffers all groups including these that are not split, so each group has a blank area before and after it. TrackGroups are not changed. These are mostly irrelevant and are there to make matching up the original points to the original group easier. The group_id on the Point will point to the new group post split. Split groups also checks where the true start and end of the groups are as occasionally, some groups start earlier or finish later than the actual tracks for some reason. Parameters: session ( Session ) \u2013 The current SQLAlchemy session. new_groups ( List [ Groups ] ) \u2013 The latest groups we want to split. new_points ( List [ Points ] ) \u2013 The latest points we want to re-assign. buffer_gap ( int , default: 4 ) \u2013 The number of seconds for the start and end buffer. Also, the minimum gap between points in a track before we split. Returns: Tuple [ List [ Groups ], List [ Points ]] \u2013 Tuple[List[Groups], List[Points]]: The new_groups and new_point with the new split-off groups and reassigned points. Source code in sealhits/sources/group.py 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 def split_groups ( session : Session , new_groups : List [ Groups ], new_points : List [ Points ], buffer_gap = 4 ) -> Tuple [ List [ Groups ], List [ Points ]]: \"\"\"It is possible that some groups may have large gaps with no tracks. Such groups need to be split, creating new groups . This function returns a new list of all the groups including the original groups and the new splits, and the changed points. Split groups also buffers all groups including these that are not split, so each group has a blank area before and after it. TrackGroups are not changed. These are mostly irrelevant and are there to make matching up the original points to the original group easier. The group_id on the Point will point to the new group post split. Split groups also checks where the true start and end of the groups are as occasionally, some groups start earlier or finish later than the actual tracks for some reason. Args: session (Session): The current SQLAlchemy session. new_groups (List[Groups]): The latest groups we want to split. new_points (List[Points]): The latest points we want to re-assign. buffer_gap (int): The number of seconds for the start and end buffer. Also, the minimum gap between points in a track before we split. Returns: Tuple[List[Groups], List[Points]]: The new_groups and new_point with the new split-off groups and reassigned points. \"\"\" return_groups = [] return_points = [] buffer = datetime . timedelta ( seconds = buffer_gap ) for group in tqdm ( new_groups , desc = \"Splitting new groups.\" ): # Check to see if this group has already been split? #logging.info(\"Group split code %s %d\", group.huid, group.split) if group . split != - 1 : logging . info ( \"Group %s has already been split\" , group . huid ) # This group has already been assessed so get all splits then continue with session . no_autoflush : q = session . query ( Groups ) . filter ( Groups . gid == group . gid , Groups . sqliteid == group . sqliteid , Groups . sqlite == group . sqlite , ) new_groups = q . all () for group_two in new_groups : return_groups . append ( group_two ) continue # It hasn't been split, so check if it needs to be # First, get all the points and their times from each track. group_points = [] for point in new_points : if point . group_id == group . uid : group_points . append ( point ) # This should never happen but thanks to issues with data not where it should be # (disks 33 and 35) it does :/ Needs a fix at some point. if len ( group_points ) == 0 : logging . error ( \"No points found on group %s in split attempt.\" , group . huid ) continue #assert len(group_points) > 0 # We need to order times, and points, tpamids into ascending order of time group_points = sorted ( group_points , key = lambda x : x . time ) # Alter the start and end times of the groups to match the ones found # from the tracks. This is a 'belt-and-braces' sort of check given the one # problem group found in riverseals. new_group_start = group_points [ 0 ] . time new_group_end = group_points [ - 1 ] . time assert ( new_group_start >= group . timestart ) assert ( new_group_end <= group . timeend ) # Splits will contain the indices on which to split the # points into new tracks and the groups into new groups. # Group splits are the new group uids we've made. splits = [] group_splits = [] # Find the gaps and mark them up for tidx in range ( len ( group_points ) - 2 ): t0 = group_points [ tidx ] . time t1 = group_points [ tidx + 1 ] . time td = t1 - t0 if td > buffer : splits . append ( tidx + 1 ) group_time_end = group . timeend if len ( splits ) > 0 : # Redo the first group - setting it's time end to the correct one. group_time_end = group_points [ splits [ 0 ] - 1 ] . time group . timeend = group_time_end group . split = 0 group_splits . append ( group ) # Now create new groups with the same gid but new times for sidx , split in enumerate ( splits ): timestart = group_points [ split ] . time timeend = group_points [ - 1 ] . time # Set to the last time for now if sidx + 1 < len ( splits ): timeend = group_points [ splits [ sidx + 1 ] - 1 ] . time # Add the buffer times for the new group timestart -= buffer timeend += buffer # New group with new times. We keep the gid but use an incremented string for the sqlite filename # 0 in split is the original group (or parent group in this case) so we increment sidx by one as this new group is the 'first split' # TODO - we keep the same PGDFs here but eventually we'll need to double check these assert ( timestart < timeend ) new_group = None with session . no_autoflush : q = session . query ( Groups ) . filter ( Groups . gid == group . gid , Groups . sqliteid == group . sqliteid , Groups . split == sidx + 1 , Groups . sqlite == group . sqlite , ) new_group = q . one_or_none () if new_group is None : new_group = Groups ( uid = uuid . uuid4 (), gid = group . gid , sqliteid = group . sqliteid , timestart = timestart , timeend = timeend , sqlite = group . sqlite , code = group . code , comment = group . comment , interact = group . interact , mammal = group . mammal , fish = group . fish , bird = group . bird , split = sidx + 1 , huid = generate_id (), pgdfs = group . pgdfs , images = [], glfs = [], ) assert ( new_group is not None ) return_groups . append ( new_group ) group_splits . append ( new_group ) # We look at each point in turn, assigning it to a new group for point in group_points : for sgroup in group_splits : if point . time >= sgroup . timestart and point . time <= sgroup . timeend : point . group_id = sgroup . uid break return_groups . append ( group ) # Original group is always returned return_points += group_points return ( return_groups , return_points ) Sources model model.py - build a model from a current ingest. Function for creating the current model of the data from a particular sqlite import. build_model ( session , sqlname ) The goal is to build an existing model from the database ready to compare with later. Anything in this model not in the new one will need to be deleted. Parameters: session ( Session ) \u2013 The current SQLAlchemy session. sqlname ( str ) \u2013 the name of the sqlite file we are attempting to model. Returns: Tuple [ List [ Groups ], List [ TrackGroup ], List [ Points ], List [ PGDFS ], List [ GLFS ], List [ Images ]] \u2013 Tuple[List[Groups], List[TrackGroup], List[Points], List[PGDFS], List[GLFS], List[Images]]: Six lists of the major objects in the database model. Source code in sealhits/sources/model.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 def build_model ( session : Session , sqlname : str ) -> Tuple [ List [ Groups ], List [ TrackGroup ], List [ Points ], List [ PGDFS ], List [ GLFS ], List [ Images ] ]: \"\"\" The goal is to build an existing model from the database ready to compare with later. Anything in this model not in the new one will need to be deleted. Args: session (Session): The current SQLAlchemy session. sqlname (str): the name of the sqlite file we are attempting to model. Returns: Tuple[List[Groups], List[TrackGroup], List[Points], List[PGDFS], List[GLFS], List[Images]]: Six lists of the major objects in the database model. \"\"\" groups = session . query ( Groups ) . filter ( Groups . sqlite == sqlname ) . all () # The following are all dependent on the groups above. tracks = [] points = [] pgdfs = [] glfs = [] images = [] # TODO - must be a faster way? for group in groups : # Tracks first ts = session . query ( TrackGroup ) . filter ( TrackGroup . group_id == group . uid ) . all () for t in ts : tracks . append ( t ) # Now lookup points ps = session . query ( Points ) . filter ( Points . group_id == group . uid ) . all () for p in ps : points . append ( p ) # pgdfs, glfs and images - just the ones attached to the group pgdfs += group . pgdfs glfs += group . glfs images += group . images return ( groups , tracks , points , pgdfs , glfs , images ) Ingesting PGDFs model.py - build a model from a current ingest. Functions for processing the PGDFs as part of the ingest. process_pgdfs ( session , pgdf_path , pgdfs , groups , tracks ) Build up the PGDFS, Points and groups_pgdfs tables from the list of PGDFs provided. Parameters: session ( Session ) \u2013 The current SQLAlchemy session. pgdf_path ( str ) \u2013 The path to the PGDFs. pgdfs ( List [ str ] ) \u2013 The list of PGDF files we want. groups ( List [ Groups ] ) \u2013 The list of Groups from the current session. tracks ( List [ TrackGroup ] ) \u2013 The list of TrackGroup in the current session. Returns: Tuple [ List [ PGDFS ], List [ Points ]] \u2013 Tuple[List[PGDFS], List[Points]]: The new PGDFS objects and the new Points objects. Source code in sealhits/sources/pgdf.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 def process_pgdfs ( session : Session , pgdf_path : str , pgdfs : List [ str ], groups : List [ Groups ], tracks : List [ TrackGroup ], ) -> Tuple [ List [ PGDFS ], List [ Points ]]: \"\"\"Build up the PGDFS, Points and groups_pgdfs tables from the list of PGDFs provided. Args: session (Session): The current SQLAlchemy session. pgdf_path (str): The path to the PGDFs. pgdfs (List[str]): The list of PGDF files we want. groups (List[Groups]): The list of Groups from the current session. tracks (List[TrackGroup]): The list of TrackGroup in the current session. Returns: Tuple[List[PGDFS], List[Points]]: The new PGDFS objects and the new Points objects. \"\"\" # TODO - this should be a set of transactions that we apply to the db # so we don't muck things up if this process crashes. assert os . path . exists ( pgdf_path ) logging . info ( \"Process PGDFs starting...\" ) logging . info ( \"Creating pgdf -> group...\" ) full_paths = pgdfs_to_full_paths ( pgdf_path , pgdfs ) new_pgdfs = [] new_points = [] # Read all the PGDFs to get the start & end times for pgpath in tqdm ( full_paths , desc = \"Reading PGDFs\" ): fgname = os . path . basename ( pgpath ) tp = pgdf . PGDF ( pgpath ) date_min = None date_max = None for pamobj in tp . module . objects : tdate = pamobj . pam . date assert tdate is not None if date_min is None : date_min = tdate elif date_min > tdate : date_min = tdate if date_max is None : date_max = tdate elif date_max < tdate : date_max = tdate # Find the tracks_groups for the groups and file we are currently processing logging . info ( \"Creating tracks lookup for file %s \" , pgpath ) # Create a second dictionary for fast checking of tracks_groups # Dictionaries have O(1) access time apparently. Hashtable like I bet. tracks_used = {} # We also don't use the pamguard id but a distinct-across-pgdf-files uid # of our own, so we need that lookup too. track_pam_to_track = {} for tg in tracks : assert ( tg . track_pam_id not in tracks_used . keys ()) tracks_used [ tg . track_pam_id ] = 0 track_pam_to_track [ tg . track_pam_id ] = tg # Create a PGDF and add it to the new list q = session . query ( PGDFS ) . filter ( PGDFS . filename == fgname ) pgdf_entry = q . one_or_none () if pgdf_entry is None : pgdf_entry = PGDFS ( uid = uuid . uuid4 (), filename = fgname , startdate = date_min , enddate = date_max , ) else : # Update PGDF with new details pgdf_entry . startdate = date_min , pgdf_entry . enddate = date_max new_pgdfs . append ( pgdf_entry ) # Now, we read the points in this PGDF but we must # only include these that reference the tracks_groups table. # We can safely assume the tracks table is not as stupidly # large as the number of annotations is small. for pamobj in tp . module . objects : pam_track = pamobj . data pam_track_uid = pamobj . pam . UID try : track = pam_track . track track_obj = track_pam_to_track [ pam_track_uid ] group = None for g in groups : if g . uid == track_obj . group_id : group = g break # Should never happen but apparently, HDD 35 and 33 are sort of # mixed. Not sure what happened here but we just log and ignore if group is None : logging . error ( \"Missing group %s for trackgroup on pgdf %s \" , track_obj . group_id , fgname ) break #assert(group is not None) # Create the link between the group and pgdf if not already if group not in pgdf_entry . groups : pgdf_entry . groups . append ( group ) # Now insert all the points into the database for this known # track. for point in track . points : q = session . query ( Points ) . filter ( Points . time == point . time , Points . sonarid == point . sonar_id , Points . minbearing == point . min_bearing , Points . maxbearing == point . max_bearing , Points . minrange == point . min_range , Points . maxrange == point . max_range , Points . peakbearing == point . peak_bearing , Points . maxvalue == point . max_value , Points . occupancy == point . occupancy , Points . objsize == point . obj_size , Points . track_id == track_obj . track_id , ) new_point = q . one_or_none () # No need to run an update existing point as a point # is unique if any of it's attributes are different. if new_point is None : new_point = Points ( uid = uuid . uuid4 (), time = point . time , sonarid = point . sonar_id , minbearing = point . min_bearing , maxbearing = point . max_bearing , minrange = point . min_range , maxrange = point . max_range , peakbearing = point . peak_bearing , peakrange = point . peak_range , maxvalue = point . max_value , occupancy = point . occupancy , objsize = point . obj_size , track_id = track_obj . track_id , group_id = group . uid ) else : # Make sure this point has it's group_uid changed! new_point . time = point . time new_point . sonarid = point . sonar_id new_point . minbearing = point . min_bearing new_point . maxbearing = point . max_bearing new_point . minrange = point . min_range new_point . maxrange = point . max_range new_point . peakbearing = point . peak_bearing new_point . peakrange = point . peak_range new_point . maxvalue = point . max_value new_point . occupancy = point . occupancy new_point . objsize = point . obj_size new_point . track_id = track_obj . track_id new_point . group_id = group . uid new_points . append ( new_point ) except KeyError : # This track isn't important so skip # print(e) # TODO - this try except thing is a bit naughty! pass return ( new_pgdfs , new_points ) tracks_to_pgdfs ( tracks ) Take the created TrackGroup list and return the PGDFs we need. Parameters: tracks ( List [ TrackGroup ] ) \u2013 The TrackGroup list of interest. Returns: List [ str ] \u2013 List[str]: List of PGDF filenames we need to cover these TrackGroup. Source code in sealhits/sources/pgdf.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def tracks_to_pgdfs ( tracks : List [ TrackGroup ]) -> List [ str ]: \"\"\"Take the created TrackGroup list and return the PGDFs we need. Args: tracks (List[TrackGroup]): The TrackGroup list of interest. Returns: List[str]: List of PGDF filenames we need to cover these TrackGroup. \"\"\" pgdfs_required = [] for track in tracks : binfile = track . binfile if binfile not in pgdfs_required : pgdfs_required . append ( binfile ) return pgdfs_required Ingesting from sqlite sqlpam.py - Reading data from the annotation SQLite file. This module contains the following SQLPAM - a class representing the PAM SQLITE file. TrackChild - a class representing an individual track TrackGroup - a group of TrackChild representing something Examples: >>> from pypam.sqlpam import SQLPAM >>> sqlitepath = \"pam.sqlite3\" >>> assert os.path.exists(sqlitepath) >>> sqlpam = SQLPAM(sqlitepath) >>> print(sqlpam.tables) SQLPAM The class that represents the data held in the SQLite annotation database. This object will hold the TrackGroups and TrackChilds. Source code in sealhits/sources/sqlpam.py 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 class SQLPAM : \"\"\"The class that represents the data held in the SQLite annotation database. This object will hold the TrackGroups and TrackChilds.\"\"\" # TODO - just as with GLF, do we want to concat multiple files? # TODO - better checking for return values from the db (I.e missing) def __init__ ( self , sqlite_path ): \"\"\"Initialise our SQLPAM object Args: sqlite_path (str): full path and name of the sqlite_path file. \"\"\" con = sqlite3 . connect ( sqlite_path ) cur = con . cursor () res = cur . execute ( \"SELECT name FROM sqlite_master\" ) self . tables = [ t [ 0 ] for t in res . fetchall ()] self . track_groups = [] self . track_children = [] res = cur . execute ( \"SELECT ID, UID, UTC, PCLocalTime, PCTime, \\ ChannelBitmap, EndTime, DataCount, Track_Type, \\ Marine_Mammal, Fish, Bird, Interaction_with_blades, \\ Comment FROM Track_Groups\" ) tracks = res . fetchall () id_uid_lookup = {} # TODO - could do some inner join stuff here instead of python combine # Read the track groups first off for ( id , uid , utc , pclocal , pctime , cbitmap , endtime , dcount , ttype , mammal , fish , bird , interaction , comment , ) in tracks : # We need extra checks here to make sure # numerics are indeed numerics. SQLLite DB has a number # of mistakes. if mammal is None or not checks . is_float ( mammal ): mammal = - 1 if fish is None or not checks . is_float ( fish ): fish = - 1 if bird is None or not checks . is_float ( bird ): bird = - 1 interact = False # This is very annoying and silly if interaction is not None : if \"1\" in interaction : interact = True if \"0\" in interaction : interact = False elif not checks . is_float ( interaction ): interact = False elif interaction == 1 : interact = True elif interaction == 0 : interact = False tg = TrackGroup ( id , uid , utc , pclocal , pctime , cbitmap , endtime , dcount , ttype , comment , mammal , fish , bird , interact , ) self . track_groups . append ( tg ) # Use a composite key as only id and uid combined are unique id_uid_lookup [ str ( tg . id ) + \"-\" + str ( tg . uid )] = tg res = cur . execute ( \"SELECT UID, UTC, PCLocalTime, PCTime, \\ ChannelBitmap, parentID, parentUID, LongDataName, \\ BinaryFile FROM Track_Groups_Children where \\ BinaryFile is not null\" ) children = res . fetchall () # Read the track children, adding them to their parent for ( uid , utc , pclocal , pctime , cbitmap , parentid , parentuid , ldata , bfile , ) in children : tc = TrackChild ( uid , utc , pclocal , pctime , cbitmap , parentid , parentuid , ldata , bfile , ) self . track_children . append ( tc ) parent = id_uid_lookup [ str ( tc . parent_id ) + \"-\" + str ( tc . parent_uid )] parent . add_child ( tc ) __init__ ( sqlite_path ) Initialise our SQLPAM object Parameters: sqlite_path ( str ) \u2013 full path and name of the sqlite_path file. Source code in sealhits/sources/sqlpam.py 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 def __init__ ( self , sqlite_path ): \"\"\"Initialise our SQLPAM object Args: sqlite_path (str): full path and name of the sqlite_path file. \"\"\" con = sqlite3 . connect ( sqlite_path ) cur = con . cursor () res = cur . execute ( \"SELECT name FROM sqlite_master\" ) self . tables = [ t [ 0 ] for t in res . fetchall ()] self . track_groups = [] self . track_children = [] res = cur . execute ( \"SELECT ID, UID, UTC, PCLocalTime, PCTime, \\ ChannelBitmap, EndTime, DataCount, Track_Type, \\ Marine_Mammal, Fish, Bird, Interaction_with_blades, \\ Comment FROM Track_Groups\" ) tracks = res . fetchall () id_uid_lookup = {} # TODO - could do some inner join stuff here instead of python combine # Read the track groups first off for ( id , uid , utc , pclocal , pctime , cbitmap , endtime , dcount , ttype , mammal , fish , bird , interaction , comment , ) in tracks : # We need extra checks here to make sure # numerics are indeed numerics. SQLLite DB has a number # of mistakes. if mammal is None or not checks . is_float ( mammal ): mammal = - 1 if fish is None or not checks . is_float ( fish ): fish = - 1 if bird is None or not checks . is_float ( bird ): bird = - 1 interact = False # This is very annoying and silly if interaction is not None : if \"1\" in interaction : interact = True if \"0\" in interaction : interact = False elif not checks . is_float ( interaction ): interact = False elif interaction == 1 : interact = True elif interaction == 0 : interact = False tg = TrackGroup ( id , uid , utc , pclocal , pctime , cbitmap , endtime , dcount , ttype , comment , mammal , fish , bird , interact , ) self . track_groups . append ( tg ) # Use a composite key as only id and uid combined are unique id_uid_lookup [ str ( tg . id ) + \"-\" + str ( tg . uid )] = tg res = cur . execute ( \"SELECT UID, UTC, PCLocalTime, PCTime, \\ ChannelBitmap, parentID, parentUID, LongDataName, \\ BinaryFile FROM Track_Groups_Children where \\ BinaryFile is not null\" ) children = res . fetchall () # Read the track children, adding them to their parent for ( uid , utc , pclocal , pctime , cbitmap , parentid , parentuid , ldata , bfile , ) in children : tc = TrackChild ( uid , utc , pclocal , pctime , cbitmap , parentid , parentuid , ldata , bfile , ) self . track_children . append ( tc ) parent = id_uid_lookup [ str ( tc . parent_id ) + \"-\" + str ( tc . parent_uid )] parent . add_child ( tc ) TrackChild The class representing the individual track from the PAMGUARD pgdf binary file. It contains a UID link to the gemini object from the PGDF. Source code in sealhits/sources/sqlpam.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 class TrackChild : \"\"\"The class representing the individual track from the PAMGUARD pgdf binary file. It contains a UID link to the gemini object from the PGDF.\"\"\" def __init__ ( self , uid : int , utc : str , pc_local : str , pc_time : str , channel : int , parent_id : int , parent_uid : int , long_data_name : str , binary_file : str , ): \"\"\"Initialise our TrackChild Object. Args: uid (int): a unique identifier matching the PGDF binary track. utc (str): a string representing the time in UTC. pc_local (str): the local time on the pc recording. pc_time (str): the time on the pc recording. channel (int): unknown. parent_id (int): The ID of the TrackGroup to which this child belongs. parent_uid (int): The UID of the TrackGroup to which this child belongs. long_data_name (str): unknown. binary_file (str): The filename of the PGDF that holds this track. \"\"\" self . uid = uid self . utc = datetime . fromisoformat ( utc ) self . utc = pytz . utc . localize ( self . utc ) self . pc_local_time = pc_local self . pc_time = pc_time self . channel_bitmap = channel self . parent_id = parent_id self . parent_uid = parent_uid self . long_data_name = long_data_name self . parent = None # Fix for the bug in pamguard with the binary file if binary_file [: 2 ] == \"i_\" : self . binary_file = \"Gemin\" + binary_file def _add_parent ( self , parent : TrackGroup ): self . parent = parent def __str__ ( self ): return ( str ( self . uid ) + \",\" + str ( self . utc ) + \",\" + str ( self . pc_local_time ) + \",\" + str ( self . pc_time ) + \",\" + str ( self . channel_bitmap ) + \",\" + str ( self . parent_id ) + \",\" + str ( self . parent_uid ) + \",\" + str ( self . long_data_name ) + \",\" + str ( self . binary_file ) ) __init__ ( uid , utc , pc_local , pc_time , channel , parent_id , parent_uid , long_data_name , binary_file ) Initialise our TrackChild Object. Parameters: uid ( int ) \u2013 a unique identifier matching the PGDF binary track. utc ( str ) \u2013 a string representing the time in UTC. pc_local ( str ) \u2013 the local time on the pc recording. pc_time ( str ) \u2013 the time on the pc recording. channel ( int ) \u2013 unknown. parent_id ( int ) \u2013 The ID of the TrackGroup to which this child belongs. parent_uid ( int ) \u2013 The UID of the TrackGroup to which this child belongs. long_data_name ( str ) \u2013 unknown. binary_file ( str ) \u2013 The filename of the PGDF that holds this track. Source code in sealhits/sources/sqlpam.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 def __init__ ( self , uid : int , utc : str , pc_local : str , pc_time : str , channel : int , parent_id : int , parent_uid : int , long_data_name : str , binary_file : str , ): \"\"\"Initialise our TrackChild Object. Args: uid (int): a unique identifier matching the PGDF binary track. utc (str): a string representing the time in UTC. pc_local (str): the local time on the pc recording. pc_time (str): the time on the pc recording. channel (int): unknown. parent_id (int): The ID of the TrackGroup to which this child belongs. parent_uid (int): The UID of the TrackGroup to which this child belongs. long_data_name (str): unknown. binary_file (str): The filename of the PGDF that holds this track. \"\"\" self . uid = uid self . utc = datetime . fromisoformat ( utc ) self . utc = pytz . utc . localize ( self . utc ) self . pc_local_time = pc_local self . pc_time = pc_time self . channel_bitmap = channel self . parent_id = parent_id self . parent_uid = parent_uid self . long_data_name = long_data_name self . parent = None # Fix for the bug in pamguard with the binary file if binary_file [: 2 ] == \"i_\" : self . binary_file = \"Gemin\" + binary_file TrackGroup A group of TrackChild that may have annotations. Source code in sealhits/sources/sqlpam.py 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 class TrackGroup : \"\"\"A group of TrackChild that may have annotations.\"\"\" def __init__ ( self , id : int , uid : int , utc : str , pc_local : str , pc_time : str , channel : int , end_time : str , dc : int , track_type : str , comment : str , mammal : int , fish : int , bird : int , interact : bool , ): \"\"\"Initialise our TrackGroup Object. Args: uid (int): a unique identifier matching the PGDF binary track, although not that unique it seems :/ utc (str): a string representing the time in UTC. utc_milli (int): an int number of milliseconds since the epoch in utc. pc_local (str): the local time on the pc recording. pc_time (str): the time on the pc recording. channel (int): unknown. end_time (str): When does the last Track in this group finish? dc (int): unknown. track_type (str): unknown. comment (str): mammal (int): Was this trackgroup a mammal? fish (int): Was this trackgroup a fish? bird (int): Was this trackgroup a bird? interact (bool): Did this trackgroup interact with the turbine? \"\"\" self . id = id self . uid = uid self . utc = datetime . fromisoformat ( utc ) self . utc = pytz . utc . localize ( self . utc ) self . pc_local_time = pc_local self . pc_time = pc_time self . channel_bitmap = channel self . end_time = datetime . fromisoformat ( end_time ) self . end_time = pytz . utc . localize ( self . end_time ) self . data_count = dc self . track_type = track_type # TODO - enum self . comment = comment self . mammal = mammal self . fish = fish self . bird = bird self . interaction = interact self . children = [] def __str__ ( self ): return ( str ( self . id ) + \",\" + str ( self . uid ) + \",\" + str ( self . utc ) + \",\" + str ( self . pc_local_time ) + \",\" + str ( self . pc_time ) + \",\" + str ( self . channel_bitmap ) + \",\" + str ( self . end_time ) + \",\" + str ( self . data_count ) + \",\" + str ( self . track_type ) + \",\" + str ( self . comment ) + \",\" + str ( self . marine_mammal ) + \",\" + str ( self . fish ) + \",\" + str ( self . bird ) + \",\" + str ( self . interaction ) + \",\" ) def add_child ( self , child : TrackChild ): \"\"\"Add a TrackChild to this TrackGroup. Args: child (TrackChild): a TrackChild object to add. \"\"\" assert child . parent_uid == self . uid self . children . append ( child ) child . _add_parent ( self ) return self __init__ ( id , uid , utc , pc_local , pc_time , channel , end_time , dc , track_type , comment , mammal , fish , bird , interact ) Initialise our TrackGroup Object. Args: uid (int): a unique identifier matching the PGDF binary track, although not that unique it seems :/ utc (str): a string representing the time in UTC. utc_milli (int): an int number of milliseconds since the epoch in utc. pc_local (str): the local time on the pc recording. pc_time (str): the time on the pc recording. channel (int): unknown. end_time (str): When does the last Track in this group finish? dc (int): unknown. track_type (str): unknown. comment (str): mammal (int): Was this trackgroup a mammal? fish (int): Was this trackgroup a fish? bird (int): Was this trackgroup a bird? interact (bool): Did this trackgroup interact with the turbine? Source code in sealhits/sources/sqlpam.py 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 def __init__ ( self , id : int , uid : int , utc : str , pc_local : str , pc_time : str , channel : int , end_time : str , dc : int , track_type : str , comment : str , mammal : int , fish : int , bird : int , interact : bool , ): \"\"\"Initialise our TrackGroup Object. Args: uid (int): a unique identifier matching the PGDF binary track, although not that unique it seems :/ utc (str): a string representing the time in UTC. utc_milli (int): an int number of milliseconds since the epoch in utc. pc_local (str): the local time on the pc recording. pc_time (str): the time on the pc recording. channel (int): unknown. end_time (str): When does the last Track in this group finish? dc (int): unknown. track_type (str): unknown. comment (str): mammal (int): Was this trackgroup a mammal? fish (int): Was this trackgroup a fish? bird (int): Was this trackgroup a bird? interact (bool): Did this trackgroup interact with the turbine? \"\"\" self . id = id self . uid = uid self . utc = datetime . fromisoformat ( utc ) self . utc = pytz . utc . localize ( self . utc ) self . pc_local_time = pc_local self . pc_time = pc_time self . channel_bitmap = channel self . end_time = datetime . fromisoformat ( end_time ) self . end_time = pytz . utc . localize ( self . end_time ) self . data_count = dc self . track_type = track_type # TODO - enum self . comment = comment self . mammal = mammal self . fish = fish self . bird = bird self . interaction = interact self . children = [] add_child ( child ) Add a TrackChild to this TrackGroup. Parameters: child ( TrackChild ) \u2013 a TrackChild object to add. Source code in sealhits/sources/sqlpam.py 190 191 192 193 194 195 196 197 198 199 def add_child ( self , child : TrackChild ): \"\"\"Add a TrackChild to this TrackGroup. Args: child (TrackChild): a TrackChild object to add. \"\"\" assert child . parent_uid == self . uid self . children . append ( child ) child . _add_parent ( self ) return self","title":"Reference and API"},{"location":"reference/#reference-and-api","text":"This page covers the various functions inside the sealhits project and is intended for Python programmers. There are two modules - sealhits and sealsources. Sealhits covers the database and pulling the data out for processing. Sealsources covers pulling the data in from the various databases, pgdfs and glf files.","title":"Reference and API"},{"location":"reference/#bounding-boxes","text":"bbox.py - Classes and functions related to bounding boxes.","title":"Bounding Boxes"},{"location":"reference/#sealhits.bbox.BearBox","text":"Source code in sealhits/bbox.py 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 class BearBox : def __init__ ( self , bearmin : float , bearmax : float , distmin : float , distmax : float , sonar_range : float , ): \"\"\"Bearings are in radians and are left-/right+ of the vertical / Y axis, but from the top. Sonar range is the range the sonar was set to in metres. Args: bearmin (float): The minimum bearing. bearmax (float): The maximum bearing. distmin (float): The minimum distance. distmax (float): The maximum distance. sonar_range (float): The range of the sonar at this point (in metres). \"\"\" self . bearing_min = bearmin self . bearing_max = bearmax self . dist_min = distmin self . dist_max = distmax self . sonar_range = sonar_range def to_xy_raw ( self , image_size : Tuple [ int , int ]) -> XYBox : \"\"\"Return a bearing box that is x,y but for the RAW, non-fan image. This image may be resized from the original but is still a rectangle. with no spatial distortion. Args: image_size (Tuple[int, int]): the size of the image this raw box belongs to (in pixels, width then height). Returns: XYBox: A new XYBox but within the *raw* image space (i.e original, not fan/polar transformed) \"\"\" from sealhits.btable import bearing_table def _find_idx ( c ): for i in range ( len ( bearing_table ) - 1 ): a = bearing_table [ i ] b = bearing_table [ i + 1 ] if a >= c and b < c : return i return 0 # xmin = int(((-self.bearing_max - math.radians(MIN_ANGLE)) / (math.radians(MAX_ANGLE) - math.radians(MIN_ANGLE))) * image_size[0]) # xmax = int(((-self.bearing_min - math.radians(MIN_ANGLE)) / (math.radians(MAX_ANGLE) - math.radians(MIN_ANGLE))) * image_size[0]) r = float ( image_size [ 0 ]) / float ( len ( bearing_table )) xmin = int ( _find_idx ( self . bearing_max ) * r ) # Swap due to the bearings being postive to negative xmax = int ( _find_idx ( self . bearing_min ) * r ) ymin = int ( self . dist_min / self . sonar_range * image_size [ 1 ]) ymax = int ( self . dist_max / self . sonar_range * image_size [ 1 ]) return XYBox ( xmin , ymin , xmax , ymax ) def to_xy ( self , image_size : Tuple [ int , int ]) -> XYBox : \"\"\"Given a fan/polar image size return the minx/y maxx/y in pixel coordinates. Image size is width/height. Resulting xy assumes origin at top left. Args: image_size (Tuple[int, int]): the size of the image this raw box belongs to (in pixels, width then height). Returns: XYBox: A new XYBox but within the *polar* image space (i.e fan/polar transformed, not raw rectangle) \"\"\" txy = [] txy . append ( dist_bearing_to_xy ( self . bearing_min , self . dist_min , self . sonar_range , image_size ) ) txy . append ( dist_bearing_to_xy ( self . bearing_max , self . dist_max , self . sonar_range , image_size ) ) txy . append ( dist_bearing_to_xy ( self . bearing_min , self . dist_max , self . sonar_range , image_size ) ) txy . append ( dist_bearing_to_xy ( self . bearing_max , self . dist_min , self . sonar_range , image_size ) ) min_x = txy [ 0 ][ 0 ] min_y = txy [ 0 ][ 1 ] max_x = txy [ 0 ][ 0 ] max_y = txy [ 0 ][ 1 ] for tt in txy [ 1 :]: if tt [ 0 ] < min_x : min_x = tt [ 0 ] if tt [ 0 ] > max_x : max_x = tt [ 0 ] if tt [ 1 ] < min_y : min_y = tt [ 1 ] if tt [ 1 ] > max_y : max_y = tt [ 1 ] return XYBox ( min_x , min_y , max_x , max_y ) def __str__ ( self ): return ( str ( self . bearing_min ) + \",\" + str ( self . bearing_max ) + \",\" + str ( self . dist_min ) + \",\" + str ( self . dist_max ) )","title":"BearBox"},{"location":"reference/#sealhits.bbox.BearBox.__init__","text":"Bearings are in radians and are left-/right+ of the vertical / Y axis, but from the top. Sonar range is the range the sonar was set to in metres. Parameters: bearmin ( float ) \u2013 The minimum bearing. bearmax ( float ) \u2013 The maximum bearing. distmin ( float ) \u2013 The minimum distance. distmax ( float ) \u2013 The maximum distance. sonar_range ( float ) \u2013 The range of the sonar at this point (in metres). Source code in sealhits/bbox.py 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 def __init__ ( self , bearmin : float , bearmax : float , distmin : float , distmax : float , sonar_range : float , ): \"\"\"Bearings are in radians and are left-/right+ of the vertical / Y axis, but from the top. Sonar range is the range the sonar was set to in metres. Args: bearmin (float): The minimum bearing. bearmax (float): The maximum bearing. distmin (float): The minimum distance. distmax (float): The maximum distance. sonar_range (float): The range of the sonar at this point (in metres). \"\"\" self . bearing_min = bearmin self . bearing_max = bearmax self . dist_min = distmin self . dist_max = distmax self . sonar_range = sonar_range","title":"__init__"},{"location":"reference/#sealhits.bbox.BearBox.to_xy","text":"Given a fan/polar image size return the minx/y maxx/y in pixel coordinates. Image size is width/height. Resulting xy assumes origin at top left. Parameters: image_size ( Tuple [ int , int ] ) \u2013 the size of the image this raw box belongs to (in pixels, width then height). Returns: XYBox ( XYBox ) \u2013 A new XYBox but within the polar image space (i.e fan/polar transformed, not raw rectangle) Source code in sealhits/bbox.py 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 def to_xy ( self , image_size : Tuple [ int , int ]) -> XYBox : \"\"\"Given a fan/polar image size return the minx/y maxx/y in pixel coordinates. Image size is width/height. Resulting xy assumes origin at top left. Args: image_size (Tuple[int, int]): the size of the image this raw box belongs to (in pixels, width then height). Returns: XYBox: A new XYBox but within the *polar* image space (i.e fan/polar transformed, not raw rectangle) \"\"\" txy = [] txy . append ( dist_bearing_to_xy ( self . bearing_min , self . dist_min , self . sonar_range , image_size ) ) txy . append ( dist_bearing_to_xy ( self . bearing_max , self . dist_max , self . sonar_range , image_size ) ) txy . append ( dist_bearing_to_xy ( self . bearing_min , self . dist_max , self . sonar_range , image_size ) ) txy . append ( dist_bearing_to_xy ( self . bearing_max , self . dist_min , self . sonar_range , image_size ) ) min_x = txy [ 0 ][ 0 ] min_y = txy [ 0 ][ 1 ] max_x = txy [ 0 ][ 0 ] max_y = txy [ 0 ][ 1 ] for tt in txy [ 1 :]: if tt [ 0 ] < min_x : min_x = tt [ 0 ] if tt [ 0 ] > max_x : max_x = tt [ 0 ] if tt [ 1 ] < min_y : min_y = tt [ 1 ] if tt [ 1 ] > max_y : max_y = tt [ 1 ] return XYBox ( min_x , min_y , max_x , max_y )","title":"to_xy"},{"location":"reference/#sealhits.bbox.BearBox.to_xy_raw","text":"Return a bearing box that is x,y but for the RAW, non-fan image. This image may be resized from the original but is still a rectangle. with no spatial distortion. Parameters: image_size ( Tuple [ int , int ] ) \u2013 the size of the image this raw box belongs to (in pixels, width then height). Returns: XYBox ( XYBox ) \u2013 A new XYBox but within the raw image space (i.e original, not fan/polar transformed) Source code in sealhits/bbox.py 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 def to_xy_raw ( self , image_size : Tuple [ int , int ]) -> XYBox : \"\"\"Return a bearing box that is x,y but for the RAW, non-fan image. This image may be resized from the original but is still a rectangle. with no spatial distortion. Args: image_size (Tuple[int, int]): the size of the image this raw box belongs to (in pixels, width then height). Returns: XYBox: A new XYBox but within the *raw* image space (i.e original, not fan/polar transformed) \"\"\" from sealhits.btable import bearing_table def _find_idx ( c ): for i in range ( len ( bearing_table ) - 1 ): a = bearing_table [ i ] b = bearing_table [ i + 1 ] if a >= c and b < c : return i return 0 # xmin = int(((-self.bearing_max - math.radians(MIN_ANGLE)) / (math.radians(MAX_ANGLE) - math.radians(MIN_ANGLE))) * image_size[0]) # xmax = int(((-self.bearing_min - math.radians(MIN_ANGLE)) / (math.radians(MAX_ANGLE) - math.radians(MIN_ANGLE))) * image_size[0]) r = float ( image_size [ 0 ]) / float ( len ( bearing_table )) xmin = int ( _find_idx ( self . bearing_max ) * r ) # Swap due to the bearings being postive to negative xmax = int ( _find_idx ( self . bearing_min ) * r ) ymin = int ( self . dist_min / self . sonar_range * image_size [ 1 ]) ymax = int ( self . dist_max / self . sonar_range * image_size [ 1 ]) return XYBox ( xmin , ymin , xmax , ymax )","title":"to_xy_raw"},{"location":"reference/#sealhits.bbox.XYBox","text":"A basic XY bounding box using minimums and maximums. Source code in sealhits/bbox.py 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 class XYBox : \"\"\"A basic XY bounding box using minimums and maximums.\"\"\" def __init__ ( self , minx : int , miny : int , maxx : int , maxy : int ): \"\"\"Create a 2D Bounding Box. Args: minx(int): minimum x value in pixels. miny(int): minimum y value in pixels. maxx(int): maximum x value in pixels. maxy(int): maximum y value in pixels. \"\"\" self . x_min = minx self . y_min = miny self . x_max = maxx self . y_max = maxy def __str__ ( self ): return ( str ( self . x_min ) + \",\" + str ( self . y_min ) + \",\" + str ( self . x_max ) + \",\" + str ( self . y_max ) ) def area ( self ) -> int : \"\"\"Return the area of this bbox. Args: None Returns: int: the area \"\"\" return ( self . x_max - self . x_min ) * ( self . y_max - self . y_min ) def tuple ( self ) -> Tuple [ int , int , int , int ]: \"\"\"Return the box as a Tuple Args: None Returns: Tuple[int,int,int,int] \"\"\" return ( self . x_min , self . y_min , self . x_max , self . y_max ) def pair ( self ) -> Tuple [ Tuple [ int , int ], Tuple [ int , int ]]: \"\"\"Return the box as two Tuples Returns: Tuple[Tuple[int,int],Tuple[int,int]]: the box as [xmin,ymin],[xmax,ymax] \"\"\" return (( self . x_min , self . y_min ), ( self . x_max , self . y_max )) def com ( self ) -> Tuple [ int , int ]: \"\"\"Return the centre of this bbox. Args: None Returns: Tuple[int,int]: the centre of this bounding box, rounded and as ints. \"\"\" return ( int (( self . x_max + self . x_min ) / 2 ), int (( self . y_max + self . y_min ) / 2 )) def flipv ( self , img_height ): \"\"\"An in-place flip vertically. Args: img_height (int): the height of the image to which this box belongs. \"\"\" tt = self . y_max self . y_max = img_height - self . y_min self . y_min = img_height - tt return self def equals ( self , b ) -> bool : \"\"\"Does this box equal another? Args: b (XYBox): the other box to compare against Returns: bool \"\"\" return ( self . x_min == b . x_min and self . x_max == b . x_max and self . y_min == b . y_min and self . y_max == b . y_max )","title":"XYBox"},{"location":"reference/#sealhits.bbox.XYBox.__init__","text":"Create a 2D Bounding Box. Args: minx(int): minimum x value in pixels. miny(int): minimum y value in pixels. maxx(int): maximum x value in pixels. maxy(int): maximum y value in pixels. Source code in sealhits/bbox.py 134 135 136 137 138 139 140 141 142 143 144 145 def __init__ ( self , minx : int , miny : int , maxx : int , maxy : int ): \"\"\"Create a 2D Bounding Box. Args: minx(int): minimum x value in pixels. miny(int): minimum y value in pixels. maxx(int): maximum x value in pixels. maxy(int): maximum y value in pixels. \"\"\" self . x_min = minx self . y_min = miny self . x_max = maxx self . y_max = maxy","title":"__init__"},{"location":"reference/#sealhits.bbox.XYBox.area","text":"Return the area of this bbox. Args: None Returns: int ( int ) \u2013 the area Source code in sealhits/bbox.py 158 159 160 161 162 163 164 165 166 def area ( self ) -> int : \"\"\"Return the area of this bbox. Args: None Returns: int: the area \"\"\" return ( self . x_max - self . x_min ) * ( self . y_max - self . y_min )","title":"area"},{"location":"reference/#sealhits.bbox.XYBox.com","text":"Return the centre of this bbox. Args: None Returns: Tuple [ int , int ] \u2013 Tuple[int,int]: the centre of this bounding box, rounded and as ints. Source code in sealhits/bbox.py 186 187 188 189 190 191 192 193 194 def com ( self ) -> Tuple [ int , int ]: \"\"\"Return the centre of this bbox. Args: None Returns: Tuple[int,int]: the centre of this bounding box, rounded and as ints. \"\"\" return ( int (( self . x_max + self . x_min ) / 2 ), int (( self . y_max + self . y_min ) / 2 ))","title":"com"},{"location":"reference/#sealhits.bbox.XYBox.equals","text":"Does this box equal another? Args: b (XYBox): the other box to compare against Returns: bool \u2013 bool Source code in sealhits/bbox.py 207 208 209 210 211 212 213 214 215 216 217 218 219 220 def equals ( self , b ) -> bool : \"\"\"Does this box equal another? Args: b (XYBox): the other box to compare against Returns: bool \"\"\" return ( self . x_min == b . x_min and self . x_max == b . x_max and self . y_min == b . y_min and self . y_max == b . y_max )","title":"equals"},{"location":"reference/#sealhits.bbox.XYBox.flipv","text":"An in-place flip vertically. Parameters: img_height ( int ) \u2013 the height of the image to which this box belongs. Source code in sealhits/bbox.py 196 197 198 199 200 201 202 203 204 205 def flipv ( self , img_height ): \"\"\"An in-place flip vertically. Args: img_height (int): the height of the image to which this box belongs. \"\"\" tt = self . y_max self . y_max = img_height - self . y_min self . y_min = img_height - tt return self","title":"flipv"},{"location":"reference/#sealhits.bbox.XYBox.pair","text":"Return the box as two Tuples Returns: Tuple [ Tuple [ int , int ], Tuple [ int , int ]] \u2013 Tuple[Tuple[int,int],Tuple[int,int]]: the box as [xmin,ymin],[xmax,ymax] Source code in sealhits/bbox.py 178 179 180 181 182 183 184 def pair ( self ) -> Tuple [ Tuple [ int , int ], Tuple [ int , int ]]: \"\"\"Return the box as two Tuples Returns: Tuple[Tuple[int,int],Tuple[int,int]]: the box as [xmin,ymin],[xmax,ymax] \"\"\" return (( self . x_min , self . y_min ), ( self . x_max , self . y_max ))","title":"pair"},{"location":"reference/#sealhits.bbox.XYBox.tuple","text":"Return the box as a Tuple Args: None Returns: Tuple [ int , int , int , int ] \u2013 Tuple[int,int,int,int] Source code in sealhits/bbox.py 168 169 170 171 172 173 174 175 176 def tuple ( self ) -> Tuple [ int , int , int , int ]: \"\"\"Return the box as a Tuple Args: None Returns: Tuple[int,int,int,int] \"\"\" return ( self . x_min , self . y_min , self . x_max , self . y_max )","title":"tuple"},{"location":"reference/#sealhits.bbox.XYZBox","text":"Source code in sealhits/bbox.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 class XYZBox : def __init__ ( self , minx : int , miny : int , minz : int , maxx : int , maxy : int , maxz : int ): \"\"\"Initialise a 3D bounding box. Args: minx (int): minimum x value in pixels. miny (int): minimum y value in pixels. minz (int): minimum z value in pixels. maxx (int): maximum x value in pixels. maxy (int): maximum y value in pixels. maxz (int): maximum z value in pixels. \"\"\" self . x_min = minx self . y_min = miny self . z_min = minz self . x_max = maxx self . y_max = maxy self . z_max = maxz def __str__ ( self ): return ( str ( self . x_min ) + \",\" + str ( self . y_min ) + \",\" + str ( self . z_min ) + \",\" + str ( self . x_max ) + \",\" + str ( self . y_max ) + \",\" + str ( self . z_max ) ) def volume ( self ) -> int : \"\"\"Return the volume of this bbox. Args: None Returns: int: the volume \"\"\" # Z is always a minimum of 1, as zmax can equal zmin - this XYZBox is ultimately an XYBox return ( ( self . x_max - self . x_min ) * ( self . y_max - self . y_min ) * max ( self . z_max - self . z_min , 1 ) ) def tuple ( self ) -> Tuple [ int , int , int , int , int , int ]: \"\"\"Return the box as a Tuple Args: None Returns: Tuple[int,int,int,int,int,int]: the volume as xmin,ymin,zmin,xmax,ymax,zmax \"\"\" return ( self . x_min , self . y_min , self . z_min , self . x_max , self . y_max , self . y_max ) def pair ( self ) -> Tuple [ Tuple [ int , int , int ], Tuple [ int , int , int ]]: \"\"\"Return the box as two Tuples Args: None Returns: Tuple[Tuple[int,int,int],Tuple[int,int,int]]: the volume as [xmin,ymin,zmin],[xmax,ymax,zmax] \"\"\" return ( ( self . x_min , self . y_min , self . z_min ), ( self . x_max , self . y_max , self . z_max ), ) def com ( self ) -> Tuple [ int , int , int ]: \"\"\"Return the centre of this bbox. Args: None Returns: Tuple[int,int,int]: the centre of this bounding box, rounded and as ints. \"\"\" return ( int (( self . x_max + self . x_min ) / 2 ), int (( self . y_max + self . y_min ) / 2 ), int (( self . z_max + self . z_min ) / 2 ), ) def equals ( self , b : XYZBox ) -> bool : \"\"\"Does this box equal another? Args: b (XYZBox): the other box to compare against Returns: bool \"\"\" return ( self . x_min == b . x_min and self . x_max == b . x_max and self . y_min == b . y_min and self . y_max == b . y_max and self . z_min == b . z_min and self . z_max == b . z_max )","title":"XYZBox"},{"location":"reference/#sealhits.bbox.XYZBox.__init__","text":"Initialise a 3D bounding box. Parameters: minx ( int ) \u2013 minimum x value in pixels. miny ( int ) \u2013 minimum y value in pixels. minz ( int ) \u2013 minimum z value in pixels. maxx ( int ) \u2013 maximum x value in pixels. maxy ( int ) \u2013 maximum y value in pixels. maxz ( int ) \u2013 maximum z value in pixels. Source code in sealhits/bbox.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 def __init__ ( self , minx : int , miny : int , minz : int , maxx : int , maxy : int , maxz : int ): \"\"\"Initialise a 3D bounding box. Args: minx (int): minimum x value in pixels. miny (int): minimum y value in pixels. minz (int): minimum z value in pixels. maxx (int): maximum x value in pixels. maxy (int): maximum y value in pixels. maxz (int): maximum z value in pixels. \"\"\" self . x_min = minx self . y_min = miny self . z_min = minz self . x_max = maxx self . y_max = maxy self . z_max = maxz","title":"__init__"},{"location":"reference/#sealhits.bbox.XYZBox.com","text":"Return the centre of this bbox. Args: None Returns: Tuple [ int , int , int ] \u2013 Tuple[int,int,int]: the centre of this bounding box, rounded and as ints. Source code in sealhits/bbox.py 99 100 101 102 103 104 105 106 107 108 109 110 111 def com ( self ) -> Tuple [ int , int , int ]: \"\"\"Return the centre of this bbox. Args: None Returns: Tuple[int,int,int]: the centre of this bounding box, rounded and as ints. \"\"\" return ( int (( self . x_max + self . x_min ) / 2 ), int (( self . y_max + self . y_min ) / 2 ), int (( self . z_max + self . z_min ) / 2 ), )","title":"com"},{"location":"reference/#sealhits.bbox.XYZBox.equals","text":"Does this box equal another? Args: b (XYZBox): the other box to compare against Returns: bool \u2013 bool Source code in sealhits/bbox.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 def equals ( self , b : XYZBox ) -> bool : \"\"\"Does this box equal another? Args: b (XYZBox): the other box to compare against Returns: bool \"\"\" return ( self . x_min == b . x_min and self . x_max == b . x_max and self . y_min == b . y_min and self . y_max == b . y_max and self . z_min == b . z_min and self . z_max == b . z_max )","title":"equals"},{"location":"reference/#sealhits.bbox.XYZBox.pair","text":"Return the box as two Tuples Args: None Returns: Tuple [ Tuple [ int , int , int ], Tuple [ int , int , int ]] \u2013 Tuple[Tuple[int,int,int],Tuple[int,int,int]]: the volume as [xmin,ymin,zmin],[xmax,ymax,zmax] Source code in sealhits/bbox.py 86 87 88 89 90 91 92 93 94 95 96 97 def pair ( self ) -> Tuple [ Tuple [ int , int , int ], Tuple [ int , int , int ]]: \"\"\"Return the box as two Tuples Args: None Returns: Tuple[Tuple[int,int,int],Tuple[int,int,int]]: the volume as [xmin,ymin,zmin],[xmax,ymax,zmax] \"\"\" return ( ( self . x_min , self . y_min , self . z_min ), ( self . x_max , self . y_max , self . z_max ), )","title":"pair"},{"location":"reference/#sealhits.bbox.XYZBox.tuple","text":"Return the box as a Tuple Args: None Returns: Tuple [ int , int , int , int , int , int ] \u2013 Tuple[int,int,int,int,int,int]: the volume as xmin,ymin,zmin,xmax,ymax,zmax Source code in sealhits/bbox.py 76 77 78 79 80 81 82 83 84 def tuple ( self ) -> Tuple [ int , int , int , int , int , int ]: \"\"\"Return the box as a Tuple Args: None Returns: Tuple[int,int,int,int,int,int]: the volume as xmin,ymin,zmin,xmax,ymax,zmax \"\"\" return ( self . x_min , self . y_min , self . z_min , self . x_max , self . y_max , self . y_max )","title":"tuple"},{"location":"reference/#sealhits.bbox.XYZBox.volume","text":"Return the volume of this bbox. Args: None Returns: int ( int ) \u2013 the volume Source code in sealhits/bbox.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def volume ( self ) -> int : \"\"\"Return the volume of this bbox. Args: None Returns: int: the volume \"\"\" # Z is always a minimum of 1, as zmax can equal zmin - this XYZBox is ultimately an XYBox return ( ( self . x_max - self . x_min ) * ( self . y_max - self . y_min ) * max ( self . z_max - self . z_min , 1 ) )","title":"volume"},{"location":"reference/#sealhits.bbox.bb_combine","text":"Combine two bounding boxes into one. Either XYBox or XYZBox. Parameters: a ( Union [ XYBox , XYZBox ] ) \u2013 the XYBox or XYZBox. b ( Union [ XYBox , XYZBox ] ) \u2013 the XYBox or XYZBox. Returns: Union [ XYBox , XYZBox ] \u2013 Union[XYBox, XYZBox]: the combined box. Source code in sealhits/bbox.py 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 def bb_combine ( a : Union [ XYBox , XYZBox ], b : Union [ XYBox , XYZBox ] ) -> Union [ XYBox , XYZBox ]: \"\"\"Combine two bounding boxes into one. Either XYBox or XYZBox. Args: a (Union[XYBox, XYZBox]): the XYBox or XYZBox. b (Union[XYBox, XYZBox]): the XYBox or XYZBox. Returns: Union[XYBox, XYZBox]: the combined box. \"\"\" assert type ( a ) == type ( b ) x_min = min ( a . x_min , b . x_min ) y_min = min ( a . y_min , b . y_min ) x_max = max ( a . x_max , b . x_max ) y_max = max ( a . y_max , b . y_max ) if hasattr ( a , \"z_min\" ): z_min = min ( a . z_min , b . z_min ) z_max = max ( a . z_max , b . z_max ) return XYZBox ( x_min , y_min , z_min , x_max , y_max , z_max ) return XYBox ( x_min , y_min , x_max , y_max )","title":"bb_combine"},{"location":"reference/#sealhits.bbox.bb_expand","text":"Expand the bounding box. bb is either an XYBox or XYZ Box. img_size is a tuple of either (w,h) or (w,h,d). b_size is either a single int, or a tuple of different sizes. Parameters: bb ( Union [ XYBox , XYZBox ] ) \u2013 the XYBox or XYZBox to expand. img_size ( Union [ Tuple [ int , int ], Tuple [ int , int , int ]] ) \u2013 The size of the image to which this box belongs, in pixels. b_size ( Union [ int , Tuple ] ) \u2013 the amount to expand either uniformly (a single int), or expand by different amounts in each dimension. Returns: Union [ XYBox , XYZBox ] \u2013 Union[XYBox, XYZBox]: A new XYBox or XYZBox but within the raw image space (i.e original, not fan/polar transformed) Source code in sealhits/bbox.py 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 def bb_expand ( bb : Union [ XYBox , XYZBox ], img_size : Union [ Tuple [ int , int ], Tuple [ int , int , int ]], b_size : Union [ int , Tuple ], ) -> Union [ XYBox , XYZBox ]: \"\"\" Expand the bounding box. bb is either an XYBox or XYZ Box. img_size is a tuple of either (w,h) or (w,h,d). b_size is either a single int, or a tuple of different sizes. Args: bb (Union[XYBox, XYZBox]): the XYBox or XYZBox to expand. img_size (Union[Tuple[int, int], Tuple[int, int, int]]): The size of the image to which this box belongs, in pixels. b_size (Union[int, Tuple]): the amount to expand either uniformly (a single int), or expand by different amounts in each dimension. Returns: Union[XYBox, XYZBox]: A new XYBox or XYZBox but within the *raw* image space (i.e original, not fan/polar transformed) \"\"\" be = [] if hasattr ( b_size , \"__iter__\" ): if len ( b_size ) < 2 : # TODO - maybe raise an error/exception here? return bb be = b_size elif isinstance ( b_size , int ): if b_size <= 0 : return bb be = [ b_size , b_size , b_size ] x = bb . x_min y = bb . y_min a = bb . x_max b = bb . y_max c = 0 z = 0 w = a - x h = b - y d = 0 if hasattr ( bb , \"z_min\" ): z = bb . z_min c = bb . z_max d = c - z d = d + be [ 2 ] z = z - be [ 2 ] x = x - be [ 0 ] if x <= 0 : x = 0 if x >= img_size [ 0 ]: x = img_size [ 0 ] - 1 y = y - be [ 1 ] if y <= 0 : y = 0 if y >= img_size [ 1 ]: y = img_size [ 1 ] - 1 w += be [ 0 ] if w >= img_size [ 0 ]: w = img_size [ 0 ] - 1 h += be [ 1 ] if h >= img_size [ 1 ]: h = img_size [ 1 ] - 1 if hasattr ( bb , \"z_min\" ): if z <= 0 : z = 0 if z >= img_size [ 2 ]: z = img_size [ 2 ] - 1 if d <= 0 : d = 0 if d >= img_size [ 2 ]: d = img_size [ 2 ] - 1 return XYZBox ( x , y , z , x + w , y + h , z + d ) return XYBox ( x , y , x + w , y + h )","title":"bb_expand"},{"location":"reference/#sealhits.bbox.bb_inside","text":"Return true if a is completely enclosed by b. False otherwise. Parameters: a ( Union [ XYBox , XYZBox ] ) \u2013 the XYBox or XYZBox. b ( Union [ XYBox , XYZBox ] ) \u2013 the XYBox or XYZBox. Returns: bool ( bool ) \u2013 is a completely enclosed by b? Source code in sealhits/bbox.py 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 def bb_inside ( a : Union [ XYBox , XYZBox ], b : Union [ XYBox , XYZBox ]) -> bool : \"\"\"Return true if a is completely enclosed by b. False otherwise. Args: a (Union[XYBox, XYZBox]): the XYBox or XYZBox. b (Union[XYBox, XYZBox]): the XYBox or XYZBox. Returns: bool: is a *completely* enclosed by b? \"\"\" if type ( a ) != type ( b ): return False if a . x_min >= b . x_min and a . x_max <= b . x_max : if a . y_min >= b . y_min and a . y_max <= b . y_max : if hasattr ( a , \"z_min\" ) and hasattr ( b , \"z_min\" ): if a . z_min >= b . z_min and a . z_max <= b . z_max : return True else : return True return False","title":"bb_inside"},{"location":"reference/#sealhits.bbox.bb_overlap","text":"Return True if two boxes overlap, False if not. Takes either an XYBox or XYZ box. For now, both a and b must be the same type, else False is returned. Parameters: a ( Union [ XYBox , XYZBox ] ) \u2013 the XYBox or XYZBox. b ( Union [ XYBox , XYZBox ] ) \u2013 the XYBox or XYZBox. Returns: bool ( bool ) \u2013 do a and b overlap? Source code in sealhits/bbox.py 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 def bb_overlap ( a : Union [ XYBox , XYZBox ], b : Union [ XYBox , XYZBox ]) -> bool : \"\"\"Return True if two boxes overlap, False if not. Takes either an XYBox or XYZ box. For now, both a and b must be the same type, else False is returned. Args: a (Union[XYBox, XYZBox]): the XYBox or XYZBox. b (Union[XYBox, XYZBox]): the XYBox or XYZBox. Returns: bool: do a and b overlap? \"\"\" if type ( a ) != type ( b ): return False if a . x_min <= b . x_max and a . x_max >= b . x_min : if a . y_min <= b . y_max and a . y_max >= b . y_min : if hasattr ( a , \"z_min\" ) and hasattr ( b , \"z_min\" ): if a . z_min <= b . z_max and a . z_max >= b . z_min : return True else : return True return False","title":"bb_overlap"},{"location":"reference/#sealhits.bbox.bb_raw_to_fan","text":"Given an XY or XYZ bounding box in the raw image space, convert to fan space. Parameters: bbox ( Union [ XYBox , XYZBox ] ) \u2013 the XYBox or XYZBox to convert. raw_size ( Tuple [ int , int ] ) \u2013 the size of the raw rectangle. fan_size ( Tuple [ int , int ] ) \u2013 the size of the fan image. sonar_range ( float ) \u2013 the range of the sonar in this image Returns: Union [ XYBox , XYZBox ] \u2013 Union[XYBox, XYZBox]: the new XYBox or XYZBox. Source code in sealhits/bbox.py 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 def bb_raw_to_fan ( bbox : Union [ XYBox , XYZBox ], raw_size : Tuple ( int , int ), fan_size : Tuple ( int , int ), sonar_range : float , ) -> Union [ XYBox , XYZBox ]: \"\"\"Given an XY or XYZ bounding box in the raw image space, convert to fan space. Args: bbox (Union[XYBox, XYZBox]): the XYBox or XYZBox to convert. raw_size (Tuple[int, int]): the size of the raw rectangle. fan_size (Tuple[int, int]): the size of the fan image. sonar_range (float): the range of the sonar in this image Returns: Union[XYBox, XYZBox]: the new XYBox or XYZBox. \"\"\" from sealhits.btable import bearing_table x_min = bearing_table [ max ( bbox . x_min , 0 )] x_max = bearing_table [ min ( bbox . x_max , len ( bearing_table ) - 1 )] y_min = bbox . y_min / raw_size [ 1 ] * sonar_range y_max = bbox . y_max / raw_size [ 1 ] * sonar_range txy = [] txy . append ( dist_bearing_to_xy ( x_min , y_min , sonar_range , fan_size )) txy . append ( dist_bearing_to_xy ( x_max , y_max , sonar_range , fan_size )) txy . append ( dist_bearing_to_xy ( x_min , y_max , sonar_range , fan_size )) txy . append ( dist_bearing_to_xy ( x_max , y_min , sonar_range , fan_size )) min_x = txy [ 0 ][ 0 ] min_y = txy [ 0 ][ 1 ] max_x = txy [ 0 ][ 0 ] max_y = txy [ 0 ][ 1 ] for tt in txy [ 1 :]: if tt [ 0 ] < min_x : min_x = tt [ 0 ] if tt [ 0 ] > max_x : max_x = tt [ 0 ] if tt [ 1 ] < min_y : min_y = tt [ 1 ] if tt [ 1 ] > max_y : max_y = tt [ 1 ] return XYBox ( min_x , min_y , max_x , max_y )","title":"bb_raw_to_fan"},{"location":"reference/#sealhits.bbox.bb_to_fix","text":"Given an XYBox, a fix_size and image_size, return an xy crop size that has a fixed size. Image size is width/height. Parameters: bbox ( XYBox ) \u2013 the XYBox we are changing. fix_size ( Tuple [ int , int ] ) \u2013 the new fixed size. image_size ( Tuple [ int , int ] ) \u2013 the size of the image to which this bbox belongs. Returns: XYBox ( XYBox ) \u2013 the new XYBox with the new size. Source code in sealhits/bbox.py 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 def bb_to_fix ( bbox : XYBox , fix_size : Tuple [ int , int ], image_size : Tuple [ int , int ] ) -> XYBox : \"\"\" Given an XYBox, a fix_size and image_size, return an xy crop size that has a fixed size. Image size is width/height. Args: bbox (XYBox): the XYBox we are changing. fix_size (Tuple[int, int]): the new fixed size. image_size (Tuple[int, int]): the size of the image to which this bbox belongs. Returns: XYBox: the new XYBox with the new size. \"\"\" (( min_x , min_y ), ( max_x , max_y )) = bbox . pair () cx = int (( max_x + min_x ) / 2 ) cy = int (( max_y + min_y ) / 2 ) s_x = cx - int ( fix_size [ 0 ] / 2 ) s_y = cy - int ( fix_size [ 1 ] / 2 ) e_x = cx + int ( fix_size [ 0 ] / 2 ) e_y = cy + int ( fix_size [ 1 ] / 2 ) if s_x < 0 : dd = 0 - s_x s_x = 0 e_x += dd if e_x >= image_size [ 0 ]: dd = e_x - image_size [ 0 ] s_x -= dd e_x -= dd if s_y < 0 : dd = 0 - s_y s_y = 0 e_y += dd if e_y >= image_size [ 1 ]: dd = e_y - image_size [ 1 ] s_y -= dd e_y -= dd return XYBox ( int ( s_x ), int ( s_y ), int ( e_x ), int ( e_y ))","title":"bb_to_fix"},{"location":"reference/#sealhits.bbox.bb_to_min","text":"Given an XYBox, return an xy crop size that has a minimum size and is within the image size. Image size is height/width. Parameters: bbox ( XYBox ) \u2013 the XYBox to crop. min_size ( Tuple [ int , int ] ) \u2013 the new minimum size. image_size ( Tuple [ int , int ] ) \u2013 the size of the image to which this bbox belongs. Returns: XYBox ( XYBox ) \u2013 the new XYBox. Source code in sealhits/bbox.py 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 def bb_to_min ( bbox : XYBox , min_size : Tuple [ int , int ], image_size : Tuple [ int , int ] ) -> XYBox : \"\"\"Given an XYBox, return an xy crop size that has a minimum size and is within the image size. Image size is height/width. Args: bbox (XYBox): the XYBox to crop. min_size (Tuple[int, int]): the new minimum size. image_size (Tuple[int, int]): the size of the image to which this bbox belongs. Returns: XYBox: the new XYBox. \"\"\" (( min_x , min_y ), ( max_x , max_y )) = bbox . pair () x = min_x y = min_y w = max_x - x h = max_y - y if h < min_size [ 1 ]: d = ( min_size [ 1 ] - h ) / 2 max_y += d min_y -= d # Checks to see if we are exceeding the size of the image if min_y < 0 : dd = 0 - min_y min_y = 0 max_y += dd if max_y > image_size [ 1 ]: dd = max_y - image_size [ 1 ] min_y -= dd max_y -= dd # Now do the width if w < min_size [ 0 ]: d = ( min_size [ 0 ] - w ) / 2 max_x += d min_x -= d if min_x < 0 : dd = 0 - min_x min_x = 0 max_x += dd if max_x > image_size [ 0 ]: dd = max_x - image_size [ 0 ] min_x -= dd max_x -= dd return XYBox ( int ( min_x ), int ( min_y ), int ( max_x ), int ( max_y ))","title":"bb_to_min"},{"location":"reference/#sealhits.bbox.combine_boxes","text":"Given a list of 2D or 3D bbs, combine if they overlap. Overlapping groups are joined to create a bigger group, which makes further overlapping easier. 2D boxes are assumed to all be on the same Z step. This algorithm has bad complexity though and needs improvement. Parameters: bbs ( List [ Union [ XYBox , XYZBox ]] ) \u2013 a list of either XYBox or XYZBox. Returns: Tuple [ List [ Union [ XYBox , XYZBox ]], List [ Union [ XYBox , XYZBox ]]] \u2013 Tuple[List[Union[XYBox, XYZBox]], List[Union[XYBox, XYZBox]]]: Two lists - new boxes combined and a list of many lists of the grouped boxes. Source code in sealhits/bbox.py 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 def combine_boxes ( bbs : List [ Union [ XYBox , XYZBox ]]) -> Tuple [ List [ Union [ XYBox , XYZBox ]], List [ Union [ XYBox , XYZBox ]]]: \"\"\"Given a list of 2D or 3D bbs, combine if they overlap. Overlapping groups are joined to create a bigger group, which makes further overlapping easier. 2D boxes are assumed to all be on the same Z step. This algorithm has bad complexity though and needs improvement. Args: bbs (List[Union[XYBox, XYZBox]]): a list of either XYBox or XYZBox. Returns: Tuple[List[Union[XYBox, XYZBox]], List[Union[XYBox, XYZBox]]]: Two lists - new boxes combined and a list of many lists of the grouped boxes. \"\"\" merged = True new_bbs = bbs . copy () bbs_grouped = [[ a ] for a in new_bbs ] while merged : # Create a pairwise matrix of all combinations pairwise = [] # Divide list into pairs for i in range ( 0 , len ( new_bbs ) - 1 ): for j in range ( i + 1 , len ( new_bbs )): pairwise . append (( i , j )) merged = False # Now combine if we can for i , j in pairwise : a = new_bbs [ i ] b = new_bbs [ j ] if bb_overlap ( a , b ): new_bbs . pop ( j ) # Pop j first as it's always larger new_bbs . pop ( i ) c = bb_combine ( a , b ) d = bbs_grouped . pop ( j ) e = bbs_grouped . pop ( i ) f = d + e bbs_grouped . append ( f ) new_bbs . append ( c ) merged = True break del pairwise [:] return new_bbs , bbs_grouped","title":"combine_boxes"},{"location":"reference/#sealhits.bbox.points_to_bb","text":"Given a list of points, generate the bounding box for them. The list of points is from the db and is (time, sonarid, minbearing, maxbearing, minrange, maxrange, track, uid). All points should be from the same sonar. The max sonar range can change and therefore needs to be passed in. Parameters: points ( List [ Points ] ) \u2013 a list of Points. sonar_range ( float ) \u2013 the range of the sonar at this time. Returns: BearBox \u2013 BearBox Source code in sealhits/bbox.py 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 def points_to_bb ( points : List [ Points ], sonar_range : float ) -> BearBox : \"\"\"Given a list of points, generate the bounding box for them. The list of points is from the db and is (time, sonarid, minbearing, maxbearing, minrange, maxrange, track, uid). All points should be from the same sonar. The max sonar range can change and therefore needs to be passed in. Args: points (List[Points]): a list of Points. sonar_range (float): the range of the sonar at this time. Returns: BearBox \"\"\" # Assume the min/maxes based on the sonar bearing_limits = ( math . radians ( MIN_ANGLE ), math . radians ( MAX_ANGLE )) distance_limits = ( 0.0 , sonar_range ) min_b = bearing_limits [ 1 ] max_b = bearing_limits [ 0 ] min_d = distance_limits [ 1 ] max_d = distance_limits [ 0 ] # TODO - Minbearing and maxbearing have been swapped here! It works but clearly # there is an issue somehere in what min and max actually mean in the PAMGuard # database for point in points : if point . maxbearing < min_b and point . maxbearing >= bearing_limits [ 0 ]: min_b = point . maxbearing if point . minbearing > max_b and point . minbearing < bearing_limits [ 1 ]: max_b = point . minbearing if point . minrange < min_d and point . minrange >= distance_limits [ 0 ]: min_d = point . minrange if point . maxrange > max_d and point . maxrange < distance_limits [ 1 ]: max_d = point . maxrange bb = BearBox ( min_b , max_b , min_d , max_d , sonar_range ) return bb","title":"points_to_bb"},{"location":"reference/#compression","text":"compress.py - image compression and decompression. Functions related to compressing and decompressing images. We use LZ4 as the various python zip routines are quite slow.","title":"Compression"},{"location":"reference/#sealhits.compress.compress","text":"Given a FITS hdul, write this out to an lz4 compressed file. We unfortunately need to take the data out of the hdul and put it back in, so we just take the np.array and header as is. Parameters: data ( array ) \u2013 the image to save. header ( PrimaryHDU ) \u2013 the header to add to the FITS file. image_path ( str ) \u2013 the image to save. Source code in sealhits/compress.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 def compress ( data : np . array , header : fits . hdu . image . PrimaryHDU , image_path : str ): ''' Given a FITS hdul, write this out to an lz4 compressed file. We unfortunately need to take the data out of the hdul and put it back in, so we just take the np.array and header as is. Args: data (np.array): the image to save. header (PrimaryHDU): the header to add to the FITS file. image_path (str): the image to save. ''' assert ( os . path . splitext ( image_path )[ 1 ] == \".lz4\" ) with lz4 . frame . open ( image_path , mode = 'wb' ) as fp : fits . writeto ( fp , data , header = header )","title":"compress"},{"location":"reference/#sealhits.compress.decompress","text":"Decompress with LZ4. We have to return the data from within the lz4 context so we assume only one HDU which is true for our data at present. Parameters: image_path ( str ) \u2013 the path to the LZ4 image we want to decompress. Returns: Tuple [ array , PrimaryHDU ] \u2013 Tuple[np.array, fits.hdu.image.PrimaryHDU]: the image as a numpy array, and the FITS Primary HDU. Source code in sealhits/compress.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 def decompress ( image_path : str ) -> Tuple [ np . array , fits . hdu . image . PrimaryHDU ]: ''' Decompress with LZ4. We have to return the data from within the lz4 context so we assume only one HDU which is true for our data at present. Args: image_path (str): the path to the LZ4 image we want to decompress. Returns: Tuple[np.array, fits.hdu.image.PrimaryHDU]: the image as a numpy array, and the FITS Primary HDU. ''' assert ( os . path . exists ( image_path )) assert ( os . path . splitext ( image_path )[ 1 ] == \".lz4\" ) with lz4 . frame . open ( image_path , mode = 'rb' ) as fp : img = fits . open ( fp , memmap = False , lazy_load_hdus = False ) return ( img [ 0 ] . data , img [ 0 ] . header )","title":"decompress"},{"location":"reference/#database-object","text":"db.py - The Postgresql Database Object. This file holds the class that represents everything about our data. We use SQLAlchemy as the ORM to wrap the various postgresql commands and schemas.","title":"Database Object"},{"location":"reference/#sealhits.db.db.DB","text":"Our database class that holds the engine and ORM and what not. Source code in sealhits/db/db.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 class DB : \"\"\"Our database class that holds the engine and ORM and what not.\"\"\" # TODO - Import the many functions onto this class (perhaps there is a better way?) from sealhits.db.dbget import ( get_groups , get_num_groups , get_groups_filters , get_pgdf_filename , get_glf_filename , get_group_id , get_group_uid , get_group_huid , get_sqlites , get_points_group , get_pgdfs , get_pgdfs_distinct , get_group_gid_sqlite_id_split , get_group_gid_sqlite_id , get_group_track , get_images_groups , get_tracks_groups , get_tracks_groups_groups_binfile , get_image_points_by_filename , get_image_points_by_filename_group , get_image_groups_by_filename , get_tracks_group_uid , get_points_from_track , get_images_group , get_images_group_sonarid , get_image_uid , get_img_fname ) from sealhits.db.dbset import ( set_group_timestart , set_group_timeend , set_point_track , set_point_group , set_group_split , set_group_huid ) from sealhits.db.dbdel import ( del_by_sqlite ) def __init__ ( self , db_name , username , password , host = \"localhost\" , echo = False ): con_str = ( \"postgresql+psycopg2://\" + username + \":\" + password + \"@\" + host + \"/\" + db_name ) self . engine = create_engine ( con_str , echo = echo )","title":"DB"},{"location":"reference/#database-getters","text":"dbget.py - The Postgresql getter functions. These functions retrieve records from the db, performing certain queries to return the data required.","title":"Database Getters"},{"location":"reference/#sealhits.db.dbget.get_glf_filename","text":"Return a single GLF matching the filename. Parameters: fname ( str ) \u2013 the filename to match against. Returns: Union [ GLFS | None] \u2013 Union[GLFS | None]: either the matching GLFS or None. Source code in sealhits/db/dbget.py 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 def get_glf_filename ( self , fname : str ) -> Union [ GLFS | None ]: \"\"\"Return a single GLF matching the filename. Args: fname (str): the filename to match against. Returns: Union[GLFS | None]: either the matching GLFS or None. \"\"\" result = None with Session ( self . engine ) as session : stmt = select ( GLFS ) . where ( GLFS . filename == fname ) result = session . scalars ( stmt ) . one_or_none () return result","title":"get_glf_filename"},{"location":"reference/#sealhits.db.dbget.get_group_gid_sqlite_id","text":"Return a Groups object matching the gid, sqliteid and sqlite name. These three fields combine for a unique key - a bit too complicated sadly. We want all the splits. Parameters: gid ( int ) \u2013 the uid to match against. sqlite_name ( str ) \u2013 value matched against the \"sqlite\" field in the Groups Schema. sqliteid ( int ) \u2013 value matched against the \"sqliteid\" field in the Groups Schema. Returns: Union [ List [ Groups ] | None] \u2013 Union[List[Groups] | None] : Either a list of matching Groups or None Source code in sealhits/db/dbget.py 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 def get_group_gid_sqlite_id ( self , gid : int , sqlite_name : str , sqliteid : int ) -> Union [ List [ Groups ] | None ]: \"\"\"Return a Groups object matching the gid, sqliteid and sqlite name. These three fields combine for a unique key - a bit too complicated sadly. We want all the splits. Args: gid (int): the uid to match against. sqlite_name (str): value matched against the \"sqlite\" field in the Groups Schema. sqliteid (int): value matched against the \"sqliteid\" field in the Groups Schema. Returns: Union[List[Groups] | None] : Either a list of matching Groups or None \"\"\" results = None with Session ( self . engine ) as session : results = session . execute ( select ( Groups ) . where ( Groups . gid == gid ) . where ( Groups . sqlite == sqlite_name ) . where ( Groups . sqliteid == sqliteid ) ) . all () results = [ r [ 0 ] for r in results ] return results","title":"get_group_gid_sqlite_id"},{"location":"reference/#sealhits.db.dbget.get_group_gid_sqlite_id_split","text":"Return a Groups object matching the gid, sqliteid, split and sqlite name. These four fields combine for a unique key. Parameters: gid ( int ) \u2013 the uid to match against. sqlite_name ( str ) \u2013 value matched against the \"sqlite\" field in the Groups Schema. sqliteid ( int ) \u2013 value matched against the \"sqliteid\" field in the Groups Schema. split ( int ) \u2013 value matched against the \"split\" field in the Groups Schema. Returns: Union [ Groups | None] \u2013 Union[Groups | None]: either the matching 'Groups' or None. Source code in sealhits/db/dbget.py 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 def get_group_gid_sqlite_id_split ( self , gid : int , sqlite_name : str , sqliteid : int , split : int ) -> Union [ Groups | None ]: \"\"\"Return a Groups object matching the gid, sqliteid, split and sqlite name. These four fields combine for a unique key. Args: gid (int): the uid to match against. sqlite_name (str): value matched against the \"sqlite\" field in the Groups Schema. sqliteid (int): value matched against the \"sqliteid\" field in the Groups Schema. split (int): value matched against the \"split\" field in the Groups Schema. Returns: Union[Groups | None]: either the matching 'Groups' or None. \"\"\" result = None with Session ( self . engine ) as session : result = session . execute ( select ( Groups ) . where ( Groups . gid == gid ) . where ( Groups . sqlite == sqlite_name ) . where ( Groups . sqliteid == sqliteid ) . where ( Groups . split == split ) ) . scalar_one_or_none () return result","title":"get_group_gid_sqlite_id_split"},{"location":"reference/#sealhits.db.dbget.get_group_huid","text":"Return a Groups object matching the human uid. Should be only one but we can't be sure. Parameters: huid ( str ) \u2013 the huid to match against. Returns: Union [ Groups | None] \u2013 Union[Groups | None]: either the matching 'Groups' or None. Source code in sealhits/db/dbget.py 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def get_group_huid ( self , huid : str ) -> Union [ Groups | None ]: \"\"\"Return a Groups object matching the human uid. Should be only one but we can't be sure. Args: huid (str): the huid to match against. Returns: Union[Groups | None]: either the matching 'Groups' or None. \"\"\" result = None with Session ( self . engine ) as session : result = session . execute ( select ( Groups ) . where ( Groups . huid == huid ) ) . scalar_one_or_none () return result","title":"get_group_huid"},{"location":"reference/#sealhits.db.dbget.get_group_id","text":"Get a group with either a string UID, or a HUID. Parameters: id ( str ) \u2013 the id either as a UUID or a HUID. Returns: Union [ Groups | None] \u2013 Union[Groups | None]: either the matching 'Groups' or None. Source code in sealhits/db/dbget.py 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 def get_group_id ( self , id : str ) -> Union [ Groups | None ]: \"\"\"Get a group with either a string UID, or a HUID. Args: id (str): the id either as a UUID or a HUID. Returns: Union[Groups | None]: either the matching 'Groups' or None. \"\"\" try : uid = uuid . UUID ( id ) return self . get_group_uid ( uid ) except ValueError : return self . get_group_huid ( id )","title":"get_group_id"},{"location":"reference/#sealhits.db.dbget.get_group_track","text":"Return a group, given a track uid. Parameters: track_id ( uuid4 ) \u2013 the uid of the track to match against. Returns: Union [ Groups | None] \u2013 Union[Groups | None]: either the matching 'Groups' or None. Source code in sealhits/db/dbget.py 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 def get_group_track ( self , track_id : uuid . uuid4 ) -> Union [ Groups | None ]: \"\"\"Return a group, given a track uid. Args: track_id (uuid.uuid4): the uid of the track to match against. Returns: Union[Groups | None]: either the matching 'Groups' or None. \"\"\" result = None with Session ( self . engine ) as session : result = session . execute ( select ( Groups ) . join ( TrackGroup ) . where ( Groups . uid == TrackGroup . group_id ) . filter ( TrackGroup . track_id == track_id ) ) . one_or_none () if result is not None : return result [ 0 ] return result","title":"get_group_track"},{"location":"reference/#sealhits.db.dbget.get_group_uid","text":"Return a Groups object matching the uid. Parameters: uid ( uuid4 ) \u2013 the uid to match against. Returns: Union [ Groups | None] \u2013 Union[Groups | None]: either the matching 'Groups' or None. Source code in sealhits/db/dbget.py 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 def get_group_uid ( self , uid : uuid . uuid4 ) -> Union [ Groups | None ]: \"\"\"Return a Groups object matching the uid. Args: uid (uuid.uuid4): the uid to match against. Returns: Union[Groups | None]: either the matching 'Groups' or None. \"\"\" result = None with Session ( self . engine ) as session : result = session . execute ( select ( Groups ) . where ( Groups . uid == uid ) ) . scalar_one_or_none () return result","title":"get_group_uid"},{"location":"reference/#sealhits.db.dbget.get_groups","text":"return all the groups, ordered by the pamguard sqlite uid ascending. Optionally a search can be made using a code. Parameters: code ( str , default: None ) \u2013 a string to match against the 'code' field on groups. Returns: List [ Groups ] \u2013 List[Groups]: the resulting list of 'Groups' Source code in sealhits/db/dbget.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 def get_groups ( self , code = None ) -> List [ Groups ]: \"\"\"return all the groups, ordered by the pamguard sqlite uid ascending. Optionally a search can be made using a code. Args: code (str): a string to match against the 'code' field on groups. Returns: List[Groups]: the resulting list of 'Groups' \"\"\" results = [] with Session ( self . engine ) as session : if code is not None : results = ( session . query ( Groups ) . where ( Groups . code == code ) . order_by ( Groups . gid . asc ()) . all () ) else : results = session . query ( Groups ) . order_by ( Groups . gid . asc ()) . all () return results","title":"get_groups"},{"location":"reference/#sealhits.db.dbget.get_groups_filters","text":"Return groups using a number of filters that the user can pass in. Filters is a list or tuple containing an appropriate filters for groups such as Groups.mammal > 1. Filters must be a list or tuple, not a single filter on it's own (though a single filter can be used so long as it's inside a list.) Parameters: filters ( List[] ) \u2013 as list of 'filters' such as 'Groups.mammal > 1' Returns: List [ Groups ] \u2013 List[Groups]: the resulting list of 'Groups' Source code in sealhits/db/dbget.py 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 def get_groups_filters ( self , filters ) -> List [ Groups ]: \"\"\"Return groups using a number of filters that the user can pass in. Filters is a list or tuple containing an appropriate filters for groups such as Groups.mammal > 1. Filters must be a list or tuple, not a single filter on it's own (though a single filter can be used so long as it's inside a list.) Args: filters (List[]): as list of 'filters' such as 'Groups.mammal > 1' Returns: List[Groups]: the resulting list of 'Groups' \"\"\" results = [] with Session ( self . engine ) as session : q = session . query ( Groups ) . filter ( * filters ) . all () results = q return results","title":"get_groups_filters"},{"location":"reference/#sealhits.db.dbget.get_image_groups_by_filename","text":"Return a list of all the groups in the image with this filename. Parameters: filename ( str ) \u2013 the image filename to check against. Returns: List [ Groups ] \u2013 List[Groups]: List of matching Groups. Source code in sealhits/db/dbget.py 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 def get_image_groups_by_filename ( self , filename : str ) -> List [ Groups ]: \"\"\"Return a list of all the groups in the image with this filename. Args: filename (str): the image filename to check against. Returns: List[Groups]: List of matching Groups. \"\"\" results = [] with Session ( self . engine ) as session : try : results = ( session . execute ( select ( Groups ) . join_from ( Groups , groups_images ) . join_from ( groups_images , Images ) . filter ( Images . filename == filename ) ) . scalars () . all () ) except IntegrityError as e : print ( \"get_images_groups failed\" , e ) return results","title":"get_image_groups_by_filename"},{"location":"reference/#sealhits.db.dbget.get_image_points_by_filename","text":"Get all the points for a particular image regardless of group. Parameters: imagefile ( str ) \u2013 the name of the imagefile to match against. Returns: List [ Points ] \u2013 List[Points]: List of matching Points. Source code in sealhits/db/dbget.py 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 def get_image_points_by_filename ( self , imagefile : str ) -> List [ Points ]: \"\"\"Get all the points for a particular image regardless of group. Args: imagefile (str): the name of the imagefile to match against. Returns: List[Points]: List of matching Points. \"\"\" results = [] with Session ( self . engine ) as session : results = ( session . execute ( select ( Points ) . join_from ( TrackGroup , Points ) . join_from ( TrackGroup , Groups ) . join_from ( Groups , groups_images ) . join_from ( groups_images , Images ) . filter ( Images . hastrack is True ) . filter ( Points . sonarid == Images . sonarid ) . filter ( Images . filename == imagefile ) ) . scalars () . all () ) return results","title":"get_image_points_by_filename"},{"location":"reference/#sealhits.db.dbget.get_image_points_by_filename_group","text":"Get all the tracks in an annotated group for this single image. We use the groups_key on points to do the matching. Parameters: imagefile ( str ) \u2013 the image filename to check against. group_id ( Union [ UUID , str ] ) \u2013 group to match against, either with a uuid or huid string. Returns: List [ Points ] \u2013 List[Points]: List of matching Points. Source code in sealhits/db/dbget.py 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 def get_image_points_by_filename_group ( self , imagefile : str , group_id : Union [ uuid . UUID , str ] ) -> List [ Points ]: \"\"\"Get all the tracks in an annotated group for this single image. We use the groups_key on points to do the matching. Args: imagefile (str): the image filename to check against. group_id (Union[uuid.UUID, str]): group to match against, either with a uuid or huid string. Returns: List[Points]: List of matching Points. \"\"\" results = [] filters = [] if type ( group_id ) is uuid . UUID : filters . append ( Groups . uid == group_id ) else : filters . append ( Groups . huid == group_id ) with Session ( self . engine ) as session : results = ( session . execute ( select ( Points ) . join_from ( Groups , Points ) . join_from ( Groups , groups_images ) . join_from ( groups_images , Images ) . filter ( * filters ) . filter ( Points . group_id == Groups . uid ) . filter ( Images . filename == imagefile ) . filter ( Points . time == Images . time ) . filter ( Points . sonarid == Images . sonarid ) ) . scalars () . all () ) return results","title":"get_image_points_by_filename_group"},{"location":"reference/#sealhits.db.dbget.get_image_uid","text":"Does this image exist already? Return it if it does or None if not. Parameters: image_uid ( uuid4 ) \u2013 the uid of the image Returns: Union [ Images | None] \u2013 Union[Images | None]: List of matching Images or None. Source code in sealhits/db/dbget.py 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 def get_image_uid ( self , image_uid : uuid . uuid4 ) -> Union [ Images | None ]: \"\"\"Does this image exist already? Return it if it does or None if not. Args: image_uid (uuid.uuid4): the uid of the image Returns: Union[Images | None]: List of matching Images or None. \"\"\" result = None try : with Session ( self . engine ) as session : result = session . query ( Images ) . where ( Images . uid == image_uid ) . first () except IntegrityError as e : print ( \"get_image_uid failed\" , e ) return result","title":"get_image_uid"},{"location":"reference/#sealhits.db.dbget.get_images_group","text":"Given a group uid return all the images for this group in order. Group_uid can be either uuid.UUID4 or a string representing the huid. Parameters: group_id ( Union [ str , UUID ] ) \u2013 either the huid (str) or uid (uuid) of the group. Returns: Union [ List [ Images ], None] \u2013 Union[List[Images], None]: List of matching Images or None. Source code in sealhits/db/dbget.py 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 def get_images_group ( self , group_id : Union [ str , uuid . UUID ]) -> Union [ List [ Images ], None ]: \"\"\"Given a group uid return all the images for this group in order. Group_uid can be either uuid.UUID4 or a string representing the huid. Args: group_id (Union[str, uuid.UUID]): either the huid (str) or uid (uuid) of the group. Returns: Union[List[Images], None]: List of matching Images or None. \"\"\" results = None filters = [] if type ( group_id ) is uuid . UUID : filters . append ( Groups . uid == group_id ) else : filters . append ( Groups . huid == group_id ) with Session ( self . engine ) as session : try : results = ( session . execute ( select ( Images ) . join_from ( Groups , groups_images ) . join_from ( groups_images , Images ) . filter ( * filters ) . order_by ( Images . time . asc ()) ) . scalars () . all () ) except IntegrityError as e : print ( \"get_images_group failed\" , e ) return results","title":"get_images_group"},{"location":"reference/#sealhits.db.dbget.get_images_group_sonarid","text":"Given a group uid and a sonarid, return all the images for this group in order. Group_id is either a uuid.uuid4 or a huid string. Parameters: group_id ( Union [ str , UUID ] ) \u2013 either the huid (str) or uid (uuid) of the group. sonar_id ( int ) \u2013 the sonar id to check against. Returns: Union [ List [ Images ], None] \u2013 Union[List[Images], None]: List of matching Images or None. Source code in sealhits/db/dbget.py 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 def get_images_group_sonarid ( self , group_id , sonar_id : int ) -> Union [ List [ Images ], None ]: \"\"\"Given a group uid and a sonarid, return all the images for this group in order. Group_id is either a uuid.uuid4 or a huid string. Args: group_id (Union[str, uuid.UUID]): either the huid (str) or uid (uuid) of the group. sonar_id (int): the sonar id to check against. Returns: Union[List[Images], None]: List of matching Images or None. \"\"\" results = None filters = [] if type ( group_id ) is uuid . UUID : filters . append ( Groups . uid == group_id ) else : filters . append ( Groups . huid == group_id ) filters . append ( Images . sonarid == sonar_id ) with Session ( self . engine ) as session : try : results = ( session . execute ( select ( Images ) . join_from ( Groups , groups_images ) . join_from ( groups_images , Images ) . filter ( * filters ) . order_by ( Images . time . asc ()) ) . scalars () . all () ) except IntegrityError as e : print ( \"get_images_group failed\" , e ) return results","title":"get_images_group_sonarid"},{"location":"reference/#sealhits.db.dbget.get_images_groups","text":"Return a list of all the images that have a group within them that is annotated for a particular sonar. It need not have an actual track within the image, therefore buffer images and images appearing between tracks appear as well. Parameters: sonar_id ( int ) \u2013 the sonar id to match against. Returns: List [ Groups ] \u2013 List[Groups]: List of matching Groups Source code in sealhits/db/dbget.py 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 def get_images_groups ( self , sonar_id : int ) -> List [ Groups ]: \"\"\"Return a list of all the images that have a group within them that is annotated for a particular sonar. It need not have an actual track within the image, therefore buffer images and images appearing between tracks appear as well. Args: sonar_id (int): the sonar id to match against. Returns: List[Groups]: List of matching Groups \"\"\" results = [] with Session ( self . engine ) as session : results = session . execute ( select ( Images , Groups . uid ) . join ( Images . groups ) . where ( Images . sonarid == sonar_id ) ) . all () return results","title":"get_images_groups"},{"location":"reference/#sealhits.db.dbget.get_img_fname","text":"Does this image exist already? Return it if it does or None if not. Parameters: image_fname ( str ) \u2013 the filename of the image (minus the .lz4) Returns: Union [ Images , None] \u2013 Union[Images | None]: List of matching Images or None. Source code in sealhits/db/dbget.py 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 def get_img_fname ( self , image_fname : str ) -> Union [ Images , None ]: \"\"\"Does this image exist already? Return it if it does or None if not. Args: image_fname (str): the filename of the image (minus the .lz4) Returns: Union[Images | None]: List of matching Images or None. \"\"\" result = None try : with Session ( self . engine ) as session : result = session . query ( Images ) . where ( Images . filename == image_fname ) . first () except MultipleResultsFound as e : print ( \"get_img_fname failed - result is not unique\" , e ) except NoResultFound : pass # Okay to pass here as this result is valid except IntegrityError as e : print ( \"get_img_fname failed\" , e ) return result","title":"get_img_fname"},{"location":"reference/#sealhits.db.dbget.get_num_groups","text":"Return the count of the groups. Optionally a search can be made using a code. Optionally, Originals means these with a split number of 0, i.e the group from PAMGuard before it was split by ingest.py. Parameters: code ( str , default: None ) \u2013 a string that matches against the 'code' field on Groups. originals ( bool , default: False ) \u2013 return only these groups that were not generated with a split. Returns: int ( int ) \u2013 the number of groups Source code in sealhits/db/dbget.py 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 def get_num_groups ( self , code = None , originals = False ) -> int : \"\"\"Return the count of the groups. Optionally a search can be made using a code. Optionally, Originals means these with a split number of 0, i.e the group from PAMGuard before it was split by ingest.py. Args: code (str): a string that matches against the 'code' field on Groups. originals (bool): return only these groups that were not generated with a split. Returns: int: the number of groups \"\"\" res = 0 # TODO - this is a bit verbose. Maybe compose where clauses? with Session ( self . engine ) as session : if code is not None : if originals : res = int ( session . query ( Groups ) . where ( Groups . code == code ) . where ( Groups . split == 0 ) . count () ) else : res = int ( session . query ( Groups ) . where ( Groups . code == code ) . count ()) else : if originals : res = int ( session . query ( Groups ) . where ( Groups . split == 0 ) . count ()) else : res = int ( session . query ( Groups ) . count ()) return res","title":"get_num_groups"},{"location":"reference/#sealhits.db.dbget.get_pgdf_filename","text":"Return a single PGDF matching the filename. Parameters: fname ( str ) \u2013 the filename to match against. Returns: Union [ PGDFS | None] \u2013 Union[PGDFS | None]: either the matching PGDFS or None. Source code in sealhits/db/dbget.py 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 def get_pgdf_filename ( self , fname : str ) -> Union [ PGDFS | None ]: \"\"\"Return a single PGDF matching the filename. Args: fname (str): the filename to match against. Returns: Union[PGDFS | None]: either the matching PGDFS or None. \"\"\" result = None with Session ( self . engine ) as session : stmt = select ( PGDFS ) . where ( PGDFS . filename == fname ) result = session . scalars ( stmt ) . one_or_none () return result","title":"get_pgdf_filename"},{"location":"reference/#sealhits.db.dbget.get_pgdfs","text":"Return all the PGDFs from the DB, with optional limit. Parameters: limit ( int , default: -1 ) \u2013 an optional limit on how many to return. -1 means no limit. Returns: List [ PGDFS ] \u2013 List[PGDFS]: the resulting list of 'PGDFS' Source code in sealhits/db/dbget.py 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 def get_pgdfs ( self , limit =- 1 ) -> List [ PGDFS ]: \"\"\"Return all the PGDFs from the DB, with optional limit. Args: limit (int): an optional limit on how many to return. -1 means no limit. Returns: List[PGDFS]: the resulting list of 'PGDFS' \"\"\" results = [] with Session ( self . engine ) as session : if limit > 0 : results = ( session . query ( PGDFS ) . order_by ( PGDFS . startdate . asc ()) . limit ( limit ) . all () ) else : results = session . query ( PGDFS ) . order_by ( PGDFS . startdate . asc ()) . all () return results","title":"get_pgdfs"},{"location":"reference/#sealhits.db.dbget.get_pgdfs_distinct","text":"Return all the PGDFs with no duplicates. Why there should be duplicates I am not sure. Returns: List [ PGDFS ] \u2013 List[PGDFS]: the resulting list of 'PGDFS' Source code in sealhits/db/dbget.py 163 164 165 166 167 168 169 170 171 172 173 174 175 def get_pgdfs_distinct ( self ) -> List [ PGDFS ]: \"\"\"Return all the PGDFs with no duplicates. Why there should be duplicates I am not sure. Returns: List[PGDFS]: the resulting list of 'PGDFS' \"\"\" results = [] with Session ( self . engine ) as session : results = session . query ( PGDFS ) . order_by ( PGDFS . startdate . asc ()) . distinct () . all () return results","title":"get_pgdfs_distinct"},{"location":"reference/#sealhits.db.dbget.get_points_from_track","text":"Get all the points for a particular track, given the track_uid. Returns a list of tuple - (time, sonarid, minbearing, maxbearing, minrange, maxrange, track, uid). Parameters: track_uid ( UUID ) \u2013 the uid of the track Returns: List [ Points ] \u2013 List[Points]: List of matching Points. Source code in sealhits/db/dbget.py 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 def get_points_from_track ( self , track_uid : uuid . UUID ) -> List [ Points ]: \"\"\" Get all the points for a particular track, given the track_uid. Returns a list of tuple - (time, sonarid, minbearing, maxbearing, minrange, maxrange, track, uid). Args: track_uid (uuid.UUID): the uid of the track Returns: List[Points]: List of matching Points. \"\"\" results = [] with Session ( self . engine ) as session : try : q = session . query ( Points ) . filter ( Points . track_id == track_uid ) . all () results = q except IntegrityError as e : print ( \"get_group_tracks_uid failed\" , e ) return results","title":"get_points_from_track"},{"location":"reference/#sealhits.db.dbget.get_points_group","text":"Get all the points for this group. group_id can either be a uuid.uuid4 or it can be huid string. Parameters: group_id ( Union [ UUID , str ] ) \u2013 Either a uuid for uid, or a string for huid matching. Returns: List [ Points ] \u2013 List[Points]: List of matching Points. Source code in sealhits/db/dbget.py 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 def get_points_group ( self , group_id : Union [ uuid . UUID , str ]) -> List [ Points ]: \"\"\"Get all the points for this group. group_id can either be a uuid.uuid4 or it can be huid string. Args: group_id (Union[uuid.UUID, str]): Either a uuid for uid, or a string for huid matching. Returns: List[Points]: List of matching Points. \"\"\" results = [] group_uid = None if type ( group_id ) is uuid . UUID : group_uid = group_id else : g = self . get_group_huid ( group_id ) group_uid = g . uid with Session ( self . engine ) as session : q = session . query ( Points ) . filter ( Points . group_id == group_uid ) . all () results = q return results","title":"get_points_group"},{"location":"reference/#sealhits.db.dbget.get_sqlites","text":"Return the SQLites we have considered. Returns: List [ str ] \u2013 List[str]: List of sqlites in the database as strings. Source code in sealhits/db/dbget.py 456 457 458 459 460 461 462 463 464 465 466 def get_sqlites ( self ) -> List [ str ]: \"\"\"Return the SQLites we have considered. Returns: List[str]: List of sqlites in the database as strings. \"\"\" results = [] with Session ( self . engine ) as session : results = session . execute ( select ( Groups . sqlite ) . distinct ()) . scalars () . all () return results","title":"get_sqlites"},{"location":"reference/#sealhits.db.dbget.get_tracks_group_uid","text":"Get all the tracks in a particular group. Parameters: group_id ( Union [ UUID , str ] ) \u2013 group to match against, either with a uuid or huid string. Returns: List [ TrackGroup ] \u2013 List[TrackGroup]: List of matching TrackGroup. Source code in sealhits/db/dbget.py 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 def get_tracks_group_uid ( self , group_id : Union [ uuid . UUID , str ]) -> List [ TrackGroup ]: \"\"\"Get all the tracks in a particular group. Args: group_id (Union[uuid.UUID, str]): group to match against, either with a uuid or huid string. Returns: List[TrackGroup]: List of matching TrackGroup. \"\"\" results = [] group_uid = None if type ( group_id ) is uuid . UUID : group_uid = group_id else : g = self . get_group_huid ( group_id ) group_uid = g . uid with Session ( self . engine ) as session : try : q = session . query ( TrackGroup ) . filter ( TrackGroup . group_id == group_uid ) . all () results = q except IntegrityError as e : print ( \"get_group_tracks_uid failed\" , e ) return results","title":"get_tracks_group_uid"},{"location":"reference/#sealhits.db.dbget.get_tracks_groups","text":"Return all the track groups table. Args: None Returns: List [ TrackGroup ] \u2013 List[TrackGroup]: List of matching TrackGroup. Source code in sealhits/db/dbget.py 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 def get_tracks_groups ( self ) -> List [ TrackGroup ]: \"\"\"Return all the track groups table. Args: None Returns: List[TrackGroup]: List of matching TrackGroup. \"\"\" results = [] with Session ( self . engine ) as session : stmt = select ( TrackGroup ) results = session . scalars ( stmt ) . all () return results","title":"get_tracks_groups"},{"location":"reference/#sealhits.db.dbget.get_tracks_groups_groups_binfile","text":"Return all the track groups table that match the group uids and pgdf binary file. Parameters: group_uids ( List [ UUID ] ) \u2013 a list of uids to match against. binfile ( str ) \u2013 a string that represents the binary file. Matched against binfile. Returns: List [ TrackGroup ] \u2013 List[TrackGroup]: List of matching TrackGroup. Source code in sealhits/db/dbget.py 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 def get_tracks_groups_groups_binfile ( self , group_uids : List [ uuid . UUID ], binfile : str ) -> List [ TrackGroup ]: \"\"\"Return all the track groups table that match the group uids and pgdf binary file. Args: group_uids (List[uuid.UUID]): a list of uids to match against. binfile (str): a string that represents the binary file. Matched against binfile. Returns: List[TrackGroup]: List of matching TrackGroup. \"\"\" results = [] with Session ( self . engine ) as session : q = ( session . query ( TrackGroup ) . filter ( TrackGroup . group_id . in_ ( group_uids )) . filter ( TrackGroup . binfile == binfile ) . all () ) results = q return results","title":"get_tracks_groups_groups_binfile"},{"location":"reference/#database-model","text":"dbmodel.py - The Postgresql database model. The model of the database, with all the objects loaded into a session for comparison. DBModel holds a representation of the database in order to compare against the real database. This comparison is used when updating the real database with new information.","title":"Database model"},{"location":"reference/#sealhits.db.dbmodel.DBModel","text":"Simmple struct like class to hold the model. Source code in sealhits/db/dbmodel.py 21 22 23 24 25 26 27 28 29 class DBModel : \"\"\" Simmple struct like class to hold the model.\"\"\" def __init__ ( self ): self . groups = [] self . points = [] self . images = [] self . pgdfs = [] self . glfs = [] self . track_groups = []","title":"DBModel"},{"location":"reference/#sealhits.db.dbmodel.diff_models","text":"Compare two models - find the objects that do not exist in model_b but do exist in model_a and therefore should be removed from model_a. Parameters: session ( Session ) \u2013 the current sqlalchemy session. model_a ( DBModel ) \u2013 the first model of the database. model_b ( DBModel ) \u2013 the second model to compare against the first. Returns: DBModel ( DBModel ) \u2013 a model of the differences. Source code in sealhits/db/dbmodel.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 def diff_models ( session : Session , model_a : DBModel , model_b : DBModel ) -> DBModel : \"\"\" Compare two models - find the objects that do not exist in model_b but do exist in model_a and therefore should be removed from model_a. Args: session (Session): the current sqlalchemy session. model_a (DBModel): the first model of the database. model_b (DBModel): the second model to compare against the first. Returns: DBModel: a model of the differences. \"\"\" model_diff = DBModel () # Start with groups groups_a = { a . uid for a in model_a . groups } groups_b = { b . uid for b in model_b . groups } groups_diff = groups_a - groups_b for uid in groups_diff : tt = ( session . query ( Groups ) . filter ( Groups . uid == uid ) . one () ) model_diff . groups . append ( tt ) # Now Points points_a = { a . uid for a in model_a . points } points_b = { b . uid for b in model_b . points } points_diff = points_a - points_b for uid in points_diff : tt = ( session . query ( Points ) . filter ( Points . uid == uid ) . one () ) model_diff . points . append ( tt ) # Images images_a = { a . uid for a in model_a . images } images_b = { b . uid for b in model_b . images } images_diff = images_a - images_b for uid in images_diff : tt = ( session . query ( Images ) . filter ( Images . uid == uid ) . one () ) model_diff . images . append ( tt ) # PGDFS pgdfs_a = { a . uid for a in model_a . pgdfs } pgdfs_b = { b . uid for b in model_b . pgdfs } pgdfs_diff = pgdfs_a - pgdfs_b for uid in pgdfs_diff : tt = ( session . query ( PGDFS ) . filter ( PGDFS . uid == uid ) . one () ) model_diff . pgdfs . append ( tt ) # GLFS glfs_a = { a . uid for a in model_a . glfs } glfs_b = { b . uid for b in model_b . glfs } glfs_diff = glfs_a - glfs_b for uid in glfs_diff : tt = ( session . query ( GLFS ) . filter ( GLFS . uid == uid ) . one () ) model_diff . glfs . append ( tt ) # TrackGroups tg_a = { a . track_id for a in model_a . track_groups } tg_b = { b . track_id for b in model_b . track_groups } tg_diff = tg_a - tg_b for uid in tg_diff : tt = ( session . query ( TrackGroup ) . filter ( TrackGroup . track_id == uid ) . one () ) model_diff . track_groups . append ( tt ) return model_diff","title":"diff_models"},{"location":"reference/#sealhits.db.dbmodel.gen_model","text":"Return the objects in our ORM model for a particular sqlname, within a session. Mostly this is used to compare against a recent ingest to see what needs to be deleted. Parameters: session ( Session ) \u2013 the current sqlalchemy session. sqlname ( str ) \u2013 The name of the sqlite file of the recent ingest. Returns: DBModel ( DBModel ) \u2013 the generated model reflecting just the single sqlite ingest. Source code in sealhits/db/dbmodel.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 def gen_model ( session : Session , sqlname : str ) -> DBModel : \"\"\" Return the objects in our ORM model for a particular sqlname, within a session. Mostly this is used to compare against a recent ingest to see what needs to be deleted. Args: session (Session): the current sqlalchemy session. sqlname (str): The name of the sqlite file of the recent ingest. Returns: DBModel: the generated model reflecting just the single sqlite ingest. \"\"\" model = DBModel () with session . no_autoflush : model . groups = session . query ( Groups ) . filter ( Groups . sqlite == sqlname ) . all () for group in model . groups : model . images += group . images model . pgdfs += group . pgdfs model . glfs += group . glfs tg = ( session . query ( TrackGroup ) . filter ( TrackGroup . group_id == group . uid ) . all () ) model . track_groups += tg tp = ( session . query ( Points ) . filter ( Points . group_id == group . uid ) . all () ) model . points += tp return model","title":"gen_model"},{"location":"reference/#database-schema","text":"dbschema.py - The Postgresql schema. author: Benjamin Blundell (bjb8@st-andrews.ac.uk) The Postgresql Database Schema. We use SQLAlchemy as the ORM to wrap the various postgresql.","title":"Database schema"},{"location":"reference/#sealhits.db.dbschema.TrackGroup","text":"Bases: Base Represents the tracks that are part of this group. We use our own UUID but the constraint is that track_id and binfile together must be unique. Source code in sealhits/db/dbschema.py 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 class TrackGroup ( Base ): \"\"\"Represents the tracks that are part of this group. We use our own UUID but the constraint is that track_id and binfile together must be unique.\"\"\" __tablename__ = \"tracks_groups\" track_pam_id : Mapped [ int ] track_id = Column ( UUID ( as_uuid = True ), primary_key = True ) group_id = Column ( UUID ( as_uuid = True ), ForeignKey ( Groups . uid ) ) # TODO - foreign relation here? binfile : Mapped [ str ] def __repr__ ( self ) -> str : return f \"TrackGroup(track_id= { self . track_id !r} , group_id= { self . group_id !r} )\"","title":"TrackGroup"},{"location":"reference/#database-searches","text":"dbsearch.py - The Postgresql search functions. Various functions for finding and generating data from our images and database.","title":"Database searches"},{"location":"reference/#sealhits.db.dbsearch.gid_to_fans_imgs","text":"Given a GID, DB, path to fits files and sonarid, return the fan images for a particular group along with the filename path. The fan image sizes are created using the heighest resolution. Parameters: uid ( str ) \u2013 uid or huid of the group as a string. db ( DB ) \u2013 The active database object. fits_path ( str ) \u2013 The path the fits files. sonar_id ( int ) \u2013 The sonar_id. scale_factor ( float , default: 1.0 ) \u2013 optional scaling factor for the fan images. limit ( int , default: -1 ) \u2013 optional number of results to process (-1 is no limit) Returns: List [ Tuple [ array , Images ]] \u2013 List[Tuple[np.array, Images]]: a list of both the fans and their associate Images records. Source code in sealhits/db/dbsearch.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def gid_to_fans_imgs ( uid : str , db : DB , fits_path : str , sonar_id : int , scale_factor = 1.0 , limit =- 1 ) -> List [ Tuple [ np . array , Images ]]: \"\"\" Given a GID, DB, path to fits files and sonarid, return the fan images for a particular group along with the filename path. The fan image sizes are created using the heighest resolution. Args: uid (str): uid or huid of the group as a string. db (DB): The active database object. fits_path (str): The path the fits files. sonar_id (int): The sonar_id. scale_factor (float): optional scaling factor for the fan images. limit (int): optional number of results to process (-1 is no limit) Returns: List[Tuple[np.array, Images]]: a list of both the fans and their associate Images records. \"\"\" current_frames = [] results = db . get_images_group ( uid , sonar_id ) if results is not None : if limit > 0 : results = results [: limit ] # Do the resize only once - All images in the group # should be the same. It'd be bad otherwise fname = results [ 0 ] . filename fresult = utils . fast_find ( fname , fits_path ) hdul = fits . open ( fresult ) fits_height = int ( hdul [ 0 ] . data . shape [ 0 ]) if scale_factor != 1.0 : fits_height = int ( math . floor ( fits_height * scale_factor )) for idx , img in enumerate ( tqdm ( results )): fname = img . filename fresult = utils . fast_find ( fname , fits_path ) if fresult is not None : # Start with the sonar image # We need flip up down and left right! hdul = fits . open ( fresult ) fan_image = image . fan_distort ( hdul [ 0 ] . data , fits_height , btable . bearing_table ) current_frames . append ( np . fliplr ( np . flipud ( fan_image ))) return zip ( current_frames , results )","title":"gid_to_fans_imgs"},{"location":"reference/#database-setters","text":"dbset.py - The Postgresql set functions. The Postgresql Database SET functions. These functions set records in the db.","title":"Database setters"},{"location":"reference/#sealhits.db.dbset.set_group_huid","text":"Set a human readable uid. Args: group_uid (uuid.uuid4): uid of the group new_huid(str): the new human readable ID (huid) Source code in sealhits/db/dbset.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 def set_group_huid ( self , group_uid : uuid . uuid4 , new_huid : str ): \"\"\"Set a human readable uid. Args: group_uid (uuid.uuid4): uid of the group new_huid(str): the new human readable ID (huid) \"\"\" with Session ( self . engine ) as session : try : session . query ( Groups ) . filter ( Groups . uid == group_uid ) . update ( { \"huid\" : new_huid } ) session . commit () except IntegrityError as e : print ( \"set_group_huid failed\" , e )","title":"set_group_huid"},{"location":"reference/#sealhits.db.dbset.set_group_split","text":"Change the group split. Useful when splitting the group. Parameters: group_uid ( uuid4 ) \u2013 uid of the group new_split ( int ) \u2013 the new datetime for the group start. Source code in sealhits/db/dbset.py 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 def set_group_split ( self , group_uid : uuid . uuid4 , new_split : int ): \"\"\"Change the group split. Useful when splitting the group. Args: group_uid (uuid.uuid4): uid of the group new_split (int): the new datetime for the group start. \"\"\" with Session ( self . engine ) as session : try : session . query ( Groups ) . filter ( Groups . uid == group_uid ) . update ( { \"split\" : new_split } ) session . commit () except IntegrityError as e : print ( \"set_group_split failed\" , e )","title":"set_group_split"},{"location":"reference/#sealhits.db.dbset.set_group_timeend","text":"Change the group timeend value. Useful when splitting the group. Parameters: group_uid ( uuid4 ) \u2013 uid of the group new_timeend ( datetime ) \u2013 the new datetime for the group end. Source code in sealhits/db/dbset.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 def set_group_timeend ( self , group_uid : uuid . uuid4 , new_timeend : datetime . datetime ): \"\"\"Change the group timeend value. Useful when splitting the group. Args: group_uid (uuid.uuid4): uid of the group new_timeend (datetime.datetime): the new datetime for the group end. \"\"\" with Session ( self . engine ) as session : try : session . query ( Groups ) . filter ( Groups . uid == group_uid ) . update ( { \"timeend\" : new_timeend } ) session . commit () except IntegrityError as e : print ( \"set_group_timeend failed\" , e )","title":"set_group_timeend"},{"location":"reference/#sealhits.db.dbset.set_group_timestart","text":"Change the group timestart value. Useful when splitting the group. Parameters: group_uid ( uuid4 ) \u2013 uid of the group new_timestart ( datetime ) \u2013 the new datetime for the group start. Source code in sealhits/db/dbset.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def set_group_timestart ( self , group_uid : uuid . uuid4 , new_timestart : datetime . datetime ): \"\"\"Change the group timestart value. Useful when splitting the group. Args: group_uid (uuid.uuid4): uid of the group new_timestart (datetime.datetime): the new datetime for the group start. \"\"\" with Session ( self . engine ) as session : try : session . query ( Groups ) . filter ( Groups . uid == group_uid ) . update ( { \"timestart\" : new_timestart } ) session . commit () except IntegrityError as e : print ( \"set_group_timeend failed\" , e )","title":"set_group_timestart"},{"location":"reference/#sealhits.db.dbset.set_point_group","text":"Change the group that this point belongs to. Parameters: point_id ( uuid4 ) \u2013 uid of the point new_group ( uuid4 ) \u2013 the new group this point belongs to. Source code in sealhits/db/dbset.py 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 def set_point_group ( self , point_id : uuid . uuid4 , new_group : uuid . uuid4 ): \"\"\"Change the group that this point belongs to. Args: point_id (uuid.uuid4): uid of the point new_group (uuid.uuid4): the new group this point belongs to. \"\"\" with Session ( self . engine ) as session : try : session . query ( Points ) . filter ( Points . uid == point_id ) . update ( { \"group_id\" : new_group } ) session . commit () except IntegrityError as e : print ( \"set_point_group failed\" , e )","title":"set_point_group"},{"location":"reference/#sealhits.db.dbset.set_point_track","text":"Change the track that this point belongs to. Parameters: point_uid ( uuid4 ) \u2013 uid of the point new_track ( uuid4 ) \u2013 the new track this point belongs to. Source code in sealhits/db/dbset.py 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 def set_point_track ( self , point_uid : uuid . uuid4 , new_track : uuid . uuid4 ): \"\"\"Change the track that this point belongs to. Args: point_uid (uuid.uuid4): uid of the point new_track (uuid.uuid4): the new track this point belongs to. \"\"\" with Session ( self . engine ) as session : try : session . query ( Points ) . filter ( Points . uid == point_uid ) . update ( { \"track_id\" : new_track } ) session . commit () except IntegrityError as e : print ( \"set_point_track failed\" , e )","title":"set_point_track"},{"location":"reference/#database-updates","text":"dbupdate.py - The Postgresql update functions. The Postgresql Database update functions. Updating the objects that may already exist.","title":"Database updates"},{"location":"reference/#sealhits.db.dbup.update_group","text":"Update a single group. UID is the main key ingredient. We ignore foreign keys for now however. This function only updates the local group. Parameters: new_group ( Groups ) \u2013 the new Groups data. We update based on the uid. Source code in sealhits/db/dbup.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def update_group ( self , new_group : Groups ): \"\"\" Update a single group. UID is the main key ingredient. We ignore foreign keys for now however. This function only updates the local group. Args: new_group (Groups): the new Groups data. We update based on the uid. \"\"\" with Session ( self . engine ) as session : exist_group = session . execute ( select ( Groups ) . where ( Groups . uid == new_group . uid ) ) . scalar_one_or_none () if exist_group is not None : exist_group . bird = new_group . bird exist_group . code = new_group . code exist_group . comment = new_group . comment exist_group . fish = new_group . fish exist_group . gid = new_group . gid exist_group . glfs = new_group . glfs exist_group . huid = new_group . huid exist_group . interact = new_group . interact exist_group . mammal = new_group . mammal exist_group . sqlite = new_group . sqlite exist_group . sqliteid = new_group . sqliteid exist_group . split = new_group . split exist_group . timestart = new_group . timestart exist_group . timeend = new_group . timeend session . commit ()","title":"update_group"},{"location":"reference/#image-functions","text":"image.py - functions for reading and drawing images. Functions related to drawing out images.","title":"Image functions"},{"location":"reference/#sealhits.image.draw_bb","text":"Given an RGB Fan distorted image and a set of bb coords, draw a bounding box. Parameters: image ( Image ) \u2013 the PIL Image to draw on. bb ( XYBox ) \u2013 the box to draw. colour ( str , default: '#1111ff' ) \u2013 the colour of the box as an '#rrggbb' string. Source code in sealhits/image.py 34 35 36 37 38 39 40 41 42 43 44 45 46 def draw_bb ( image : Image , bb : XYBox , colour = \"#1111ff\" ): \"\"\"Given an RGB Fan distorted image and a set of bb coords, draw a bounding box. Args: image (PIL.Image): the PIL Image to draw on. bb (XYBox): the box to draw. colour (str): the colour of the box as an '#rrggbb' string. \"\"\" assert image . mode == \"RGB\" draw = ImageDraw . Draw ( image ) (( x0 , y0 ), ( x1 , y1 )) = bb . pair () xy = (( x0 , y0 ), ( x0 , y1 ), ( x1 , y1 ), ( x1 , y0 )) draw . polygon ( xy , fill = None , outline = colour )","title":"draw_bb"},{"location":"reference/#sealhits.image.draw_text","text":"Draw some text on our image. Parameters: image ( Image ) \u2013 the PIL Image to draw on. pos ( Tuple [ int , int ] ) \u2013 the coordinates to draw at. colour ( str , default: '#1111ff' ) \u2013 the colour of the box as an '#rrggbb' string. text ( str , default: 'none' ) \u2013 the text to draw. Source code in sealhits/image.py 49 50 51 52 53 54 55 56 57 58 59 def draw_text ( image : Image , pos : Tuple [ int , int ], colour = \"#1111ff\" , text = \"none\" ): \"\"\"Draw some text on our image. Args: image (PIL.Image): the PIL Image to draw on. pos (Tuple[int, int]): the coordinates to draw at. colour (str): the colour of the box as an '#rrggbb' string. text (str): the text to draw. \"\"\" draw = ImageDraw . Draw ( image ) draw . text ( pos , text , align = \"left\" , fill = colour )","title":"draw_text"},{"location":"reference/#sealhits.image.fan_distort","text":"The fan distortion function. We choose a height that works as our scaling ratio (1.732). Parameters: input_array ( ndarray ) \u2013 the image to distort. fan_height ( int ) \u2013 the height of the resulting fan image. bearing_table ( List [ float ] ) \u2013 The bearing table. Returns: ndarray \u2013 np.ndarray: the new fan image. Source code in sealhits/image.py 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 @numba . njit def fan_distort ( input_array : np . ndarray , fan_height : int , bearing_table : NumbaList [ float ] ) -> np . ndarray : \"\"\"The fan distortion function. We choose a height that works as our scaling ratio (1.732). Args: input_array (np.ndarray): the image to distort. fan_height (int): the height of the resulting fan image. bearing_table (NumbaList[float]): The bearing table. Returns: np.ndarray: the new fan image. \"\"\" fan_size = ( int ( math . floor ( 1.732 * float ( fan_height ))), fan_height , ) # cant use get_fan_size(fan_height) function with numba sadly fan_image = np . zeros (( fan_height , fan_size [ 0 ])) hx = int ( fan_size [ 0 ] / 2 ) # Allow any width, not just 512. wratio = float ( input_array . shape [ 1 ]) / 512.0 # TODO - Definitely room for speedup here - can use tables and lookups. # TODO - bilinear filtering - but as an option as masks shouldn't be filtered. for y in range ( 1 , fan_height ): # Limit x range as we progress up the fan tt = int ( y * math . tan ( math . radians ( 60 ))) sx = max ( 0 , hx - tt - 10 ) sy = min ( fan_size [ 0 ], hx + tt + 10 ) for x in range ( sx , sy ): dx = int ( x - hx ) bearing = 0 if dx != 0 : bearing = math . atan2 ( dx , y ) bearing_deg = math . degrees ( bearing ) if bearing_deg >= MIN_ANGLE and bearing_deg <= MAX_ANGLE : # We are inside the sweep so we need to now find the lookup # in the original image. However, we have a non-linear relationship # between the sonar x position and the true bearing, so we need # to find the closest sample point to our bearing via the lookup # table bearing_sidx = 0 distance = int ( y / math . cos ( math . fabs ( bearing )) / fan_height * input_array . shape [ 0 ] ) if distance < 0 or distance >= input_array . shape [ 0 ]: continue for idx in range ( 511 ): # Size of the bearing table bt = bearing_table . getitem_unchecked ( idx ) # This seems to speed up the numbalist stuff a lot! bn = bearing_table . getitem_unchecked ( idx + 1 ) if bt >= bearing and bn < bearing : bearing_sidx = int ( float ( idx ) * wratio ) break sample = input_array [ distance ][ bearing_sidx ] fan_image [ y ][ x ] = sample return fan_image","title":"fan_distort"},{"location":"reference/#sealhits.image.fits_to_np","text":"Given a path to a FITS file, attempt to return the numpy array and the fits header, or None if fails. Parameters: fits_path ( str ) \u2013 the path to the fits to load. Returns: Tuple [ array , PrimaryHDU ] \u2013 Tuple[np.array, fits.hdu.image.PrimaryHDU]: the loaded image as np.array and the FITS PrimaryHDU. Source code in sealhits/image.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 def fits_to_np ( fits_path : str ) -> Tuple [ np . array , fits . hdu . image . PrimaryHDU ]: \"\"\"Given a path to a FITS file, attempt to return the numpy array and the fits header, or None if fails. Args: fits_path (str): the path to the fits to load. Returns: Tuple[np.array, fits.hdu.image.PrimaryHDU]: the loaded image as np.array and the FITS PrimaryHDU. \"\"\" if \".lz4\" in fits_path : return decompress ( fits_path ) img = fits . open ( fits_path , memmap = False , lazy_load_hdus = False ) return ( img [ 0 ] . data , img [ 0 ] . header )","title":"fits_to_np"},{"location":"reference/#sealhits.image.get_group_images","text":"Return the image data and images for the group with this HUID. We check the cache first. All images must be preset and the correct size, otherwise we generate from new. Parameters: db ( DB ) \u2013 the database object. fits_path ( str ) \u2013 the path to the fits files. group_id ( group_id ) \u2013 Union[uuid.UUID, str]): either the uid or the huid for the group. sonar_id ( int , default: 854 ) \u2013 the id of the sonar to export. height ( int , default: 400 ) \u2013 the height of the resulting images. cache_path ( str , default: '.' ) \u2013 the path to the image cache. fan_transform ( bool , default: True ) \u2013 return fans instead of rectangles. Returns: Tuple [ array , List [ Images ]] \u2013 Tuple[np.array, List[Images]]: the images as a 3D np.array and a list of Images objects for each frame/image. Source code in sealhits/image.py 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 def get_group_images ( db : DB , fits_path : str , group_id : Union [ uuid . UUID , str ], sonar_id = 854 , height = 400 , cache_path = \".\" , fan_transform = True , ) -> Tuple [ np . array , List [ Images ]]: \"\"\"Return the image data and images for the group with this HUID. We check the cache first. All images must be preset and the correct size, otherwise we generate from new. Args: db (DB): the database object. fits_path (str): the path to the fits files. group_id (group_id: Union[uuid.UUID, str]): either the uid or the huid for the group. sonar_id (int): the id of the sonar to export. height (int): the height of the resulting images. cache_path (str): the path to the image cache. fan_transform (bool): return fans instead of rectangles. Returns: Tuple[np.array, List[Images]]: the images as a 3D np.array and a list of Images objects for each frame/image. \"\"\" if type ( group_id ) is uuid . UUID : group_uid = group_id else : g = db . get_group_huid ( group_id ) group_uid = g . uid group = db . get_group_uid ( group_uid ) uid = group . uid group_images = db . get_images_group_sonarid ( uid , sonar_id ) imgs = [] num_images = len ( group_images ) if num_images <= 0 : print ( \"No images found for group\" , uid ) return None np_frames = [] # Find all the images and create the base frames. Must be in the cache! Is probably # compressed as well. for img in group_images : dpath = is_cached_fan ( cache_path , img . filename ) cached = False if dpath is not None : data , header = decompress ( dpath ) np_frames . append ( data ) cached = True if not cached : fresult = fast_find ( img . filename , fits_path ) if fresult is not None : data , _ = fits_to_np ( fresult ) if fan_transform : fan_image = np . fliplr ( np . flipud ( fan_distort ( data , height , bearing_table )) ) np_frames . append ( fan_image ) else : np_frames . append ( data ) imgs . append ( img ) # It is possible, for some reason, that frames may not be the same size when in the RAW form, so # we run a check, making all images equal to the first. If the difference is too big we throw an error if not fan_transform : tframes = np_frames . copy () np_frames = [] first_shape = tframes [ 0 ] . shape np_frames . append ( tframes [ 0 ]) for frame in tframes [ 1 :]: # Row differences diff = first_shape [ 0 ] - frame . shape [ 0 ] if abs ( diff ) > 1 : raise AssertionError ( \"get_groups_images: RAW images in group differ too much\" ) if diff == - 1 : np . delete ( frame , - 1 , 0 ) if diff == 1 : frame = np . append ( frame , np . zeros (( 1 , first_shape [ 1 ])), 0 ) # Column differences diff = first_shape [ 1 ] - frame . shape [ 1 ] if abs ( diff ) > 1 : raise AssertionError ( \"get_groups_images: RAW images in group differ too much\" ) if diff == - 1 : np . delete ( frame , - 1 , 1 ) if diff == 1 : frame = np . append ( frame , np . zeros (( first_shape [ 0 ], 1 )), 1 ) np_frames . append ( frame ) try : np_frames = np . stack ( np_frames ) except Exception as e : print ( e ) return None return np_frames , imgs","title":"get_group_images"},{"location":"reference/#sealhits.image.normalise_image","text":"Normalise the image to range 0 to 1. Parameters: img ( array ) \u2013 the image to normalise. Returns: array \u2013 np.array: the normalised image (with a float32 type) Source code in sealhits/image.py 182 183 184 185 186 187 188 189 190 191 192 193 194 195 def normalise_image ( img : np . array ) -> np . array : \"\"\"Normalise the image to range 0 to 1. Args: img (np.array): the image to normalise. Returns: np.array: the normalised image (with a float32 type) \"\"\" fimg = img . astype ( np . float32 ) min_val = np . min ( img ) max_val = np . max ( img ) rval = ( fimg - min_val ) / ( max_val - min_val ) return rval","title":"normalise_image"},{"location":"reference/#sealhits.image.np_to_fits","text":"Given an np array, write out our fits image to a cache. Parameters: save_path ( str ) \u2013 the path and filename to save the fits to. img ( array ) \u2013 the image to save. image_time ( datetime ) \u2013 the time the image was taken. Source code in sealhits/image.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 def np_to_fits ( save_path : str , img : np . array , image_time : datetime . datetime ): \"\"\"Given an np array, write out our fits image to a cache. Args: save_path (str): the path and filename to save the fits to. img (np.array): the image to save. image_time: the time the image was taken. \"\"\" if not os . path . exists ( save_path ): try : hdr = fits . Header () hdr [ \"WIDTH\" ] = img . shape [ 1 ] hdr [ \"HEIGHT\" ] = img . shape [ 0 ] hdr [ \"YEAR\" ] = image_time . year hdr [ \"MONTH\" ] = image_time . month hdr [ \"DAY\" ] = image_time . day hdr [ \"HOUR\" ] = image_time . hour hdr [ \"MINUTE\" ] = image_time . minute hdr [ \"SECOND\" ] = image_time . second hdr [ \"MILLI\" ] = int ( image_time . microsecond / 1000 ) hdr = fits . PrimaryHDU ( img , header = hdr ) hdul = fits . HDUList ([ hdr ]) hdul . writeto ( save_path ) except Exception as e : print ( \"Could not save fits image:\" , save_path , e )","title":"np_to_fits"},{"location":"reference/#track-functions","text":"track.py - functions relating to tracks, such as overlaps. A number of functions used to detect tracks in the fan sonar images.","title":"Track functions"},{"location":"reference/#sealhits.track.get_bounding_boxes","text":"Get the track details for this group. Parameters: db ( DB ) \u2013 the database object. huid ( str ) \u2013 the huid for hte group we are looking for. imgs ( List [ Images ] ) \u2013 the list of Images records for this group. img_size ( Tuple [ int , int ] ) \u2013 The size of the images in pixels. Width then height. fan_distort ( bool , default: True ) \u2013 Are these images fans? Returns: List [ Tuple [ int , XYBox , str ]] \u2013 List[Tuple[int, XYBox, str]]: a list of frame numbers, bounding boxes and corresponding colours. Source code in sealhits/track.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 def get_bounding_boxes ( db : DB , huid : str , imgs : List [ Images ], img_size : Tuple [ int , int ], fan_distort = True ) -> List [ Tuple [ int , XYBox , str ]]: \"\"\"Get the track details for this group. Args: db (DB): the database object. huid (str): the huid for hte group we are looking for. imgs (List[Images]): the list of Images records for this group. img_size (Tuple[int,int]): The size of the images in pixels. Width then height. fan_distort (bool): Are these images fans? Returns: List[Tuple[int, XYBox, str]]: a list of frame numbers, bounding boxes and corresponding colours. \"\"\" bbs = [] group = db . get_group_huid ( huid ) uid = group . uid for idx , img in enumerate ( imgs ): points = db . get_image_points_by_filename_group ( img . filename , uid ) if ( len ( points ) > 0 ): # Since we have buffer start / end images and intermediates, 0 tracks are possible bb = points_to_bb ( points , img . range ) if fan_distort : bbox = bb . to_xy ( img_size ) (( xmin , ymin ), ( xmax , ymax )) = bbox . pair () # Flip BBS vertically to match flipped fan (normally flipped. #TODO - We should make this consistent) # This is a bit silly it seems bbs . append ( ( idx , XYBox ( xmin , img_size [ 1 ] - ymax , xmax , img_size [ 1 ] - ymin )) ) else : bbox = bb . to_xy_raw ( img_size ) # Flip BBS vertically to match flipped fan (normally flipped. #TODO - We should make this consistent) # This is a bit silly it seems bbs . append (( idx , bbox )) # Add a colour to the bounding box for ease of video and drawing. bbs = [( f , b , \"#ff0000\" ) for ( f , b ) in bbs ] return bbs","title":"get_bounding_boxes"},{"location":"reference/#utility-functions","text":"utils.py - misc utility functions. Useful utilities for various Sealhits related tasks.","title":"Utility functions"},{"location":"reference/#sealhits.utils.create_dir","text":"Create a directory if it doesn't already exist. Parameters: path ( str ) \u2013 the path to the directory we want to make. Returns: bool ( bool ) \u2013 success? Source code in sealhits/utils.py 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 def create_dir ( path : str ) -> bool : \"\"\"Create a directory if it doesn't already exist. Args: path (str): the path to the directory we want to make. Returns: bool: success? \"\"\" if not os . path . exists ( path ): try : os . makedirs ( path ) except Exception : return False return True","title":"create_dir"},{"location":"reference/#sealhits.utils.dist_bearing_to_xy","text":"Convert bearing (radians) and distance from a track to fan/polar image x, y coords. Image size is width/height. X and Y are integers within image_size. Parameters: bearing ( float ) \u2013 the bearing in radians. distance ( float ) \u2013 the distance in metres. max_range ( float ) \u2013 the maximum possible range (the sonar range) in metres. image_size ( Tuple [ int , int ] ) \u2013 the size of the image in pixels - width then height. Returns: Tuple [ int , int ] \u2013 Tuple[int, int]: the x and y coordinates in pixels. Source code in sealhits/utils.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 def dist_bearing_to_xy ( bearing : float , distance : float , max_range : float , image_size : Tuple [ int , int ] ) -> Tuple [ int , int ]: \"\"\"Convert bearing (radians) and distance from a track to fan/polar image x, y coords. Image size is width/height. X and Y are integers within image_size. Args: bearing (float): the bearing in radians. distance (float): the distance in metres. max_range (float): the maximum possible range (the sonar range) in metres. image_size: the size of the image in pixels - width then height. Returns: Tuple[int, int]: the x and y coordinates in pixels. \"\"\" # Assume bearing is either side of the vertical line? # Assume distance is in metres with the max being passed in as max_range # assert math.degrees(bearing) >= MIN_ANGLE and math.degrees(bearing) <= MAX_ANGLE # We are effectively rotating this 90 degrees anti clockwise and a POSITIVE bearing # should be to the LEFT of the centre line post rotation (this seems silly but is # the way according to PAMGuard) # TODO - we are returning integers here. We might wish to do floats at some point # so we can do interpolation. d0 = float ( distance / max_range * image_size [ 1 ]) x0 = math . sin ( math . fabs ( bearing )) * d0 y0 = math . cos ( math . fabs ( bearing )) * d0 hl = image_size [ 0 ] / 2 if bearing > 0 : x0 = hl - x0 else : x0 = hl + x0 return ( int ( x0 ), int ( y0 ))","title":"dist_bearing_to_xy"},{"location":"reference/#sealhits.utils.fast_find","text":"Find the full FITS bath based on how we store the images. This is much faster than a full path walk. We also look for images without the .lz4 extension. We prioritise the non compressed file. Parameters: name ( str ) \u2013 the file we are looking for. base_path ( str ) \u2013 the directory we are searching. Returns: Union [ str , None] \u2013 Union[str, None]: the path to the file if found, or None. Source code in sealhits/utils.py 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 def fast_find ( name : str , base_path : str ) -> Union [ str , None ]: \"\"\"Find the full FITS bath based on how we store the images. This is much faster than a full path walk. We also look for images without the .lz4 extension. We prioritise the non compressed file. Args: name (str): the file we are looking for. base_path (str): the directory we are searching. Returns: Union[str, None]: the path to the file if found, or None. \"\"\" joined_unzipped = os . path . join ( base_path , name ) if os . path . exists ( joined_unzipped ): return joined_unzipped joined = os . path . join ( base_path , name + \".lz4\" ) if os . path . exists ( joined ): return joined for tt in os . listdir ( base_path ): tt = os . path . join ( base_path , tt ) if os . path . isdir ( tt ): full_path_unzipped = os . path . join ( tt , name ) if os . path . isfile ( full_path_unzipped ): return full_path_unzipped full_path = os . path . join ( tt , name + \".lz4\" ) if os . path . isfile ( full_path ): return full_path print ( \"Could not find:\" , name ) return None","title":"fast_find"},{"location":"reference/#sealhits.utils.find","text":"Find a file somewhere underneath path, matching the pattern. Args: pattern (str): a pattern string for fnmatch. distance (float): a path to a directory to search. Returns: List [ str ] \u2013 List[str]: the resulting files as full paths. Source code in sealhits/utils.py 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 def find ( pattern : str , path : str ) -> List [ str ]: \"\"\"Find a file somewhere underneath path, matching the pattern. Args: pattern (str): a pattern string for fnmatch. distance (float): a path to a directory to search. Returns: List[str]: the resulting files as full paths. \"\"\" results = [] for root , dirs , files in os . walk ( path ): for name in files : if fnmatch . fnmatch ( name , pattern ): results . append ( os . path . join ( root , name )) return results","title":"find"},{"location":"reference/#sealhits.utils.get_fan_size","text":"Return a fan size, given the height. Uses a ratio of 1.732 given the 120 degree spread. Parameters: height ( int ) \u2013 the height we want in pixels. Returns: Tuple [ int , int ] \u2013 Tuple[int, int]: the width and height of the fan image in pixels. Source code in sealhits/utils.py 124 125 126 127 128 129 130 131 132 133 134 def get_fan_size ( height : int ) -> Tuple [ int , int ]: \"\"\" Return a fan size, given the height. Uses a ratio of 1.732 given the 120 degree spread. Args: height (int): the height we want in pixels. Returns: Tuple[int, int]: the width and height of the fan image in pixels. \"\"\" return ( int ( math . floor ( 1.732 * float ( height ))), height )","title":"get_fan_size"},{"location":"reference/#video-functions","text":"video.py - create videos from a numpy array. Generate video from a list of frames, bounding boxes and group information.","title":"Video functions"},{"location":"reference/#sealhits.video.gen_video","text":"Given the original frames and a Bounding Box list, create our final video. The BBS list is in the format (frame number, xmin, ymin, xmax, ymax, colour). The path is the full path to the video file, plus the extension (which determines the type). Parameters: frames ( array ) \u2013 the frames that makeup the video. bbs ( List [ Tuple [ int , XYBox , str ]] ) \u2013 the list of bounding boxes, frame numbers and the colour. text ( str ) \u2013 text to draw. out_path ( str , default: '.' ) \u2013 the full path to the video. Must end in a video type like '.mp4' or '.webm'. colour_map ( Palette , default: Batlow_20 ) \u2013 the colour palette to use. rate ( int , default: 4 ) \u2013 frames per second. Returns: str ( str ) \u2013 path to the saved video. Source code in sealhits/video.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def gen_video ( frames : np . array , bbs : List [ Tuple [ int , XYBox , str ]], text : str , out_path = \".\" , colour_map = Batlow_20 , rate = 4 ) -> str : \"\"\"Given the original frames and a Bounding Box list, create our final video. The BBS list is in the format (frame number, xmin, ymin, xmax, ymax, colour). The path is the full path to the video file, plus the extension (which determines the type). Args: frames (np.array): the frames that makeup the video. bbs (List[Tuple[int, XYBox, str]]): the list of bounding boxes, frame numbers and the colour. text (str): text to draw. out_path (str): the full path to the video. Must end in a video type like '.mp4' or '.webm'. colour_map (palettable.Palette): the colour palette to use. rate (int): frames per second. Returns: str: path to the saved video. \"\"\" over_frames = [] font = ImageFont . truetype ( \"./Hack-Regular.ttf\" , 16 ) for fidx in range ( frames . shape [ 0 ]): frame = frames [ fidx ] blank_image = Image . fromarray ( np . full (( frame . shape [ 0 ], frame . shape [ 1 ], 3 ), ( 0 , 0 , 0 ), np . uint8 ) ) draw = ImageDraw . Draw ( blank_image ) lines = text . split ( \" \\n \" ) for i , line in enumerate ( lines ): draw . text (( 10 , i * 20 ), str ( line ), font = font ) if bbs is not None : for frame_id , bbox , colour in bbs : if frame_id == fidx : draw . rectangle ( bbox . tuple (), outline = colour ) over_frames . append ( np . array ( blank_image )) # Now create a quick video coloured = frames # If this is a luminance image, do the nice colour mapping if len ( frames . shape ) == 3 or ( frames . shape [ - 1 ] == 1 and len ( frames . shape ) == 4 ): np_fan = np . array ( normalise_image ( frames ) * 255 , dtype = np . uint8 ) coloured = np . zeros (( * np_fan . shape , 3 ), dtype = np . uint8 ) # Take entries from RGB LUT according to greyscale values in image lut = [ colour_map . mpl_colormap ( x / 255.0 ) for x in range ( 256 )] lut = [[ int ( x [ 0 ] * 255 ), int ( x [ 1 ] * 255 ), int ( x [ 2 ] * 255 )] for x in lut ] np . take ( lut , np_fan , axis = 0 , out = coloured ) np_track = np . array ( over_frames , dtype = np . uint8 ) coloured = np . maximum ( coloured , np_track ) ffmpegio . video . write ( out_path , rate , coloured , overwrite = True , show_log = False ) return out_path","title":"gen_video"},{"location":"reference/#file-functions","text":"files.py - find the files on disk that we need. The various functions that search out the files we want and the tracks we are looking for.","title":"File functions"},{"location":"reference/#sealhits.sources.files.bin_files_avail","text":"Return a list of binary PGDF files with the corresponding date range. Parameters: pdir ( str ) \u2013 the path to the gemini binary files. Returns: List [ Tuple [ str , datetime , datetime ]] \u2013 List[Tuple[str, datetime.datetime, datetime.datetime]]: A list of paths to the pgdf including the start and end datetimes Source code in sealhits/sources/files.py 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 def bin_files_avail ( pdir : str , ) -> List [ Tuple [ str , datetime . datetime , datetime . datetime ]]: \"\"\"Return a list of binary PGDF files with the corresponding date range. Args: pdir (str): the path to the gemini binary files. Returns: List[Tuple[str, datetime.datetime, datetime.datetime]]: A list of paths to the pgdf including the start and end datetimes \"\"\" efiles = [] for root , _ , files in os . walk ( pdir , topdown = False ): for name in files : fpath = os . path . join ( root , name ) _ , file_extension = os . path . splitext ( name ) try : if \"pgdf\" in file_extension . lower (): print ( \"Opening pgdf:\" , fpath ) tp = pgdf . PGDF ( fpath ) if len ( tp . module . objects ) > 0 : date_min = None date_max = None for pamobj in tp . module . objects : tdate = pamobj . pam . date assert tdate is not None if date_min is None : date_min = tdate elif date_min > tdate : date_min = tdate if date_max is None : date_max = tdate elif date_max < tdate : date_max = tdate print ( \"Adding pgdf:\" , fpath ) efiles . append (( fpath , date_min , date_max )) else : print ( \"PGDF has no module objects.\" ) except Exception as e : print ( \"Could not read\" , fpath , e ) # Sort the efile bases on the dates efiles . sort ( key = lambda ef : ef [ 1 ]) return efiles","title":"bin_files_avail"},{"location":"reference/#sealhits.sources.files.get_tracks","text":"Read the found PGDF files and generate the tracks for our eventual dataset. Parameters: bin_files ( str ) \u2013 the path to the PGDF files. Returns: List [ PGObject ] \u2013 List[PGObject]: A list of PGObject representing tracks from PAMGuard. Source code in sealhits/sources/files.py 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 def get_tracks ( bin_files : List [ str ]) -> List [ PGObject ]: \"\"\"Read the found PGDF files and generate the tracks for our eventual dataset. Args: bin_files (str): the path to the PGDF files. Returns: List[PGObject]: A list of PGObject representing tracks from PAMGuard. \"\"\" tracks = [] # TODO - it is possible that one track can cross multiple files. # We've ignored this for now # https://github.com/PAMGuard/PAMGuardMatlab/blob/main/pgmatlab/loadPamguardMultiFile.m # UID in the track group is different to the UID in the Track group children. # Trackgroup children UID *should* match the UID in the pgdf for fb in bin_files : print ( \"Reading bin file:\" , fb ) pbin = pgdf . PGDF ( fb ) assert pbin . header . module_type == \"Gemini Threshold Detector\" for obj in pbin . module . objects : tracks . append ( obj ) # Sort tracks in order of time if not already tracks = sorted ( tracks , key = lambda track : track . data . track . time_start ) return tracks","title":"get_tracks"},{"location":"reference/#sealhits.sources.files.glf_files_avail","text":"Given a starting path, find all the possible glf files. Parameters: glf_path ( str ) \u2013 the path to the GLF files. Returns: List [ str ] \u2013 List[str]: A list of full paths to all found GLF files. Source code in sealhits/sources/files.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 def glf_files_avail ( glf_path : str ) -> List [ str ]: \"\"\"Given a starting path, find all the possible glf files. Args: glf_path (str): the path to the GLF files. Returns: List[str]: A list of full paths to all found GLF files. \"\"\" full_paths = [] for root , _ , files in os . walk ( glf_path , topdown = False ): for name in files : _ , file_extension = os . path . splitext ( name ) if file_extension . lower () == \".glf\" : full_paths . append ( os . path . join ( root , name )) return full_paths","title":"glf_files_avail"},{"location":"reference/#sealhits.sources.files.pgdfs_paths","text":"Find all PGDFs under a given path. Parameters: pgdf_path ( str ) \u2013 the path to the PGDF files. Returns: List [ str ] \u2013 List[str]: A list of full paths to all found PGDF files. Source code in sealhits/sources/files.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 def pgdfs_paths ( pgdf_path : str ) -> List [ str ]: \"\"\" Find all PGDFs under a given path. Args: pgdf_path (str): the path to the PGDF files. Returns: List[str]: A list of full paths to all found PGDF files. \"\"\" full_paths = [] for root , _ , files in os . walk ( pgdf_path , topdown = False ): for name in files : _ , file_extension = os . path . splitext ( name ) if file_extension . lower () == \".pgdf\" : fpath = os . path . join ( root , name ) full_paths . append ( fpath ) return full_paths","title":"pgdfs_paths"},{"location":"reference/#sealhits.sources.files.pgdfs_to_full_paths","text":"Fill out the pgdf filenames with their full paths. Parameters: pgdf_path ( str ) \u2013 the path to the PGDFS files. pgdfs ( List [ str ] ) \u2013 a list of PGDF names to look for Returns: List [ str ] \u2013 List[str]: A list of full paths to the PGDFS in the pgdfs list. Source code in sealhits/sources/files.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 def pgdfs_to_full_paths ( pgdf_path : str , pgdfs : List [ str ]) -> List [ str ]: \"\"\"Fill out the pgdf filenames with their full paths. Args: pgdf_path (str): the path to the PGDFS files. pgdfs (List[str]): a list of PGDF names to look for Returns: List[str]: A list of full paths to the PGDFS in the pgdfs list. \"\"\" full_paths = [] for root , _ , files in os . walk ( pgdf_path , topdown = False ): for name in files : if name in pgdfs : fpath = os . path . join ( root , name ) full_paths . append ( fpath ) return full_paths","title":"pgdfs_to_full_paths"},{"location":"reference/#ingesting-glfs","text":"glf.py - all the functions related to GLF files Functions for ingesting the GLF files. This involves finding the files on disk, picking the ones we need, reading the required image records, then saving these images to disk as LZ4 compressed files.","title":"Ingesting GLFs"},{"location":"reference/#sealhits.sources.glf.GDat","text":"Bases: object An internal object that holds all the details we want on our GLFs as we process them. Source code in sealhits/sources/glf.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 class GDat ( object ): \"\"\"An internal object that holds all the details we want on our GLFs as we process them.\"\"\" def __init__ ( self , startdate : datetime . datetime , enddate : datetime . datetime , gobj : GLFS , full_path : str , ): self . start_date = startdate self . end_date = enddate self . gobj = gobj self . full_path = full_path","title":"GDat"},{"location":"reference/#sealhits.sources.glf.get_times","text":"Get the start and end times from this GLF file. Parameters: gpath ( str ) \u2013 path to a single GLF file. Returns: Union [ Tuple [ datetime , datetime ], None] \u2013 Union[Tuple[datetime.datetime, datetime.datetime], None]: either the start and end times, or None. Source code in sealhits/sources/glf.py 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 def get_times ( gpath : str ) -> Union [ Tuple [ datetime . datetime , datetime . datetime ], None ]: \"\"\"Get the start and end times from this GLF file. Args: gpath (str): path to a single GLF file. Returns: Union[Tuple[datetime.datetime, datetime.datetime], None]: either the start and end times, or None. \"\"\" try : with GLF ( gpath ) as gf : time_start = None time_end = None for image_rec in gf . images : image_time = image_rec . db_tx_time if time_start is None : time_start = image_time elif image_time < time_start : time_start = image_time if time_end is None : time_end = image_time elif image_time > time_end : time_end = image_time assert time_start is not None assert time_end is not None return ( time_start , time_end ) except Exception as e : logging . error ( \"Failed to read glf: %s , %s \" , gpath , e ) return None","title":"get_times"},{"location":"reference/#sealhits.sources.glf.glf_get_image","text":"Get an image from a GLF file using the given record Parameters: gpath ( str ) \u2013 path to a single GLF file. image_rec ( ImageRecord ) \u2013 the GLF File image record Returns: Tuple [ bytes , Tuple [ int , int ]] \u2013 Tuple[bytes, Tuple[int, int]]: raw data as bytes and the width and height in pixels as ints. Source code in sealhits/sources/glf.py 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 def glf_get_image ( gpath : str , image_rec ) -> Tuple [ bytes , Tuple [ int , int ]]: \"\"\" Get an image from a GLF file using the given record Args: gpath (str): path to a single GLF file. image_rec (ImageRecord): the GLF File image record Returns: Tuple[bytes, Tuple[int, int]]: raw data as bytes and the width and height in pixels as ints. \"\"\" # TODO - as nice as it is to wrap the GLF path here, this means a double # open often, as glf_get_image can be inside a loop with glf_times_range above # Might need to think about how these two functions are used together. # It's also a small function so maybe there's a better way? try : with GLF ( gpath ) as gf : image_data , image_size = gf . extract_image ( image_rec ) return image_data , image_size except Exception as e : logging . error ( \"_glf_times failed to read glf: %s \" , gpath ) logging . error ( \"Exception %s \" , e ) return None","title":"glf_get_image"},{"location":"reference/#sealhits.sources.glf.glf_times_range","text":"Given a path to a GLF file, and a start and end time, return an img record and the sonar range. This is an iterator function. Parameters: gpath ( str ) \u2013 path to a single GLF file. start_t ( datetime ) \u2013 the start datetime. end_t ( datetime ) \u2013 the end datetime. Returns: Tuple [ ImageRecord , float ] \u2013 Union[Tuple[datetime.datetime, datetime.datetime], None]: either the start and end times, or None. Source code in sealhits/sources/glf.py 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 def glf_times_range ( gpath : str , start_t : datetime . datetime , end_t : datetime . datetime ) -> Tuple [ ImageRecord , float ]: \"\"\" Given a path to a GLF file, and a start and end time, return an img record and the sonar range. This is an iterator function. Args: gpath (str): path to a single GLF file. start_t (datetime.datetime): the start datetime. end_t (datetime.datetime): the end datetime. Returns: Union[Tuple[datetime.datetime, datetime.datetime], None]: either the start and end times, or None. \"\"\" # TODO - I wonder if this could be made faster with a time index? # Also, yielding from within a with statement might mean a lot of opening and closing? Or maybe not? try : with GLF ( gpath ) as gf : for image_rec in gf . images : image_time = image_rec . db_tx_time if image_time >= start_t and image_time <= end_t : sonar_range = int ( round ( calculate_range ( image_rec ))) yield ( image_rec , sonar_range ) except Exception as e : logging . error ( \"_glf_times failed to read glf: %s \" , gpath ) logging . error ( \"Exception %s \" , e ) raise IOError","title":"glf_times_range"},{"location":"reference/#sealhits.sources.glf.has_track","text":"Look in the database to see if this image has a track. Parameters: session ( Session ) \u2013 current SQLAlchemy session. image_time ( datetime ) \u2013 the datetime of the current image. sonar_id ( int ) \u2013 the sonar id. Returns: bool ( bool ) \u2013 does this image have a track Source code in sealhits/sources/glf.py 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 def has_track ( session : Session , image_time : datetime . datetime , sonar_id : int ) -> bool : \"\"\"Look in the database to see if this image has a track. Args: session (Session): current SQLAlchemy session. image_time (datetime.datetime): the datetime of the current image. sonar_id (int): the sonar id. Returns: bool: does this image have a track \"\"\" delta = datetime . timedelta ( milliseconds = 10 ) points = [] with session . no_autoflush : points = ( session . query ( Points ) . filter ( Points . time >= image_time - delta , Points . time <= image_time + delta ) . all () ) for point in points : if point . sonarid == sonar_id : return True return False","title":"has_track"},{"location":"reference/#sealhits.sources.glf.process_glf_by_group","text":"Process a single group, outputting all of the images from both sonars for the time period of this group. fname_lookup is altered by the function, storing the new image objects by the filename. Parameters: session ( Session ) \u2013 the current SQLAlchemy session. group ( Groups ) \u2013 the Group we are currently looking at. fname_lookup ( dict ) \u2013 a lookup of images by filename. Can be an empty dict. gdats ( List [ GDat ] ) \u2013 a list of GDat objects. max_glf ( int ) \u2013 the maximum number of images to consider. outpath ( str ) \u2013 where to save the output images. Returns: Tuple [ List [ Images ], dict ] \u2013 Tuple[List[Images], dict]: A list of the new Images objects created and the dict mapping filenames onto these images objects. Source code in sealhits/sources/glf.py 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 def process_glf_by_group ( session : Session , group : Groups , fname_lookup , gdats : List [ GDat ], max_glf : int , outpath : str ) -> Tuple [ List [ Images ], dict ]: \"\"\"Process a single group, outputting all of the images from both sonars for the time period of this group. fname_lookup is altered by the function, storing the new image objects by the filename. Args: session (Session): the current SQLAlchemy session. group (Groups): the Group we are currently looking at. fname_lookup (dict): a lookup of images by filename. Can be an empty dict. gdats (List[GDat]): a list of GDat objects. max_glf (int): the maximum number of images to consider. outpath (str): where to save the output images. Returns: Tuple[List[Images], dict]: A list of the new Images objects created and the dict mapping filenames onto these images objects. \"\"\" group_start = group . timestart group_end = group . timeend group_image_count = 0 new_images = [] # We need a check here on the length of the group as there are some erroneous # SUPER long groups we can't ingest really. Anything longer than 2 minutes ignore # TODO - could potentialy ingest *up-to* this seconds amount? gdd = group . timeend - group . timestart if gdd . total_seconds () > max_glf : logging . warn ( \"Group %s is much too long with a time of %s seconds.\" , group . huid , str ( gdd . total_seconds ()), ) return new_images , fname_lookup # Empty at this point found_gdat = False for gdat in gdats : glfname = gdat . gobj . filename # If our start is greater, we've already passed it as gdats is ordered. # probably doesn't save us too much time. if gdat . start_date > group_end : break if group_start <= gdat . end_date and group_end >= gdat . start_date : # Link GLF to group. with session . no_autoflush : # Check it doesn't exist already as sometimes we get duplicates if group not in gdat . gobj . groups : gdat . gobj . groups . append ( group ) # Now fully read in the GLF # We read one frame at a time because it will use a lot of memory # TODO - could we check to see if the FITS already exists BEFORE the call to _glf_times? try : for image_rec , srange in glf_times_range ( gdat . full_path , group_start , group_end ): found_gdat = True image_time = image_rec . db_tx_time sonar_id = image_rec . header . device_id milli = int ( image_time . microsecond / 1000 ) ext = \".fits\" # Setup the FITS filename; a combination of time and sonar id. fname = ( image_time . strftime ( \"%Y_%m_ %d _%H_%M_%S_\" ) + f \" { milli : 03d } \" + \"_\" + str ( sonar_id ) + ext ) fname_compressed = fname + \".lz4\" subdir = os . path . join ( outpath , image_time . strftime ( \"%Y_%m_ %d \" )) if not os . path . exists ( subdir ): os . mkdir ( subdir ) # Check to see if this image already exists. It might if groups overlap full_fits_path = os . path . join ( subdir , fname_compressed ) if not os . path . exists ( full_fits_path ): # TODO - remove this I think or fire up an error to ignore this GLF # as looping around is not ideal! # For some reason, this can fail too but the file seems fine. # Therefore, we should loop around again till it reads correctly res = glf_get_image ( gdat . full_path , image_rec ) image_data , image_size = res try : image_np = np . frombuffer ( image_data , dtype = np . uint8 ) . reshape ( ( image_size [ 1 ], image_size [ 0 ]) ) hdr = fits . Header () hdr [ \"SONARID\" ] = sonar_id hdr [ \"WIDTH\" ] = image_size [ 0 ] hdr [ \"HEIGHT\" ] = image_size [ 1 ] hdr [ \"YEAR\" ] = image_time . year hdr [ \"MONTH\" ] = image_time . month hdr [ \"DAY\" ] = image_time . day hdr [ \"HOUR\" ] = image_time . hour hdr [ \"MINUTE\" ] = image_time . minute hdr [ \"SECOND\" ] = image_time . second hdr [ \"MILLI\" ] = int ( image_time . microsecond / 1000 ) # hdr = fits.PrimaryHDU(image_np, header=hdr) # hdul = fits.HDUList([hdr]) # hdul.writeto(full_fits_path) compress ( image_np , hdr , full_fits_path ) del image_data del image_np del hdr logging . info ( \"Generated %s from %s \" , full_fits_path , gdat . full_path ) except Exception as e : logging . error ( \"Could not generate FITS: %s , %s \" , fname , e , ) logging . error ( \"Traceback %s \" , traceback . format_exc ()) # else: # logging.info(\"File %s already exists for group %s\", full_fits_path, group.huid) # Now create the DB objects. We create new image objects, or # we find the existing one and modify it. We return all images # and hope our transaction does the right thing in adding or # updating. group_image_count += 1 new_image = None with session . no_autoflush : q = session . query ( Images ) . filter ( Images . filename == fname ) new_image = q . one_or_none () # It's also possible that we already have this image ready to be committed to the # DB but it gets pulled in again for a different group so we must check if fname in fname_lookup . keys (): new_image = fname_lookup [ fname ] if new_image is None : ht = has_track ( session , image_time , sonar_id ) new_image = Images ( uid = uuid . uuid4 (), filename = fname , hastrack = ht , glf = glfname , time = image_time , sonarid = sonar_id , range = srange , ) fname_lookup [ fname ] = new_image if group not in new_image . groups : new_image . groups . append ( group ) new_images . append ( new_image ) except IOError : logging . Error ( \"Failed to read GLF %s . Skipping...\" , glfname ) if not found_gdat : logging . error ( \"Found no GDATS for group %s \" , group . huid ) if group_image_count == 0 : logging . error ( \"*** Group %s has no images! ***\" , group . huid ) return new_images , fname_lookup","title":"process_glf_by_group"},{"location":"reference/#sealhits.sources.glf.process_glfs","text":"Once the PGDFs and SQLITE are processed, we can begin to look for the GLF files we need. We want each new group to have a number of FITS images fitting the range timestart - buffer to time-end + buffer. The function split_groups will have split groups and adjusted start and end times already so we judt need to look at the group times. Parameters: session ( Session ) \u2013 the current SQLAlchemy session. groups ( List [ Groups ] ) \u2013 the Groups we are currently looking at. glfpath ( str ) \u2013 the path to the GLF fules max_glf ( int ) \u2013 the maximum number of images to consider. outpath ( str ) \u2013 where to save the output images. Returns: Tuple [ List [ GLFS ], List [ Images ]] \u2013 Tuple[List[GLFS], List[Images]]: Two lists - the new GLFS objects to save to the DB and the new Images objects to save to the DB. Source code in sealhits/sources/glf.py 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 def process_glfs ( session : Session , groups : List [ Groups ], glfpath : str , outpath : str , max_glf : int ) -> Tuple [ List [ GLFS ], List [ Images ]]: \"\"\"Once the PGDFs and SQLITE are processed, we can begin to look for the GLF files we need. We want each new group to have a number of FITS images fitting the range timestart - buffer to time-end + buffer. The function split_groups will have split groups and adjusted start and end times already so we judt need to look at the group times. Args: session (Session): the current SQLAlchemy session. groups (List[Groups]): the Groups we are currently looking at. glfpath (str): the path to the GLF fules max_glf (int): the maximum number of images to consider. outpath (str): where to save the output images. Returns: Tuple[List[GLFS], List[Images]]: Two lists - the new GLFS objects to save to the DB and the new Images objects to save to the DB. \"\"\" new_glfs = [] logging . info ( \"Finding available GLF files...\" ) glf_files = glf_files_avail ( glfpath ) # Go through all GLFs and get the times of each. # Check the database first for the times we already have and # only grab the times for GLFs we don't have # There might be duplicates due to directory renaming or similar and rsync. times , glf_files_missing = _find_glf_times_db ( session , glf_files ) logging . info ( \"Retrieving GLF Times (this may take a while)...\" ) if len ( glf_files_missing ) > 0 : logging . info ( \"GLFs missing from DB: %s \" , len ( glf_files_missing )) # commented out due to errors on hdd5 for some reason? # with ThreadPool(NUM_THREADS) as pool: # times += pool.map(_get_glf_times, glf_files_missing, chunksize=NUM_THREADS) for glf_file in glf_files_missing : new_times = _get_glf_times ( glf_file ) if new_times [ 1 ] is not None : times . append ( new_times ) # Add the GLFS regardless of whether or not they are used. # times_glfs is the list of all the info we need. # gdats are just a struct that holds the times, database # object and path together. gdats = [] for ( glf_start , glf_end ), glf_path in times : glfname = os . path . basename ( glf_path ) new_glf = None with session . no_autoflush : q = session . query ( GLFS ) . filter ( GLFS . filename == glfname ) new_glf = q . one_or_none () # It's possible (for some reason) that GLFS might get double # counted in the addition phase, so make sure it doesn't exist # in the new_glfs already for ng in new_glfs : if ng . filename == glfname : new_glf = ng if new_glf is None : new_glf = GLFS ( uid = uuid . uuid4 (), filename = glfname , startdate = glf_start , enddate = glf_end , groups = [], ) new_glfs . append ( new_glf ) gdats . append ( GDat ( glf_start , glf_end , new_glf , glf_path )) # Make sure there are no errored entries and organise by time # Sorting by time makes the FITS export a little quicker. gdats = sorted ( gdats , key = functools . cmp_to_key ( sort_times )) logging . info ( \"Number of initial time ranges: %s \" , len ( gdats )) gdat_latest = gdats [ - 1 ] . end_date for gd in gdats : if gd . end_date > gdat_latest : gdat_latest = gd . end_date logging . info ( \"GDats earliest %s and latest %s .\" , str ( gdats [ 0 ] . start_date ), str ( gdat_latest ) ) sofar = 0 tlist = [] logging . info ( \"Number of groups: %s \" , len ( groups )) # The goal at this stage is to match up new group times with GLF times # and only process the files we need to, even if we've recorded a whole # batch of GLFs for a time period. groups = sorted ( groups , key = functools . cmp_to_key ( sort_times_group )) group_earliest = groups [ 0 ] . timestart group_latest = groups [ 0 ] . timeend for g in groups : if g . timeend > group_latest : group_latest = g . timeend assert group_earliest < group_latest logging . info ( \"Groups earliest %s and latest %s .\" , str ( group_earliest ), str ( group_latest ) ) new_images = [] new_images_by_fname = {} # Fast lookup # Start off the threads, chunking up the IDs then submitting to the threaded function. while len ( groups ) > 0 : while len ( groups ) > 0 and len ( tlist ) < NUM_THREADS : tlist . append ( groups . pop ()) for group in tlist : ni , nf = process_glf_by_group ( session , group , new_images_by_fname , gdats , max_glf , outpath ) new_images += ni new_images_by_fname = nf sofar += len ( tlist ) tlist = [] logging . info ( \"Processed %s . %s remaining.\" , sofar , len ( groups )) return ( new_glfs , new_images )","title":"process_glfs"},{"location":"reference/#sealhits.sources.glf.sort_times","text":"Sort GDat by start date Source code in sealhits/sources/glf.py 101 102 103 104 105 106 107 108 109 110 111 112 def sort_times ( a , b ): \"\"\"Sort GDat by start date\"\"\" s0 = a . start_date s1 = b . start_date if s0 < s1 : return - 1 if s0 == s1 : return 1 return 0","title":"sort_times"},{"location":"reference/#sealhits.sources.glf.sort_times_group","text":"Sort Groups by time start Source code in sealhits/sources/glf.py 115 116 117 118 119 120 121 122 123 124 125 126 def sort_times_group ( a , b ): \"Sort Groups by time start\" s0 = a . timestart s1 = b . timestart if s0 < s1 : return - 1 if s0 == s1 : return 1 return 0","title":"sort_times_group"},{"location":"reference/#extraction-from-glfs","text":"glfextract.py - extract numpy arrays from GLF files. A short utility function to extract numpy arrays from a GLF file within a particular time range.","title":"Extraction from GLFs"},{"location":"reference/#sealhits.sources.glfextract.extract","text":"Given a path to a GLF file, a start and end time, return a 3D numpy array of images within that time frame. Parameters: gpath ( str ) \u2013 The path to a GLF file. start_t ( datetime ) \u2013 The starting date time. end_t ( datetime ) \u2013 The ending date time. Returns: Union [None, array ] \u2013 Union[None, np.array]: either None, or an np.array of frames Source code in sealhits/sources/glfextract.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 def extract ( gpath : str , start_t : datetime . datetime , end_t : datetime . datetime ) -> Union [ None , np . array ]: \"\"\" Given a path to a GLF file, a start and end time, return a 3D numpy array of images within that time frame. Args: gpath (str): The path to a GLF file. start_t (datetime.datetime): The starting date time. end_t (datetime.datetime): The ending date time. Returns: Union[None, np.array]: either None, or an np.array of frames \"\"\" try : with GLF ( gpath ) as gf : time_start = None time_end = None frames = [] for image_rec in tqdm ( gf . images , desc = \"Ingesting Images\" ): image_time = image_rec . db_tx_time if time_start is None : time_start = image_time elif image_time < time_start : time_start = image_time if time_end is None : time_end = image_time elif image_time > time_end : time_end = image_time if image_time >= start_t and image_time <= end_t : image_data , image_size = gf . extract_image ( image_rec ) image_np = np . frombuffer ( image_data , dtype = np . uint8 ) . reshape (( image_size [ 1 ], image_size [ 0 ])) frames . append ( image_np ) frames = np . array ( frames ) return frames except Exception as e : print ( e ) return None","title":"extract"},{"location":"reference/#ingesting-groups","text":"group.py - group object functions Functions for sorting out group objects, such as fixing the times and splitting based on a buffer time.","title":"Ingesting Groups"},{"location":"reference/#sealhits.sources.group.find_group_objects","text":"Start by finding the Groups and TrackGroups from the SQLITE file and creating these in memory. sqlalias is required in case one is reading in an updated or otherwise changed sqlitefile that has a different name. Groups will exist from this previous version of the file and should be included. sqlalias should be set to the name of that older file. Otherwise, alias should match sqlname. Parameters: session ( Session ) \u2013 The current SQLAlchemy session. sqldb ( SQLPAM ) \u2013 The SQLPAM database object we are reading. sqlname ( str ) \u2013 The filename of the sqlite file. sqlalias ( str ) \u2013 If we are reading from a different sqlite file but want to lookup with an alias, add a different name here. max_secs ( int , default: 800 ) \u2013 The maximum length of group to consider in seconds. Returns: Tuple [ List [ Groups ], List [ TrackGroup ]] \u2013 Tuple[List[Groups], List[TrackGroup]]: Two lists, the new Groups objects and the TrackGroup objects. Source code in sealhits/sources/group.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 def find_group_objects ( session : Session , sqldb : sqlpam . SQLPAM , sqlname : str , sqlalias : str , max_secs = 800 ) -> Tuple [ List [ Groups ], List [ TrackGroup ]]: \"\"\"Start by finding the Groups and TrackGroups from the SQLITE file and creating these in memory. sqlalias is required in case one is reading in an updated or otherwise changed sqlitefile that has a different name. Groups will exist from this previous version of the file and should be included. sqlalias should be set to the name of that older file. Otherwise, alias should match sqlname. Args: session (Session): The current SQLAlchemy session. sqldb (SQLPAM): The SQLPAM database object we are reading. sqlname (str): The filename of the sqlite file. sqlalias (str): If we are reading from a different sqlite file but want to lookup with an alias, add a different name here. max_secs (int): The maximum length of group to consider in seconds. Returns: Tuple[List[Groups], List[TrackGroup]]: Two lists, the new Groups objects and the TrackGroup objects. \"\"\" group_list = sqldb . track_groups found_groups = [] found_tracks = [] logging . info ( \"Reading Groups...\" ) for g in tqdm ( group_list , desc = \"Reading Groups\" ): interaction = g . interaction code = \"none\" if g . track_type is not None : code = g . track_type . lower () # Start with the master group object # Check the *real* composite primary key. If it exists, # return that instead of a new thing. Means we can use # merge later on to do easy updates. # split can be -1 or 0. If it's not been split or it's been split and it's the first one. new_group = None with session . no_autoflush : q = session . query ( Groups ) . filter ( Groups . gid == g . uid , Groups . sqliteid == g . id , or_ ( Groups . split == - 1 , Groups . split == 0 ), Groups . sqlite == sqlalias , ) new_group = q . one_or_none () if new_group is None : huid = generate_id () new_group = Groups ( uid = uuid . uuid4 (), gid = g . uid , sqliteid = g . id , sqlite = sqlname , huid = huid , timestart = g . utc , timeend = g . end_time , code = code , comment = g . comment , mammal = g . mammal , fish = g . fish , bird = g . bird , interact = interaction , split =- 1 , pgdfs = [], images = [], glfs = [], ) logging . info ( \"New Group: %d , %d , %s , %s \" , g . uid , g . id , sqlname , huid ) else : # Check to see if this group needs updating new_group . timestart = g . utc new_group . timeend = g . end_time new_group . code = code new_group . comment = g . comment new_group . mammal = g . mammal new_group . fish = g . fish new_group . bird = g . bird new_group . interact = interaction # We may need to change the sqlname which is part of the composite key # We set it to the new SQLITEDB we are importing. new_group . sqlite = sqlname # Early rejection of groups that are too long. Note that due to bugs in PAMGuard # the group times may not be accurate. However, we reject here so that the tracks # are also not included. TODO - this could be improved. td = datetime . timedelta ( seconds = max_secs ) if new_group . timeend - new_group . timestart > td : logging . warn ( \"Found %s Group that exceed %d seconds! Not including!\" , new_group . huid , max_secs ) continue # Now Look at the track children - this links to the # PGDFs we are interested in. # We also add the track_id to the tracks_groups table # using our own UUID instead of the gids for c in g . children : binary_file = c . binary_file # Make sure the binary file is a single filename # with the pgdf extension binary_file = os . path . basename ( binary_file ) if binary_file [ - 5 :] != \".pgdf\" : binary_file += \".pgdf\" new_track = None with session . no_autoflush : q = session . query ( TrackGroup ) . filter ( TrackGroup . track_pam_id == c . uid , #TrackGroup.group_id == new_group.uid, # Commented as it's pam and binary that are unique identifiers before we add uids TrackGroup . binfile == binary_file , ) new_track = q . one_or_none () if new_track is None : new_track = TrackGroup ( track_id = uuid . uuid4 (), track_pam_id = c . uid , group_id = new_group . uid , binfile = binary_file , ) else : # update the new trackgroup with latest new_track . track_pam_id = c . uid new_track . binfile = binary_file found_tracks . append ( new_track ) if len ( found_tracks ) > 0 : found_groups . append ( new_group ) else : logging . warn ( \"Found %d Group with 0 tracks! Not including!\" , new_group . huid ) logging . info ( \"Found %d Groups and %d Tracks.\" , len ( found_groups ), len ( found_tracks )) earliest = found_groups [ 0 ] . timestart latest = found_groups [ 0 ] . timeend for group in found_groups : if group . timestart < earliest : earliest = group . timestart if group . timeend < earliest : latest = group . timeend logging . info ( \"Found %s Groups and %s Tracks.\" , str ( earliest ), str ( latest )) return ( found_groups , found_tracks )","title":"find_group_objects"},{"location":"reference/#sealhits.sources.group.fix_group_times","text":"PAMGuard SQLITE file reports incorrect group times. We therefore look at all the tracks and find the earliest and latest times and set the group times to match these. Parameters: new_groups ( List [ Groups ] ) \u2013 The latest groups we want to split. new_points ( List [ Points ] ) \u2013 The latest points we want to re-assign. Returns: List [ Groups ] \u2013 List[Groups]: The corrected groups. Source code in sealhits/sources/group.py 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 def fix_group_times ( new_groups : List [ Groups ], new_points : List [ Points ]) -> List [ Groups ]: \"\"\"PAMGuard SQLITE file reports incorrect group times. We therefore look at all the tracks and find the earliest and latest times and set the group times to match these. Args: new_groups (List[Groups]): The latest groups we want to split. new_points (List[Points]): The latest points we want to re-assign. Returns: List[Groups]: The corrected groups. \"\"\" logging . info ( \"Fixing group times...\" ) fixed_groups = [] for group in tqdm ( new_groups , desc = \"Groups fixed\" ): min_time = datetime . datetime . now () . astimezone ( tz = pytz . UTC ) max_time = datetime . datetime ( 2000 , 1 , 1 ) . astimezone ( tz = pytz . UTC ) # Check to see if we have points directly on this group # if we do then we can use these as the groups is points = [] for point in new_points : if point . group_id == group . uid : points . append ( point ) if len ( points ) > 0 : for point in points : if point . time < min_time : min_time = point . time if point . time > max_time : max_time = point . time group . timestart = min_time group . timeend = max_time fixed_groups . append ( group ) return fixed_groups","title":"fix_group_times"},{"location":"reference/#sealhits.sources.group.split_groups","text":"It is possible that some groups may have large gaps with no tracks. Such groups need to be split, creating new groups . This function returns a new list of all the groups including the original groups and the new splits, and the changed points. Split groups also buffers all groups including these that are not split, so each group has a blank area before and after it. TrackGroups are not changed. These are mostly irrelevant and are there to make matching up the original points to the original group easier. The group_id on the Point will point to the new group post split. Split groups also checks where the true start and end of the groups are as occasionally, some groups start earlier or finish later than the actual tracks for some reason. Parameters: session ( Session ) \u2013 The current SQLAlchemy session. new_groups ( List [ Groups ] ) \u2013 The latest groups we want to split. new_points ( List [ Points ] ) \u2013 The latest points we want to re-assign. buffer_gap ( int , default: 4 ) \u2013 The number of seconds for the start and end buffer. Also, the minimum gap between points in a track before we split. Returns: Tuple [ List [ Groups ], List [ Points ]] \u2013 Tuple[List[Groups], List[Points]]: The new_groups and new_point with the new split-off groups and reassigned points. Source code in sealhits/sources/group.py 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 def split_groups ( session : Session , new_groups : List [ Groups ], new_points : List [ Points ], buffer_gap = 4 ) -> Tuple [ List [ Groups ], List [ Points ]]: \"\"\"It is possible that some groups may have large gaps with no tracks. Such groups need to be split, creating new groups . This function returns a new list of all the groups including the original groups and the new splits, and the changed points. Split groups also buffers all groups including these that are not split, so each group has a blank area before and after it. TrackGroups are not changed. These are mostly irrelevant and are there to make matching up the original points to the original group easier. The group_id on the Point will point to the new group post split. Split groups also checks where the true start and end of the groups are as occasionally, some groups start earlier or finish later than the actual tracks for some reason. Args: session (Session): The current SQLAlchemy session. new_groups (List[Groups]): The latest groups we want to split. new_points (List[Points]): The latest points we want to re-assign. buffer_gap (int): The number of seconds for the start and end buffer. Also, the minimum gap between points in a track before we split. Returns: Tuple[List[Groups], List[Points]]: The new_groups and new_point with the new split-off groups and reassigned points. \"\"\" return_groups = [] return_points = [] buffer = datetime . timedelta ( seconds = buffer_gap ) for group in tqdm ( new_groups , desc = \"Splitting new groups.\" ): # Check to see if this group has already been split? #logging.info(\"Group split code %s %d\", group.huid, group.split) if group . split != - 1 : logging . info ( \"Group %s has already been split\" , group . huid ) # This group has already been assessed so get all splits then continue with session . no_autoflush : q = session . query ( Groups ) . filter ( Groups . gid == group . gid , Groups . sqliteid == group . sqliteid , Groups . sqlite == group . sqlite , ) new_groups = q . all () for group_two in new_groups : return_groups . append ( group_two ) continue # It hasn't been split, so check if it needs to be # First, get all the points and their times from each track. group_points = [] for point in new_points : if point . group_id == group . uid : group_points . append ( point ) # This should never happen but thanks to issues with data not where it should be # (disks 33 and 35) it does :/ Needs a fix at some point. if len ( group_points ) == 0 : logging . error ( \"No points found on group %s in split attempt.\" , group . huid ) continue #assert len(group_points) > 0 # We need to order times, and points, tpamids into ascending order of time group_points = sorted ( group_points , key = lambda x : x . time ) # Alter the start and end times of the groups to match the ones found # from the tracks. This is a 'belt-and-braces' sort of check given the one # problem group found in riverseals. new_group_start = group_points [ 0 ] . time new_group_end = group_points [ - 1 ] . time assert ( new_group_start >= group . timestart ) assert ( new_group_end <= group . timeend ) # Splits will contain the indices on which to split the # points into new tracks and the groups into new groups. # Group splits are the new group uids we've made. splits = [] group_splits = [] # Find the gaps and mark them up for tidx in range ( len ( group_points ) - 2 ): t0 = group_points [ tidx ] . time t1 = group_points [ tidx + 1 ] . time td = t1 - t0 if td > buffer : splits . append ( tidx + 1 ) group_time_end = group . timeend if len ( splits ) > 0 : # Redo the first group - setting it's time end to the correct one. group_time_end = group_points [ splits [ 0 ] - 1 ] . time group . timeend = group_time_end group . split = 0 group_splits . append ( group ) # Now create new groups with the same gid but new times for sidx , split in enumerate ( splits ): timestart = group_points [ split ] . time timeend = group_points [ - 1 ] . time # Set to the last time for now if sidx + 1 < len ( splits ): timeend = group_points [ splits [ sidx + 1 ] - 1 ] . time # Add the buffer times for the new group timestart -= buffer timeend += buffer # New group with new times. We keep the gid but use an incremented string for the sqlite filename # 0 in split is the original group (or parent group in this case) so we increment sidx by one as this new group is the 'first split' # TODO - we keep the same PGDFs here but eventually we'll need to double check these assert ( timestart < timeend ) new_group = None with session . no_autoflush : q = session . query ( Groups ) . filter ( Groups . gid == group . gid , Groups . sqliteid == group . sqliteid , Groups . split == sidx + 1 , Groups . sqlite == group . sqlite , ) new_group = q . one_or_none () if new_group is None : new_group = Groups ( uid = uuid . uuid4 (), gid = group . gid , sqliteid = group . sqliteid , timestart = timestart , timeend = timeend , sqlite = group . sqlite , code = group . code , comment = group . comment , interact = group . interact , mammal = group . mammal , fish = group . fish , bird = group . bird , split = sidx + 1 , huid = generate_id (), pgdfs = group . pgdfs , images = [], glfs = [], ) assert ( new_group is not None ) return_groups . append ( new_group ) group_splits . append ( new_group ) # We look at each point in turn, assigning it to a new group for point in group_points : for sgroup in group_splits : if point . time >= sgroup . timestart and point . time <= sgroup . timeend : point . group_id = sgroup . uid break return_groups . append ( group ) # Original group is always returned return_points += group_points return ( return_groups , return_points )","title":"split_groups"},{"location":"reference/#sources-model","text":"model.py - build a model from a current ingest. Function for creating the current model of the data from a particular sqlite import.","title":"Sources model"},{"location":"reference/#sealhits.sources.model.build_model","text":"The goal is to build an existing model from the database ready to compare with later. Anything in this model not in the new one will need to be deleted. Parameters: session ( Session ) \u2013 The current SQLAlchemy session. sqlname ( str ) \u2013 the name of the sqlite file we are attempting to model. Returns: Tuple [ List [ Groups ], List [ TrackGroup ], List [ Points ], List [ PGDFS ], List [ GLFS ], List [ Images ]] \u2013 Tuple[List[Groups], List[TrackGroup], List[Points], List[PGDFS], List[GLFS], List[Images]]: Six lists of the major objects in the database model. Source code in sealhits/sources/model.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 def build_model ( session : Session , sqlname : str ) -> Tuple [ List [ Groups ], List [ TrackGroup ], List [ Points ], List [ PGDFS ], List [ GLFS ], List [ Images ] ]: \"\"\" The goal is to build an existing model from the database ready to compare with later. Anything in this model not in the new one will need to be deleted. Args: session (Session): The current SQLAlchemy session. sqlname (str): the name of the sqlite file we are attempting to model. Returns: Tuple[List[Groups], List[TrackGroup], List[Points], List[PGDFS], List[GLFS], List[Images]]: Six lists of the major objects in the database model. \"\"\" groups = session . query ( Groups ) . filter ( Groups . sqlite == sqlname ) . all () # The following are all dependent on the groups above. tracks = [] points = [] pgdfs = [] glfs = [] images = [] # TODO - must be a faster way? for group in groups : # Tracks first ts = session . query ( TrackGroup ) . filter ( TrackGroup . group_id == group . uid ) . all () for t in ts : tracks . append ( t ) # Now lookup points ps = session . query ( Points ) . filter ( Points . group_id == group . uid ) . all () for p in ps : points . append ( p ) # pgdfs, glfs and images - just the ones attached to the group pgdfs += group . pgdfs glfs += group . glfs images += group . images return ( groups , tracks , points , pgdfs , glfs , images )","title":"build_model"},{"location":"reference/#ingesting-pgdfs","text":"model.py - build a model from a current ingest. Functions for processing the PGDFs as part of the ingest.","title":"Ingesting PGDFs"},{"location":"reference/#sealhits.sources.pgdf.process_pgdfs","text":"Build up the PGDFS, Points and groups_pgdfs tables from the list of PGDFs provided. Parameters: session ( Session ) \u2013 The current SQLAlchemy session. pgdf_path ( str ) \u2013 The path to the PGDFs. pgdfs ( List [ str ] ) \u2013 The list of PGDF files we want. groups ( List [ Groups ] ) \u2013 The list of Groups from the current session. tracks ( List [ TrackGroup ] ) \u2013 The list of TrackGroup in the current session. Returns: Tuple [ List [ PGDFS ], List [ Points ]] \u2013 Tuple[List[PGDFS], List[Points]]: The new PGDFS objects and the new Points objects. Source code in sealhits/sources/pgdf.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 def process_pgdfs ( session : Session , pgdf_path : str , pgdfs : List [ str ], groups : List [ Groups ], tracks : List [ TrackGroup ], ) -> Tuple [ List [ PGDFS ], List [ Points ]]: \"\"\"Build up the PGDFS, Points and groups_pgdfs tables from the list of PGDFs provided. Args: session (Session): The current SQLAlchemy session. pgdf_path (str): The path to the PGDFs. pgdfs (List[str]): The list of PGDF files we want. groups (List[Groups]): The list of Groups from the current session. tracks (List[TrackGroup]): The list of TrackGroup in the current session. Returns: Tuple[List[PGDFS], List[Points]]: The new PGDFS objects and the new Points objects. \"\"\" # TODO - this should be a set of transactions that we apply to the db # so we don't muck things up if this process crashes. assert os . path . exists ( pgdf_path ) logging . info ( \"Process PGDFs starting...\" ) logging . info ( \"Creating pgdf -> group...\" ) full_paths = pgdfs_to_full_paths ( pgdf_path , pgdfs ) new_pgdfs = [] new_points = [] # Read all the PGDFs to get the start & end times for pgpath in tqdm ( full_paths , desc = \"Reading PGDFs\" ): fgname = os . path . basename ( pgpath ) tp = pgdf . PGDF ( pgpath ) date_min = None date_max = None for pamobj in tp . module . objects : tdate = pamobj . pam . date assert tdate is not None if date_min is None : date_min = tdate elif date_min > tdate : date_min = tdate if date_max is None : date_max = tdate elif date_max < tdate : date_max = tdate # Find the tracks_groups for the groups and file we are currently processing logging . info ( \"Creating tracks lookup for file %s \" , pgpath ) # Create a second dictionary for fast checking of tracks_groups # Dictionaries have O(1) access time apparently. Hashtable like I bet. tracks_used = {} # We also don't use the pamguard id but a distinct-across-pgdf-files uid # of our own, so we need that lookup too. track_pam_to_track = {} for tg in tracks : assert ( tg . track_pam_id not in tracks_used . keys ()) tracks_used [ tg . track_pam_id ] = 0 track_pam_to_track [ tg . track_pam_id ] = tg # Create a PGDF and add it to the new list q = session . query ( PGDFS ) . filter ( PGDFS . filename == fgname ) pgdf_entry = q . one_or_none () if pgdf_entry is None : pgdf_entry = PGDFS ( uid = uuid . uuid4 (), filename = fgname , startdate = date_min , enddate = date_max , ) else : # Update PGDF with new details pgdf_entry . startdate = date_min , pgdf_entry . enddate = date_max new_pgdfs . append ( pgdf_entry ) # Now, we read the points in this PGDF but we must # only include these that reference the tracks_groups table. # We can safely assume the tracks table is not as stupidly # large as the number of annotations is small. for pamobj in tp . module . objects : pam_track = pamobj . data pam_track_uid = pamobj . pam . UID try : track = pam_track . track track_obj = track_pam_to_track [ pam_track_uid ] group = None for g in groups : if g . uid == track_obj . group_id : group = g break # Should never happen but apparently, HDD 35 and 33 are sort of # mixed. Not sure what happened here but we just log and ignore if group is None : logging . error ( \"Missing group %s for trackgroup on pgdf %s \" , track_obj . group_id , fgname ) break #assert(group is not None) # Create the link between the group and pgdf if not already if group not in pgdf_entry . groups : pgdf_entry . groups . append ( group ) # Now insert all the points into the database for this known # track. for point in track . points : q = session . query ( Points ) . filter ( Points . time == point . time , Points . sonarid == point . sonar_id , Points . minbearing == point . min_bearing , Points . maxbearing == point . max_bearing , Points . minrange == point . min_range , Points . maxrange == point . max_range , Points . peakbearing == point . peak_bearing , Points . maxvalue == point . max_value , Points . occupancy == point . occupancy , Points . objsize == point . obj_size , Points . track_id == track_obj . track_id , ) new_point = q . one_or_none () # No need to run an update existing point as a point # is unique if any of it's attributes are different. if new_point is None : new_point = Points ( uid = uuid . uuid4 (), time = point . time , sonarid = point . sonar_id , minbearing = point . min_bearing , maxbearing = point . max_bearing , minrange = point . min_range , maxrange = point . max_range , peakbearing = point . peak_bearing , peakrange = point . peak_range , maxvalue = point . max_value , occupancy = point . occupancy , objsize = point . obj_size , track_id = track_obj . track_id , group_id = group . uid ) else : # Make sure this point has it's group_uid changed! new_point . time = point . time new_point . sonarid = point . sonar_id new_point . minbearing = point . min_bearing new_point . maxbearing = point . max_bearing new_point . minrange = point . min_range new_point . maxrange = point . max_range new_point . peakbearing = point . peak_bearing new_point . peakrange = point . peak_range new_point . maxvalue = point . max_value new_point . occupancy = point . occupancy new_point . objsize = point . obj_size new_point . track_id = track_obj . track_id new_point . group_id = group . uid new_points . append ( new_point ) except KeyError : # This track isn't important so skip # print(e) # TODO - this try except thing is a bit naughty! pass return ( new_pgdfs , new_points )","title":"process_pgdfs"},{"location":"reference/#sealhits.sources.pgdf.tracks_to_pgdfs","text":"Take the created TrackGroup list and return the PGDFs we need. Parameters: tracks ( List [ TrackGroup ] ) \u2013 The TrackGroup list of interest. Returns: List [ str ] \u2013 List[str]: List of PGDF filenames we need to cover these TrackGroup. Source code in sealhits/sources/pgdf.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def tracks_to_pgdfs ( tracks : List [ TrackGroup ]) -> List [ str ]: \"\"\"Take the created TrackGroup list and return the PGDFs we need. Args: tracks (List[TrackGroup]): The TrackGroup list of interest. Returns: List[str]: List of PGDF filenames we need to cover these TrackGroup. \"\"\" pgdfs_required = [] for track in tracks : binfile = track . binfile if binfile not in pgdfs_required : pgdfs_required . append ( binfile ) return pgdfs_required","title":"tracks_to_pgdfs"},{"location":"reference/#ingesting-from-sqlite","text":"sqlpam.py - Reading data from the annotation SQLite file. This module contains the following SQLPAM - a class representing the PAM SQLITE file. TrackChild - a class representing an individual track TrackGroup - a group of TrackChild representing something Examples: >>> from pypam.sqlpam import SQLPAM >>> sqlitepath = \"pam.sqlite3\" >>> assert os.path.exists(sqlitepath) >>> sqlpam = SQLPAM(sqlitepath) >>> print(sqlpam.tables)","title":"Ingesting from sqlite"},{"location":"reference/#sealhits.sources.sqlpam.SQLPAM","text":"The class that represents the data held in the SQLite annotation database. This object will hold the TrackGroups and TrackChilds. Source code in sealhits/sources/sqlpam.py 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 class SQLPAM : \"\"\"The class that represents the data held in the SQLite annotation database. This object will hold the TrackGroups and TrackChilds.\"\"\" # TODO - just as with GLF, do we want to concat multiple files? # TODO - better checking for return values from the db (I.e missing) def __init__ ( self , sqlite_path ): \"\"\"Initialise our SQLPAM object Args: sqlite_path (str): full path and name of the sqlite_path file. \"\"\" con = sqlite3 . connect ( sqlite_path ) cur = con . cursor () res = cur . execute ( \"SELECT name FROM sqlite_master\" ) self . tables = [ t [ 0 ] for t in res . fetchall ()] self . track_groups = [] self . track_children = [] res = cur . execute ( \"SELECT ID, UID, UTC, PCLocalTime, PCTime, \\ ChannelBitmap, EndTime, DataCount, Track_Type, \\ Marine_Mammal, Fish, Bird, Interaction_with_blades, \\ Comment FROM Track_Groups\" ) tracks = res . fetchall () id_uid_lookup = {} # TODO - could do some inner join stuff here instead of python combine # Read the track groups first off for ( id , uid , utc , pclocal , pctime , cbitmap , endtime , dcount , ttype , mammal , fish , bird , interaction , comment , ) in tracks : # We need extra checks here to make sure # numerics are indeed numerics. SQLLite DB has a number # of mistakes. if mammal is None or not checks . is_float ( mammal ): mammal = - 1 if fish is None or not checks . is_float ( fish ): fish = - 1 if bird is None or not checks . is_float ( bird ): bird = - 1 interact = False # This is very annoying and silly if interaction is not None : if \"1\" in interaction : interact = True if \"0\" in interaction : interact = False elif not checks . is_float ( interaction ): interact = False elif interaction == 1 : interact = True elif interaction == 0 : interact = False tg = TrackGroup ( id , uid , utc , pclocal , pctime , cbitmap , endtime , dcount , ttype , comment , mammal , fish , bird , interact , ) self . track_groups . append ( tg ) # Use a composite key as only id and uid combined are unique id_uid_lookup [ str ( tg . id ) + \"-\" + str ( tg . uid )] = tg res = cur . execute ( \"SELECT UID, UTC, PCLocalTime, PCTime, \\ ChannelBitmap, parentID, parentUID, LongDataName, \\ BinaryFile FROM Track_Groups_Children where \\ BinaryFile is not null\" ) children = res . fetchall () # Read the track children, adding them to their parent for ( uid , utc , pclocal , pctime , cbitmap , parentid , parentuid , ldata , bfile , ) in children : tc = TrackChild ( uid , utc , pclocal , pctime , cbitmap , parentid , parentuid , ldata , bfile , ) self . track_children . append ( tc ) parent = id_uid_lookup [ str ( tc . parent_id ) + \"-\" + str ( tc . parent_uid )] parent . add_child ( tc )","title":"SQLPAM"},{"location":"reference/#sealhits.sources.sqlpam.SQLPAM.__init__","text":"Initialise our SQLPAM object Parameters: sqlite_path ( str ) \u2013 full path and name of the sqlite_path file. Source code in sealhits/sources/sqlpam.py 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 def __init__ ( self , sqlite_path ): \"\"\"Initialise our SQLPAM object Args: sqlite_path (str): full path and name of the sqlite_path file. \"\"\" con = sqlite3 . connect ( sqlite_path ) cur = con . cursor () res = cur . execute ( \"SELECT name FROM sqlite_master\" ) self . tables = [ t [ 0 ] for t in res . fetchall ()] self . track_groups = [] self . track_children = [] res = cur . execute ( \"SELECT ID, UID, UTC, PCLocalTime, PCTime, \\ ChannelBitmap, EndTime, DataCount, Track_Type, \\ Marine_Mammal, Fish, Bird, Interaction_with_blades, \\ Comment FROM Track_Groups\" ) tracks = res . fetchall () id_uid_lookup = {} # TODO - could do some inner join stuff here instead of python combine # Read the track groups first off for ( id , uid , utc , pclocal , pctime , cbitmap , endtime , dcount , ttype , mammal , fish , bird , interaction , comment , ) in tracks : # We need extra checks here to make sure # numerics are indeed numerics. SQLLite DB has a number # of mistakes. if mammal is None or not checks . is_float ( mammal ): mammal = - 1 if fish is None or not checks . is_float ( fish ): fish = - 1 if bird is None or not checks . is_float ( bird ): bird = - 1 interact = False # This is very annoying and silly if interaction is not None : if \"1\" in interaction : interact = True if \"0\" in interaction : interact = False elif not checks . is_float ( interaction ): interact = False elif interaction == 1 : interact = True elif interaction == 0 : interact = False tg = TrackGroup ( id , uid , utc , pclocal , pctime , cbitmap , endtime , dcount , ttype , comment , mammal , fish , bird , interact , ) self . track_groups . append ( tg ) # Use a composite key as only id and uid combined are unique id_uid_lookup [ str ( tg . id ) + \"-\" + str ( tg . uid )] = tg res = cur . execute ( \"SELECT UID, UTC, PCLocalTime, PCTime, \\ ChannelBitmap, parentID, parentUID, LongDataName, \\ BinaryFile FROM Track_Groups_Children where \\ BinaryFile is not null\" ) children = res . fetchall () # Read the track children, adding them to their parent for ( uid , utc , pclocal , pctime , cbitmap , parentid , parentuid , ldata , bfile , ) in children : tc = TrackChild ( uid , utc , pclocal , pctime , cbitmap , parentid , parentuid , ldata , bfile , ) self . track_children . append ( tc ) parent = id_uid_lookup [ str ( tc . parent_id ) + \"-\" + str ( tc . parent_uid )] parent . add_child ( tc )","title":"__init__"},{"location":"reference/#sealhits.sources.sqlpam.TrackChild","text":"The class representing the individual track from the PAMGUARD pgdf binary file. It contains a UID link to the gemini object from the PGDF. Source code in sealhits/sources/sqlpam.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 class TrackChild : \"\"\"The class representing the individual track from the PAMGUARD pgdf binary file. It contains a UID link to the gemini object from the PGDF.\"\"\" def __init__ ( self , uid : int , utc : str , pc_local : str , pc_time : str , channel : int , parent_id : int , parent_uid : int , long_data_name : str , binary_file : str , ): \"\"\"Initialise our TrackChild Object. Args: uid (int): a unique identifier matching the PGDF binary track. utc (str): a string representing the time in UTC. pc_local (str): the local time on the pc recording. pc_time (str): the time on the pc recording. channel (int): unknown. parent_id (int): The ID of the TrackGroup to which this child belongs. parent_uid (int): The UID of the TrackGroup to which this child belongs. long_data_name (str): unknown. binary_file (str): The filename of the PGDF that holds this track. \"\"\" self . uid = uid self . utc = datetime . fromisoformat ( utc ) self . utc = pytz . utc . localize ( self . utc ) self . pc_local_time = pc_local self . pc_time = pc_time self . channel_bitmap = channel self . parent_id = parent_id self . parent_uid = parent_uid self . long_data_name = long_data_name self . parent = None # Fix for the bug in pamguard with the binary file if binary_file [: 2 ] == \"i_\" : self . binary_file = \"Gemin\" + binary_file def _add_parent ( self , parent : TrackGroup ): self . parent = parent def __str__ ( self ): return ( str ( self . uid ) + \",\" + str ( self . utc ) + \",\" + str ( self . pc_local_time ) + \",\" + str ( self . pc_time ) + \",\" + str ( self . channel_bitmap ) + \",\" + str ( self . parent_id ) + \",\" + str ( self . parent_uid ) + \",\" + str ( self . long_data_name ) + \",\" + str ( self . binary_file ) )","title":"TrackChild"},{"location":"reference/#sealhits.sources.sqlpam.TrackChild.__init__","text":"Initialise our TrackChild Object. Parameters: uid ( int ) \u2013 a unique identifier matching the PGDF binary track. utc ( str ) \u2013 a string representing the time in UTC. pc_local ( str ) \u2013 the local time on the pc recording. pc_time ( str ) \u2013 the time on the pc recording. channel ( int ) \u2013 unknown. parent_id ( int ) \u2013 The ID of the TrackGroup to which this child belongs. parent_uid ( int ) \u2013 The UID of the TrackGroup to which this child belongs. long_data_name ( str ) \u2013 unknown. binary_file ( str ) \u2013 The filename of the PGDF that holds this track. Source code in sealhits/sources/sqlpam.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 def __init__ ( self , uid : int , utc : str , pc_local : str , pc_time : str , channel : int , parent_id : int , parent_uid : int , long_data_name : str , binary_file : str , ): \"\"\"Initialise our TrackChild Object. Args: uid (int): a unique identifier matching the PGDF binary track. utc (str): a string representing the time in UTC. pc_local (str): the local time on the pc recording. pc_time (str): the time on the pc recording. channel (int): unknown. parent_id (int): The ID of the TrackGroup to which this child belongs. parent_uid (int): The UID of the TrackGroup to which this child belongs. long_data_name (str): unknown. binary_file (str): The filename of the PGDF that holds this track. \"\"\" self . uid = uid self . utc = datetime . fromisoformat ( utc ) self . utc = pytz . utc . localize ( self . utc ) self . pc_local_time = pc_local self . pc_time = pc_time self . channel_bitmap = channel self . parent_id = parent_id self . parent_uid = parent_uid self . long_data_name = long_data_name self . parent = None # Fix for the bug in pamguard with the binary file if binary_file [: 2 ] == \"i_\" : self . binary_file = \"Gemin\" + binary_file","title":"__init__"},{"location":"reference/#sealhits.sources.sqlpam.TrackGroup","text":"A group of TrackChild that may have annotations. Source code in sealhits/sources/sqlpam.py 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 class TrackGroup : \"\"\"A group of TrackChild that may have annotations.\"\"\" def __init__ ( self , id : int , uid : int , utc : str , pc_local : str , pc_time : str , channel : int , end_time : str , dc : int , track_type : str , comment : str , mammal : int , fish : int , bird : int , interact : bool , ): \"\"\"Initialise our TrackGroup Object. Args: uid (int): a unique identifier matching the PGDF binary track, although not that unique it seems :/ utc (str): a string representing the time in UTC. utc_milli (int): an int number of milliseconds since the epoch in utc. pc_local (str): the local time on the pc recording. pc_time (str): the time on the pc recording. channel (int): unknown. end_time (str): When does the last Track in this group finish? dc (int): unknown. track_type (str): unknown. comment (str): mammal (int): Was this trackgroup a mammal? fish (int): Was this trackgroup a fish? bird (int): Was this trackgroup a bird? interact (bool): Did this trackgroup interact with the turbine? \"\"\" self . id = id self . uid = uid self . utc = datetime . fromisoformat ( utc ) self . utc = pytz . utc . localize ( self . utc ) self . pc_local_time = pc_local self . pc_time = pc_time self . channel_bitmap = channel self . end_time = datetime . fromisoformat ( end_time ) self . end_time = pytz . utc . localize ( self . end_time ) self . data_count = dc self . track_type = track_type # TODO - enum self . comment = comment self . mammal = mammal self . fish = fish self . bird = bird self . interaction = interact self . children = [] def __str__ ( self ): return ( str ( self . id ) + \",\" + str ( self . uid ) + \",\" + str ( self . utc ) + \",\" + str ( self . pc_local_time ) + \",\" + str ( self . pc_time ) + \",\" + str ( self . channel_bitmap ) + \",\" + str ( self . end_time ) + \",\" + str ( self . data_count ) + \",\" + str ( self . track_type ) + \",\" + str ( self . comment ) + \",\" + str ( self . marine_mammal ) + \",\" + str ( self . fish ) + \",\" + str ( self . bird ) + \",\" + str ( self . interaction ) + \",\" ) def add_child ( self , child : TrackChild ): \"\"\"Add a TrackChild to this TrackGroup. Args: child (TrackChild): a TrackChild object to add. \"\"\" assert child . parent_uid == self . uid self . children . append ( child ) child . _add_parent ( self ) return self","title":"TrackGroup"},{"location":"reference/#sealhits.sources.sqlpam.TrackGroup.__init__","text":"Initialise our TrackGroup Object. Args: uid (int): a unique identifier matching the PGDF binary track, although not that unique it seems :/ utc (str): a string representing the time in UTC. utc_milli (int): an int number of milliseconds since the epoch in utc. pc_local (str): the local time on the pc recording. pc_time (str): the time on the pc recording. channel (int): unknown. end_time (str): When does the last Track in this group finish? dc (int): unknown. track_type (str): unknown. comment (str): mammal (int): Was this trackgroup a mammal? fish (int): Was this trackgroup a fish? bird (int): Was this trackgroup a bird? interact (bool): Did this trackgroup interact with the turbine? Source code in sealhits/sources/sqlpam.py 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 def __init__ ( self , id : int , uid : int , utc : str , pc_local : str , pc_time : str , channel : int , end_time : str , dc : int , track_type : str , comment : str , mammal : int , fish : int , bird : int , interact : bool , ): \"\"\"Initialise our TrackGroup Object. Args: uid (int): a unique identifier matching the PGDF binary track, although not that unique it seems :/ utc (str): a string representing the time in UTC. utc_milli (int): an int number of milliseconds since the epoch in utc. pc_local (str): the local time on the pc recording. pc_time (str): the time on the pc recording. channel (int): unknown. end_time (str): When does the last Track in this group finish? dc (int): unknown. track_type (str): unknown. comment (str): mammal (int): Was this trackgroup a mammal? fish (int): Was this trackgroup a fish? bird (int): Was this trackgroup a bird? interact (bool): Did this trackgroup interact with the turbine? \"\"\" self . id = id self . uid = uid self . utc = datetime . fromisoformat ( utc ) self . utc = pytz . utc . localize ( self . utc ) self . pc_local_time = pc_local self . pc_time = pc_time self . channel_bitmap = channel self . end_time = datetime . fromisoformat ( end_time ) self . end_time = pytz . utc . localize ( self . end_time ) self . data_count = dc self . track_type = track_type # TODO - enum self . comment = comment self . mammal = mammal self . fish = fish self . bird = bird self . interaction = interact self . children = []","title":"__init__"},{"location":"reference/#sealhits.sources.sqlpam.TrackGroup.add_child","text":"Add a TrackChild to this TrackGroup. Parameters: child ( TrackChild ) \u2013 a TrackChild object to add. Source code in sealhits/sources/sqlpam.py 190 191 192 193 194 195 196 197 198 199 def add_child ( self , child : TrackChild ): \"\"\"Add a TrackChild to this TrackGroup. Args: child (TrackChild): a TrackChild object to add. \"\"\" assert child . parent_uid == self . uid self . children . append ( child ) child . _add_parent ( self ) return self","title":"add_child"},{"location":"tutorials/","text":"Tutorials The following tutorial will show you how to go from the base datafiles to a final dataset that can be used with ClassySeal . Ingesting data The first thing to do is to ingest the data using the ingest.py script. There are three sources of data required: The PAMGuard derived SQLITE database file. This contains the metadata and the group annotations A directory of PGDF files. These binary files are also taken from PAMGuard A directory of GLF files. These hold the Tritech Sonar images and can be quite large. We will generate FITS images from these. The ingest command can be executed as follows: python ingest.py -s /path/to/sqlite3.file -g /path/to/glf/dir -p /path/to/pgdf/dir -b 4 -o /path/to/output/fits/dir -d <database_name> The '-b' switch specifies the length of the buffer in seconds. This amount of images will be appended and prepended to the group. For more information on the various switches type: python ingest.py -h This command can take up to several days to complete depending on the number and size of the various groups. Multiple ingest commands can be run at once. Depending on the disks, network speed or other restrictions, you may or may not be able to run a large number of ingests at once. Generating a dataset Once the ingest is complete, you can generate a dataset. Let's generate an Rdata dataset. This consists of a number of images, each a crop of the original sonar image and the metadata for each crop. python tordata.py -o /path/to/output/dir -c /path/to/cache -i /path/to/fits/dir -e -d <database name> The cache directory saves the fan distorted images for use later and is optional. The '-e' switch interpolates and cleans the original track.","title":"Tutorials"},{"location":"tutorials/#tutorials","text":"The following tutorial will show you how to go from the base datafiles to a final dataset that can be used with ClassySeal .","title":"Tutorials"},{"location":"tutorials/#ingesting-data","text":"The first thing to do is to ingest the data using the ingest.py script. There are three sources of data required: The PAMGuard derived SQLITE database file. This contains the metadata and the group annotations A directory of PGDF files. These binary files are also taken from PAMGuard A directory of GLF files. These hold the Tritech Sonar images and can be quite large. We will generate FITS images from these. The ingest command can be executed as follows: python ingest.py -s /path/to/sqlite3.file -g /path/to/glf/dir -p /path/to/pgdf/dir -b 4 -o /path/to/output/fits/dir -d <database_name> The '-b' switch specifies the length of the buffer in seconds. This amount of images will be appended and prepended to the group. For more information on the various switches type: python ingest.py -h This command can take up to several days to complete depending on the number and size of the various groups. Multiple ingest commands can be run at once. Depending on the disks, network speed or other restrictions, you may or may not be able to run a large number of ingests at once.","title":"Ingesting data"},{"location":"tutorials/#generating-a-dataset","text":"Once the ingest is complete, you can generate a dataset. Let's generate an Rdata dataset. This consists of a number of images, each a crop of the original sonar image and the metadata for each crop. python tordata.py -o /path/to/output/dir -c /path/to/cache -i /path/to/fits/dir -e -d <database name> The cache directory saves the fan distorted images for use later and is optional. The '-e' switch interpolates and cleans the original track.","title":"Generating a dataset"}]}